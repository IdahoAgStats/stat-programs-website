---
title: "Applied ANOVA in R"
authors:
- jpiaskowski
categories: ["R"]
draft: false
date: "2021-11-17T00:00:00Z"
image:
  caption: ""
  focal_point: ""
lastMod: "2021-11-17T00:00:00Z"
projects: []
subtitle: "Navigating the R linear model wilderness"
summary: 
tags: ["ANOVA", "linear models", "lme4", "emmeans"]
---

 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


![xkcd commentary on p-values](p_values.png)

### Introduction

ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility. The result is that it is easy to misspecify a model or make other mistakes. The information below is intended to serve as a guide through the R ANOVA wilderness. 

#### Packages Needed 

There are many packages to load. Here is a (very) brief summary of what each package does.

| Package | Purpose |
|------------|---------------|
|car | Anova() function to extract type III & II sums of squares |
|lme4 | mixed models |
|nlme | mixed models, non-linear models, alternative covariance structures |
|emmeans | for extracting least squares means and contrasts |
|lmer test | improved summary functions of lmer objects |
|dplyr | data organization |
|forcats | for managing categorical data |
|agridat | has many agricultural data sets |
|agricolae | has options for many common agricultural experimental designs |


```{r message=FALSE, warning=FALSE}
library(car)
library(lme4)
library(nlme)
library(emmeans) #in older version of R, you may need to install "multcompView" separately to access full functionality of the emmeans package
library(lmerTest)
library(dplyr)
library(forcats)
library(agridat)
```


#### Formula Notation

There are some consistent features across ANOVA methods in R. Formula notation is often used in the R syntax for ANOVA functions. It looks like this: `$Y ~ X`, where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments). 

```{r}
my_formula <- formula(Y ~ treatment1 + treatment2)
class(my_formula)
my_formula
```

Often the independent variables (i,e, the treatments or the x variables) are expected to be factors, another type of R object:

```{r}
my_var <- c(rep("low",5), rep("high", 5))
class(my_var) #check what variable type it is
```

Although "my_var" is not type factor, it is type "character" which is automatically converted to a factor in `lm()`, `lmer()`, `lme()` and many other linear modeling functions. There are some packages that do not follow this convention, so it's helpful to read function documentation, especially if you get unexpected results. 

Variables like year, which are often imported as a number or integer, do need to be converted to a factor or a character variable prior to analysis. Otherwise, they will be interpreted as a number in linear modelling and treated as a covariate, e.g, 2020 would be 2,020. Here is one way to do this conversion: 

```{r}
my_factor <- as.character(my_var) # convert to a character
class(my_factor) # check variable type to confirm
my_factor <- as.factor(my_var) # convert to a factor
class(my_factor) # check variable type again to confirm
```

The choice of whether to convert a categorical variable to a character or factor depends on the comfort of the user with these structures and package requirements. 

Sometimes, there is a need to alter the order of treatment levels (that is, how R sees those levels). The default behavior of R is to order categorical levels alphanumerically.  However, sometimes there are reasons you may not want this (for example, you want to set a particular reference level as the first factor level). 

Below is one example of how to reorder factor levels in a variable. The first step is to see which levels are present in the variable and how they are ordered: 

```{r}
levels(my_factor) 
```

Once that is known, you can use that information to manually set the levels and their order. Note that spelling of each level much match what is actually present in the variable. Unmatched levels in the variable will be set to NA automatically by R in the following step. 

```{r}
my_factor <- factor(my_factor, levels = c("low", "high")) 
levels(my_factor) # check the new ordering
```

Knowing the level order is important because in the implementation of ANOVA in R, the first level is treated as the reference level. Manipulating factors is a challenging task in R. The package [forcats](https://forcats.tidyverse.org/) contains a collection of accessory functions for managing factors ("forcats" = for categories). The tutorial uses the forcats function `fct_drop()`. 

*More on formulas:*

The formula first shown, `Y ~ treatment1 + treatment2`, includes main effects only. Other formula notation includes the symbols `:` and `*`, indicating notation for interaction only and main effects plus the interaction term, respectively. 

```{r}
formula(Y ~ treatment1:treatment2) # interaction only
formula(Y ~ treatment1*treatment2) # interaction plus main effects
```

These two formulas are equivalent:
```
formula(Y ~ treatment1 + treatment2 + treatment1:treatment2) 
formula(Y ~ treatment1*treatment2) 
```
Perhaps you can see from these examples that formulas are a really just a collections of characters (that is, a string) and exist independent of any data set. Later, we will need to link these formulas to a data set to actually construct a linear model and conduct statistical analysis.  


### ANOVA for fixed effects models

Here is a function for reporting the number of missing data in each column. There are other ways to do this, but I find this function easy enough to write and use. 

```{r}
count_na <- function(df) {
  apply(df, 2, function(x) sum(is.na(x))) 
}
```

#### Completely Randomised design 

First, load the data set "warpbreaks" (a data set from base R). This is an old data set with variables for wool type (A and B) and tension on the loom (L, M or H). The response variable is "breaks", the number of times the wool thread breaks on industrial looms. 

I always like to have a quick look at the data before running any statistical tests. So, here we go:  


```{r}
data(warpbreaks)
count_na(warpbreaks)
str(warpbreaks)

warpbreaks$wool <- factor(warpbreaks$wool, levels = c("A", "B", "C"))

table(warpbreaks$wool, warpbreaks$tension)
hist(warpbreaks$breaks, col = "gold")
boxplot(breaks ~ wool, data = warpbreaks, col = "orangered")
boxplot(breaks ~ tension, data = warpbreaks, col = "chartreuse") #why not have colorful plots?
```

This data set has 2 treatments. We don't know if there is an interaction between the variables, yet. A good start is to run a linear model using `lm()` function, the linear regression function. As a reminder, ANOVA is a special case of the linear regression model where the predictors (the experimental treatments) are categories rather than a continuous variable. 

```{r}
# run standard linear model for main effects only
lm.mod1 <- lm(breaks ~ wool + tension, data = warpbreaks)

# extract type III sums of squares from that model
Anova(lm.mod1, type = "3") 

# run a linear model with main effects and interactions
lm.mod2 <- lm(breaks ~ wool*tension, data = warpbreaks)

# ...and type III sums of squares 
Anova(lm.mod2, type = "III")
```

**FYI**

*functions only shown as an example and not actually run.*
```
# this function runs type II sums of squares: 
Anova(lm.mod2, type = "II")
# this function runs type I sums of squares: 
anova(lm.mod2)
```

<span style="color:mediumblue"> **A few comments on types of sums of squares:** </span>

As a reminder, the type of sums of squares used in statistical tests can impact the results and subsequent interpretation. Type I, sums of squares tests for statistical significance by adding one variable to the model at time (and hence is also called "sequential"). If there is any unbalance in the treatments, the type I sums of squares are dependent on the order variables are added to the model and hence is often not the best choice for many agricultural experiment. Type III sums of squares (also called "partial" or "marginal") evaluates the statistical significance of variable or interaction, assuming that the other variables are in the model. This is a decent default option. The last option is Type II sums of squares, which is the best option when *you are sure there are no interactions between variables*. If there is complete balance among treatments (each treatment is observed the same number of times with no missing data), then there is no need to concern yourself with these different types of sums of squares. 

#### Compare Models

```{r}
# conduct an F test comparing the models
anova(lm.mod1, lm.mod2)

# also, consider doing a stepwise approach for finding the best model:
step(lm.mod2)
```
#### Model diagnostics

```{r}
plot(lm.mod2) #this will produce 4 plots of residuals
shapiro.test(resid(lm.mod2)) #standard shapiro-wilk test. 
# this variable could be analyzed with a log-normal model instead
```

#### Least squares means & contrasts

The emmeans package is a flexible package for extracting the estimated marginal means (in SAS, the "least squares means") from different linear models. It is compatible with a large number of R linear modelling packages. 

Here is some code for extracting the marginal means and conducting contrasts. 

```{r}
# extract least squares means for 'tension'
(lsm <- emmeans(lm.mod2, ~ tension))
emmeans(lm.mod2, "wool")
```

All pairwise comparisons within each level of tension: 
```{r}
contrast(lsm, "pairwise")
```

Conduct custom contrasts comparing 'Low' tension versus 'Medium' and 'High' and 'High' versus 'Medium' and 'Low'. 
```{r}
# see the order of each level in a factor
levels(warpbreaks$tension)
# construct a list of constructs 
# each item must be same length as the the number of levels present in the variable tension
# use numbers and fracions to indicate the contrasting levels
# the numbers must sum to zero 
cList <- list(LvMH = c(1, -0.5, -0.5), # low vs high + medium
              HvLM = c(0.5, 0.5, -1))  # high vs low + medium

# check that each contrast sums to zero
lapply(cList, sum)

# perform custom contrast and include a Bonferroni adjustment
summary(contrast(lsm, cList, adjust = "bonferroni"))
```

#### Randomised Complete Block Design (RCBD) - fixed effects model

This example uses rapeseed yield data from multiple locations, years and cultivars. Within a single location or year, the replication is often balanced. 

**Load Data and examine:**
```{r}
data(shafii.rapeseed) # from the 'agridat' package

rapeseed1987 <- shafii.rapeseed %>% filter(year == 87) %>% 
  mutate(block = fct_drop(rep), Cv = fct_drop(gen), loc = fct_drop(loc))

str(rapeseed1987)
count_na(rapeseed1987)
table(rapeseed1987$Cv, rapeseed1987$loc) #experiment has 1 rep per block 

hist(rapeseed1987$yield, col = "gold")
boxplot(yield ~ Cv, data = rapeseed1987, col = "orangered")
boxplot(yield ~ loc, data = rapeseed1987, col = "chartreuse")
```

**Analyse experiment:**

```{r}
# for this example, the analysis will only be done for a single year
# block is nested within location
# if each block had a unique name, 'Error(block)' would suffce
shaf.aov <- aov(yield ~ Cv*loc + Error(block), data = rapeseed1987)

summary(shaf.aov)
emmeans(shaf.aov, ~ Cv | loc)
```

### ANOVA for mixed models
*(models with random and fixed effects)*

Random effects are often those treatments levels drawn from a large population of possible treatment levels and there is interest in understanding the distribution and variance of that population. This in contrast to fixed effects, where the inferences are restricted to the treatment levels tested.

Blocking factors and Year are often considered random factors because a researcher is not interested in particular years or a blocking factor. When there is unbalanced replication, the variance components should be estimated with maximum likelihood or REML, which implies use of the packages "lmer" and/or "nlme". 


#### Randomised Complete Block Design (RCBD) - mixed effects

The "shafii.rapeseed" data set will be used for this section. 

**Analyse experiment using a mixed model:**

This uses the function `lme()` from the package "nlme". Functionally, it is very similar to calling `lme4::lmer()`. The degrees of freedom are different (`lmer()` is using Satterthwaite's approximation), but the p-values are the same. 
```{r}
# turn year into the factor "Year"
shafii.rapeseed$Year <- as.factor(shafii.rapeseed$year)
# create a blocking variable that is unique for each location-by-year combination
# so R doesn't conflate "R1" from one location/year with another location/year
shafii.rapeseed$Rep <- as.factor(paste(shafii.rapeseed$loc, shafii.rapeseed$year, shafii.rapeseed$rep, sep = "_"))

shaf.lme <- lme(fixed = yield ~ gen*loc + Year,
                  random = ~ 1|Rep,
                  data = shafii.rapeseed, method = "REML")

# view sum of squares table 
# when anova() is called for an lme object, the function called is actually anova.lme()
anova(shaf.lme, type = "marginal") # "marginal" is equivalent to type III sums of squares
Anova(shaf.lme, type = "3")
# FYI: use "anova(model.lme)" for type I sums of squares

# lmer notation
shaf.lmer <- lmer(yield ~ gen*loc + Year + (1|Rep),
                  data = shafii.rapeseed, REML = T)
anova(shaf.lmer, type = "marginal")
Anova(shaf.lmer, type = "3")
```

#### Diagnostics, model building
```{r}
plot(shaf.lme)
qqnorm(shaf.lme, abline = c(0, 1))
```

#### Least squares means

```{r include=FALSE}
library(multcomp)
```


```{r}
# for cultivar 
(lme.means.cv <- emmeans(shaf.lme, "gen"))
# for location
(lme.means.loc <- emmeans(shaf.lme, "loc"))
# for cultivar means within each location
lme.means.int <- emmeans(shaf.lme, ~ gen | loc + Year)

# this code would produce location means within each cultivar 
# emmeans(model.lme, ~ loc | gen))
# also: 
# emmeans(model.lme, ~ loc | gen)) provides the same estimates as 'emmeans(model.lme, ~ gen | loc))'
```

#### Pairwise Contrasts:
```{r}
# all pairwise
pairs(lme.means.cv)
# plot results
plot(lme.means.cv, comparison = T)
plot(lme.means.loc, comparison = T, horizontal = F) # rotate plots to vertical position
# blue bars = lsmeans confidence 95% confidence intervals
# red arrows. pairwise differences (overlapping arrows = not significantly different)
```

For those who want the letters assigned to treatments based on all pairwise comparisons, it's an unwieldy road: 

```{r warning=FALSE}
library(multcomp) # this will need to be installed if you do not already have it
tukey <- glht(shaf.lme, linfct = mcp(loc = "Tukey"))
### extract information
cld_tukey <- cld(tukey)
print(cld_tukey)
```


**Interaction plots can also be done:**  

*(but, it gets unwieldy)*

```{r fig.height=20}
plot(lme.means.int, comparison = T, adjust = "tukey")
```


#### Other pre-set contrasts 

```{r}
# compare to a control, e.g. "Bridger"
levels(shafii.rapeseed$gen)

# Bridger is listed in position 2 of the factor 'shafii.rapeseed$gen'
# so '2' is set as the reference level in the following contrast statement: 

# "trt.vs.ctrlk" (treatment versus control treatment k) is a specific option to compare all treatment levels to a user-defined level
# by default, it will use the last level as the reference level

contrast(lme.means.cv, "trt.vs.ctrlk", ref = 2) 

```
Search `?contrast.emmGrid` to see full list of options for preset contrasts. 

#### Custom contrasts

```{r}
# example: contrast Western locations versus Southern locations

# first, find out what levels are present
unique(shafii.rapeseed$loc)

# next create a contrast list 
# this is a list of coefficients as long your list of treatment levels
# indicating what coefficients to give each treatment level

# in this example, levels "ID", "MT", "OR", and "WA" are contrasted versus
# "NC", "SC", "MS", "TN", "TX" and "VA"

cList <- list(West_V_South = c(0, 1/4, 0, -1/6, 1/4, -1/6, 0, 1/4, -1/6, 0, -1/6, -1/6, -1/6, 1/4))

# check that each contrast sums to zero:
lapply(cList, sum)

lme.means.loc2 <- emmeans(shaf.lme, "loc", contr = cList)
summary(lme.means.loc2)

# same contrast can also be done within each level of 'gen':
emmeans(shaf.lme, ~ loc | gen, contr = cList)

```

To perform custom contrasts on a another variable, a cList and emmeans call for that variable is required.  

### ANCOVA
(analysis of covariance)
From a R programming perspective, this is no different than running a standard linear model. A data set from **agridat**, "theobald.covariate" comparing corn silage yields across multiple years, locations and cultivars. The data set includes a covariate, "chu" (corn heat units, a bit like growing degree days). 

**Load data and examine:**
```{r}
data(theobald.covariate)
str(theobald.covariate)
count_na(theobald.covariate)
```

**Exploratory plots:**
```{r}
# distributions of continuous variables
hist(theobald.covariate$yield, col = "gold")
hist(theobald.covariate$chu, col = "gray70")

# relationship between reponse variable and covariate:
with(theobald.covariate, plot(chu, yield))
length(unique(theobald.covariate$chu))

# the usual boxplots: 
boxplot(yield ~ env, data = theobald.covariate, col = "orangered")
boxplot(yield ~ year, data = theobald.covariate, col = "chartreuse")
boxplot(yield ~ gen, data = theobald.covariate, col = "darkcyan")
```

Check the extent of replication: 
```{r}
theobald.covariate$Year <- as.factor(theobald.covariate$year)
replications(yield ~ Year + env + gen, data = theobald.covariate)
# with(theobald.covariate, table(gen, env, Year)) # lots of useful output
```

The treatments are not fully crossed, so a fully specified model of the form `yield ~  Year*env*gen*chu` cannot be tested. The treatments and interactions were tested in reduced models and compared (not shown). The final "best" model is shown below. 

```{r}
# the covariate, chu, is added in like any other effect. 
theobald.lm2 <- lm(yield ~  Year + env*chu, data = theobald.covariate)
Anova(theobald.lm2, type = "III")

# how to extract the covariate slope(s): 
emtrends(theobald.lm2, ~ env, "chu")

# emmeans extracted as usual:
emmeans(theobald.lm2, ~ env)
emmeans(theobald.lm2, ~ Year)
```

#### Split-plot

Load "Oats" from nlme. Nitrogen level ("nitro") is the main plot, cultivar ("Variety") is the sub-plot and "Block" describes the blocking layout.

```{r}
data(Oats) 
str(Oats)
count_na(Oats)

Oats$N <- as.factor(Oats$nitro)
replications(yield ~ Variety*N*Block, data = Oats)
table(Oats$Variety, Oats$N)

hist(Oats$yield, col = "gold")
boxplot(yield ~ N, data = Oats, col = "dodgerblue1")
boxplot(yield ~ Variety, data = Oats, col = "red3")
```

**Balanced Trial Analysis**  

The format for specifying split-plot error terms is `Error(blocking factor/main plot)`. 
```{r warning=FALSE}
#contrasts("contr.sum")
spl.oats <- aov(yield ~ Variety*N + Error(Block:N), data = Oats) 
summary(spl.oats)

emmeans(spl.oats, "N") 
emmeans(spl.oats, ~ Variety) 
```

**Unbalanced Trial Analysis**  

```{r warning=FALSE}
spl.oats2 <- lmer(yield ~ N*Variety + (1|Block:N), data = Oats) 
Anova(spl.oats2, type = "3")


emmeans(spl.oats2, "N") 
emmeans(spl.oats2, ~ Variety) 
```

### Other Designs

There are many other experimental designs commonly used in agricultural trials (split-split plot, split-block, alpha lattice, etc). We have written an online resource for routine incorporation of spatial covariates into field trial analysis that includes information on how to [analyze different designs](https://idahoagstats.github.io/guide-to-field-trial-spatial-analysis/model-extension-r.html). You could also consider using the [agricolae](https://CRAN.R-project.org/package=agricolae) package. 


### Extra Functions 
**for extracting model parameters, diagnostics and other model information**  

These work differently with different R object types. That is, different output will result depending on if a "lm", "lme" or "merMod" (lmer) object is used in the function call. 

```
# extract model summary
summary()

#extract coefficients:
coef()

#extract residuals
resid()
rstudent()
residuals()

# extract predicted values
fits()

# make diagnostic plots
plot()

# extract influence measures:
influence.measures()

#other fir diagnostics:
cooks.distance()
dffits()
dfbeta()
hat()

```

To see the all functions available for a particular type of linear model object, use:

```{r}
methods(class = "lm") # for lm objects
methods(class = "lme") # for lme4 objects
methods(class = "merMod") # for nlme objects 
```
 
The package **emmeans** also supports [a large number of models](https://cran.r-project.org/web/packages/emmeans/vignettes/models.html).  

