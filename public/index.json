[{"authors":null,"categories":null,"content":"","date":1663804800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618444800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/authors/bprice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bprice/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1655942400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1655942400,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/authors/jpiaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jpiaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"Harpreet Kaur is a consulting statistician at the University of Idaho.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d418cd995e4594ea649975db6ef4c94e","permalink":"/authors/hkaur/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hkaur/","section":"authors","summary":"Harpreet Kaur is a consulting statistician at the University of Idaho.","tags":null,"title":"Harpreet Kaur","type":"authors"},{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\nThis resource is provided to address common problems we see in analysis of agricultural studies.\nTutorials is organized into three sections: R-focused, SAS-focused and general tutorials covering statistical topics.\nBlog posts are short essays on topical issues in agricultural and adjacent statistics.\nWorkshops includes course materials from previous workshops.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\nThis resource is provided to address common problems we see in analysis of agricultural studies.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/authors/xdai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xdai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"\r\r Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D Salt Lake City, Utah\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n\rcheck your system\r\r\rSAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SASÂ® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n\rbegin the workshop\r\r\rThis workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"5bdfbb99e430a3c1a2a4056386e4765a","permalink":"/workshops/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/","section":"workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"\r Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()\r If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()\r If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation\r\u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting\r\u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling\r\u0026quot;SpATS\u0026quot;, # for fitting splines\r\u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;spaMM\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling\rinstall.packages(package_list)\rsapply(package_list, require, character.only = TRUE)\r \rPlease note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. The packages have all been successfully installed if after the sapply(package_list, require, character.only = TRUE) is run, the R output is \u0026ldquo;TRUE\u0026rdquo; for each package. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\r\r\rLibrary Information    package usage     dplyr, tidyr, standard data manipulation   purrr for repeat functions   nlme mixed linear models with options for spatial covariates   lme4 mixed linear models with crossed random effects   ggplot, desplot standard plotting packge and extension for plotting block outlines   SpATS spline-fitting   sp preparation of spatial objects   spdep Moran\u0026rsquo;s I test   gstat for fitting empirical variogram   spaMM fits Matern covariances structure for mixed linear models   emmeans extracts marginal means from linear model objects    ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"15841bfb95305ff58eb70b526b39d8a2","permalink":"/workshops/spatial-workshop/prep-work/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/prep-work/","section":"workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n\r Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n\r Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n\r The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.  \r  Error terms are often correlated with each other, invalidating the downstream analysis  \r  high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics \r\ranother distracted boyfriend meme!\r Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"a31a33acbb99d2f0e16a5caf445f69ea","permalink":"/workshops/spatial-workshop/why-spatial/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/why-spatial/","section":"workshops","summary":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":"\rThis workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n\r While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n\r Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R\r# load libraries\rlibrary(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme)\r# read in data and prepare it\rNin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row)\rNin_na \u0026lt;- filter(Nin, !is.na(rep))\r# make exploratory plot\rggplot(Nin, aes(x = row, y = col)) +\rgeom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) +\rgeom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) +\rlabs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() +\rtheme(axis.text = element_text(size = 12),\raxis.title = element_text(size = 14),\rlegend.title = element_text(size = 14),\rlegend.text = element_text(size = 12))\r## conduct moran's I test ##\r# set neighbors with convenience function for grids\rxy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals\rnin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep,\rdata = Nin, na.action = na.exclude)\rresid_lme \u0026lt;- residuals(nin.lme)\rnames(resid_lme) \u0026lt;- Nin$plot\r# two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude)\rmoran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)\r  SAS\r# read in data\rproc format;\rinvalue has_NA\r'NA' = .;\r;\rfilename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;;\rdata alliance;\rinfile NIN firstobs=2 delimiter=',';\rinformat yield has_NA.;\rinput entry $ rep $ yield col row;\rRow = 4.3*Row;\rCol = 1.2*Col;\rif yield=. then delete;\rrun;\r# heatmap\rproc sgplot data=alliance;\rHEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run;\r# linear mixed model\rproc mixed data=alliance;\rclass Rep Entry;\rmodel Yield = Entry / outp=residuals;\rrandom Rep;\rrun;\r# Moran's I\rproc variogram data=residuals plots(only)=moran ;\rcompute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ;\rcoordinates xc=row yc=col;\rvar resid;\rrun;\r  ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb80fd2ad66cac061f57b377665b0236","permalink":"/workshops/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/diagnosis/","section":"workshops","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n\r\rExample Empirical Variogram\r  range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Exponential Variogram\r Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Gaussian Variogram\r MatÃ©rn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n\r\rEmpirical MatÃ©rn Variogram\r There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n\rFor more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.\r\r\rVariogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n\r Not all fitted variogram models are worthy\n\r\rVariogram gone bad\r Code for this section The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(gstat); library(spaMM)\r# set up spatial object\rNin_spatial \u0026lt;- Nin_na\rcoordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte\rclass(Nin_spatial)\r# establish max distance for variogram estimation\rmax_dist = 0.6*max(dist(coordinates(Nin_spatial)))\r# calculate empirical variogram\rresid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist,\rwidth = max_dist/15, # 15 is the number of bins\rdata = Nin_spatial)\rplot(resid_var1) # empirical variogram\r#Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram)\r# starting value for the nugget\rnugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much)\rNin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential\rNin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian\rNin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern\r# actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp)\rNin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau)\rNin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T)\rplot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;)\rplot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;)\rplot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;)\r# parameters:\rNin_variofit_gau\rnugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error)\rrange \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points\rsill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance\r  SAS\r# calculate semivariance and compute empirical variogram\rproc variogram data=residuals plots(only)=(semivar);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rvar resid;\rrun;\r# fit models to the empirical variogram\rproc variogram data=residuals plots(only)=(fitplot);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"cd8303e2c52504ec61991e37816c126c","permalink":"/workshops/spatial-workshop/variograms/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/variograms/","section":"workshops","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR\rlibrary(emmeans); library()\r# (nlme and gstat should already be loaded)\rlibrary(spaMM) # for running `corMatern()`\r# standard linear model\rnin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep,\rdata = Nin,\rna.action = na.exclude)\r# extract the esimated marginal means for variety\rpreds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;))\r# use information from the variogram fitting for intialising the parameters\rnugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill\r# initalise the covariance structure (from the nlme package)\rcor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F,\rtype = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)\r# update the rcbd model\rnin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus)\r# extract predictions for 'gen'\rpreds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;)\r# a similar procedure can be follow for other models\r# but we are going to take a shortcut and not specify the parameters\r# exponential\rcor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_exp \u0026lt;- update(nin_lme, corr = cor.exp)\rpreds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;))\r# Matern structure\rcor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_matern \u0026lt;- update(nin_lme, corr = cor.mat)\rpreds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)\r  SAS\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry ;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_RCBD_means;\rtitle1 'NIN data: RCBD';\rrun;\rproc mixed data=alliance maxiter=150;\rclass entry;\rmodel yield = entry /ddfm=kr;\rrepeated/subject=intercept type=sp(gau) (Row Col) local;\rparms (11) (22) (19);\rlsmeans entry/cl;\rods output LSMeans=NIN_Spatial_means;\rtitle1 'NIN data: Gaussian Spatial Adjustment';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"057408dc907a581ba7ba45d742b73dbd","permalink":"/workshops/spatial-workshop/correlated-error-models/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/correlated-error-models/","section":"workshops","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n\r The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(lme4)\r# exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;)\rboxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;)\rboxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;)\r## row/column model ##\r# data prep\rNin$rowF = as.factor(Nin$row)\rNin$colF = as.factor(Nin$col)\r# specify model\rnin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF),\rdata = Nin, na.action = na.exclude)\r# extract random effects for row and column\rranef(nin_rc)\r# extract predictions\rnin_rc \u0026lt;- as.data.frame(emmeans(nin.rc, \u0026quot;gen\u0026quot;))\r  SAS\r# exploratory boxplots\rproc sgplot data=alliance;\rvbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\r# row/column model\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry row col/ddfm=kr;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_row_col_means;\rtitle1 'NIN data: RCBD';\rrun;\r  Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR\rnin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20),\rdegree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0))\rpreds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)\r  SAS\rproc glimmix data=alliance ;\rclass entry rep;\reffect sp_r = spline(row col);\rmodel yield = entry sp_r/ddfm=kr;\rrandom row col/type=rsmooth;\rlsmeans entry/cl;\rods output LSMeans=NIN_smooth_means;\rtitle1 'NIN data: RCBD';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"b280dc26c3a20f98ee5ed0cf8a9403e6","permalink":"/workshops/spatial-workshop/trend-modelling/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/trend-modelling/","section":"workshops","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)  \rComparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.\r\r\rUnfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R\rlibrary(tidyr)\r# remove some objects we don't need (and will interfere with downstream processes)\rrm(nin_variofit, nin_vgm)\rrm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect)\r# assemble objects into a list\rnlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern)\rnames(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;)\r# extract log likelihood, AIC, BIC\rdata.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC),\rBIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglik))\r# (higher is better for loglik, lower is better for AIC and BIC)\r# compare post-hoc power\r# conduct ANOVA\ranovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]})\r# bind all the output together\ra \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;, \u0026quot;row-col\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a)\ra\r## compare precision of estimates\rall.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;))\rerrors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;)\rpred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors))\rerror_df \u0026lt;- bind_cols(errors)\rcolnames(error_df) \u0026lt;- pred.names\rboxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;)\r# compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;)\rpreds_df \u0026lt;- bind_cols(preds)\rcolnames(preds_df) \u0026lt;- pred.names\rpreds_df$gen \u0026lt;- preds_exp$gen\r# plot changes in rank\rlev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;)\rpivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) +\rgeom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) +\rgeom_line() +\rylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal()\r  SAS\rdata NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_RCBD_means;\rRCB_est = estimate;\rRCB_se = stderr;\rrun;\rdata NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_Spatial_means;\rSp_est = estimate;\rSp_se = stderr;\rrun;\rproc sort data=NIN_RCBD_means;\rby entry;\rrun;\rproc sort data=NIN_Spatial_means;\rby entry;\rrun;\rdata compare;\rmerge NIN_RCBD_means NIN_Spatial_means;\rby entry;\rrun;\rproc rank data=compare out=compare descending;\rvar RCB_est Sp_est;\rranks RCB_Rank Sp_Rank;\rrun;\rproc sort data=compare;\rby Sp_rank;\rrun;\rproc print data=compare(obs=15);\rvar entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"006196c4ae04f5e62ccfd0b0e4974438","permalink":"/workshops/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/model-comparison/","section":"workshops","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in BurgueÃ±o et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (ânameâ) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR\r# (if not already loaded)\rlibrary(dplyr); library(nlme); library(ggplot2)\rlibrary(gstat); library(sp)\r# read in data\raug_data_origin \u0026lt;- read.csv(\u0026quot;data/augmented_lind.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed\rmutate(yieldkg = yieldg/1000) # to prevent overflow\r# summarise the genoytypic data by checks/not checks\rgen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when(\rcounts \u0026gt; 1 ~ \u0026quot;check\u0026quot;,\rcounts == 1 ~ \u0026quot;unrep\u0026quot;))\r# need info on just the checks\rchecks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes\rgen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when(\rdelta == \u0026quot;check\u0026quot; ~ gamma,\rdelta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when(\rdelta == \u0026quot;unrep\u0026quot; ~ gamma,\rdelta == \u0026quot;check\u0026quot; ~ gamma))\r# merge original data set with info on treatment levels\raug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldkg, yieldg) %\u0026gt;%\rmutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) ## modelling\raug1 \u0026lt;- lme(fixed = yieldg ~ tau,\rrandom = ~ 1|tau/beta,\rdata = aug_data, na.action = na.exclude)\r# extract residuals\raug_data$res \u0026lt;- residuals(aug1)\r# plot residual chloroepleth map:\rggplot(aug_data, aes(y = row, x = col)) +\rgeom_tile(aes(fill = res)) +\rscale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) +\rscale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) +\rscale_y_continuous(breaks = 1:max(aug_data$col)) +\rcoord_equal() +\rtheme_void() # add spatial covariates\raug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res))\rcoordinates(aug_spatial) \u0026lt;- ~ col + row\rmax_dist = 0.5*max(dist(coordinates(aug_spatial)))\raug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist,\rwidth = max_dist/10, data = aug_spatial)\r# optional to run: nugget_start \u0026lt;- min(aug_vario$gamma)\raug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start)\raug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm)\rplot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;)\rcor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F,\rtype = \u0026quot;exponential\u0026quot;)\raug1_sp \u0026lt;- update(aug1, corr = cor_exp)\r# spatial parameters:\raug1_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug1_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r##### OR #######\r# another formulation\r# delta estimates effects of replicated versus unreplicated genotypes\r# gamma estimates the effecs of all genotypes evaluated in the trial\raug2 \u0026lt;- lme(fixed = yieldkg ~ delta,\rrandom = ~ 1|delta/gamma,\rdata = aug_data, na.action = na.exclude)\raug2_sp \u0026lt;- update(aug2, corr = cor_exp)\r# spatial parameters:\raug2_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug_blups2 \u0026lt;- ranef(aug2_sp)$gamma %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r  SAS\rfilename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;;\rPROC IMPORT OUT= WORK.augmented\rDATAFILE= AUG\rDBMS=CSV REPLACE;\rGETNAMES=YES;\rDATAROW=2; RUN;\rdata augmented;\rset augmented;\rif yieldg = 999999 or yieldg=. then delete; /* Remove missing values */\rprow=prow*11.7; /*convert row and column indices to feet */\rpcol=pcol*5.5;\rrun;\rproc freq noprint data=augmented;\rtables name/out=controls;\rrun;\rdata controls;\rset controls;\rif count \u0026gt;1;\rrun;\rproc sort data=controls;\rby name;\rrun;\rproc sort data=augmented;\rby name;\rrun;\rdata augmented;\rmerge augmented controls;\rby name;\rif count=. then d2=2; /* Unreplicated */\relse d2=1; /* Replicated */\ryieldkg=yieldg/1000;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2/noint outp=residuals ddf=229 229;\rlsmeans d2;\r*lsmeans name(d2)/slice = d2;\rrun;\rproc sgplot data=residuals;\rHEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map';\rrun;\rproc variogram data=residuals plots(only)=(fitplot);\rwhere yieldkg ^= .;\rcoordinates xc=pcol yc=pRow;\rcompute lagd=6.6 maxlags=25;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229;\rlsmeans d2;\rrepeated/subject=intercept type=sp(pow)(prow pcol) local;\rods output SolutionR =parms;\rparms (0.074) (0.0051)(0.475) ;\r*lsmeans name(d2)/slice = d2; # alot of output!!\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"561372141df0a39b09e92d9d036e8b87","permalink":"/workshops/spatial-workshop/augmented/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/augmented/","section":"workshops","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm MatÃ©rn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive \r   Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"c08d0a8717c5c9f76fe45f925c14f37a","permalink":"/workshops/spatial-workshop/conclusion/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/conclusion/","section":"workshops","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["William Price"],"categories":["p-values","statistics","reproducible research"],"content":"What to make of p-values Among various scientific communities there have been questions and concerns raised regarding hypothesis testing and, specifically, the interpretation and use of the p-value. The p-value is a number that results from a statistical test of a hypothesis and can be generally described as: The probability of a specified hypothesis given an observed set of data and the assumptions underlying the hypothesis. The hypothesis in question is typically the NULL hypothesis, which in controlled experiments is that there are no differences among treatments. Problems from p-values have come about from a variety of reasons including over reliance on statistical testing, misunderstanding and misinterpreting the meaning of a p-value, and deliberate or unconscious manipulation of results and analyses to obtain âdesirableâ p-values, a process referred to as p-hacking (see Greenland, et al. 2016 for a more comprehensive list of problems with p-values).\nOne common practice in research, including agriculture, is that of âbright lineâ testing where decisions on statistical significance and relevance are based strictly on p-values being greater or less than a specified value such as 0.05. Unfortunately, such decision rules have been repeatedly shown to perform poorly. Many aspects such as sample size, inaccurate hypotheses, incorrect analyses, violated assumptions and a slew of other, often uncontrollable, factors can interfere with this decision-making process. For many of us, this is a bitter pill to swallow. We, and I include myself here, have been trained or lulled into using statistical results in this manner and our minds naturally gravitate to this simplistic binary, accept/reject paradigm. The real world, however, is more complex and we should accommodate those complexities into our interpretations of research results. The question is then, how we should move forward.\nHow should we use p-values, or more importantly, how should we present research results. Some journals have taken the extreme step of banning p-values completely, i.e., Basic and Applied Social Psychology, while others have suggested methods such as applying multiple analytical techniques to the data to assess consistency in results, using more sophisticated Bayesian techniques to quantify the probability that a hypothesis is true, or emphasizing model estimates and their confidence bounds, rather than outcomes of testing. In most cases, practitioners generally agree that p-values can still play a role in model evaluation, but they are not the end of the story. A complete assessment of data should consider effect size (how different the estimates are in real terms), agreement with previous data/studies, and expert opinion in addition to p-values. The American Statistical Association lists 6 basic principles when considering p-values:\n P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.  How to not say Significant or Nonsignificant. OK, thatâs all well and fine, but when it comes down to it, what do we write when reporting our analyses? There are many ways to answer this, and ultimately, the style will be up to the author. I find it useful to first recall and follow the distinction between Results and Discussion sections of the standard scientific reporting format. Results are for describing what you saw, while Discussions are for interpreting, speculating, and applying your scientific expertise to those results. Some examples:\nSuppose we have two dairy supplement treatments, A and B. An Analysis of Variance estimates the average milk production for the treatments in 3900 dairy cows as A: 30.30 kg/d and B: 31.83 kg/d; p = 0.0001. In a Results section, we might say something like âTreatment B was estimated to have greater mean milk production than treatment A (31.83 and 30.30, respectively; p=0.0001)â. However, in the Discussion section we could elaborate further with âWhile treatment B gave a higher milk yield than treatment A, the difference of 1.53 kg/d is not of practical importance and likely due to the large number of animals used.â\nAlternatively, suppose the estimated means were A: 30.30 kg/d and B: 45.62 kg/d; p=0.4328 in 34 dairy cows. Again, in the Results section, we could say âA higher milk yield was observed in treatment B compared to treatment A, although the difference was not detectable in this data (45.62 and 30.30 kg/d, respectively; p=0.4328).â A Discussion of these data could further explain âAlthough evidence for the difference in treatments A and B is limited, the 15.32 kg/d increase in milk yield of treatment B over A may warrant further investigation with a larger sample size.â\nPoints to remember: A small p-value does not indicate the strength of an effect and is not sufficient for drawing conclusions on treatment effects. Alternatively, a large p-value is not evidence of âno differenceâ in treatments. It simply implies that, with the data at hand and the model assumed, a difference could not be detected.\nIn general, employing common sense with professional modesty and avoiding strong declarative statements when reporting and discussing experimental results can go a long way towards averting p-value abuse. It may be useful to recall the research by Philip Tetlock regarding expert opinions Why Foxes Are Better Forecasters Than Hedgehogs: Pundants that make assertive statements with over confidence tend to be wrong in the long run, while those that are more tentative and conditional in their statements have a better track record. As scientists, we should strive to be a fox, not a hedgehog.\n","date":1663804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"ea1ba41b6c2c689e7e880474456773e9","permalink":"/posts/p-values/","publishdate":"2022-09-22T00:00:00Z","relpermalink":"/posts/p-values/","section":"posts","summary":"Some lessons on proper interpretation and use of p-values","tags":null,"title":"Interpreting p-values","type":"posts"},{"authors":["William Price"],"categories":["design","statistics","experiment","treatment"],"content":"In describing an experiment, we often use the word design. Design, however, can take on several meanings within the structure of a research study. This work summarizes an older article that describes this structure and the levels of design it is composed of (Urquhart, N. S. 1981). Understanding the types of design is not only useful for reporting on the study, but can also inform the analysis of the associated data.\nLevels of Design There are three main levels of design in a study: Treatment, Experimental, and Response. These each relate to the relationships between and among the treatments and their arrangement in the study.\nTreatment Design Treatment design describes how the experimental treatments relate to one another. The simplest treatment design is unstructured where there are no dependencies or relatedness among treatments. These could, for example, be various varieties, herbicides, or dietary supplement types. Alternatively, there may be an implied structure among the treatments such as rates or dosages where the treatments incrementally change by specified amounts. Another common treatment design is the complete factorial design where there are two or more types of treatments and all levels of each treatment type occur at all levels of the other types. They are typically denoted by the number of levels for each treatment type, e.g. a 2x3 factorial design or 3x4x2 factorial design. This structure is often modified by the addition of an untreated or standard control treatment, and in that case, we might describe the treatment design as a 2x3 + 1 factorial. There are also some specialized alternatives to the factorial where some treatment combinations are left out which are referred to as incomplete factorials.\nExperimental Design This is probably the most commonly used term for describing an experiment, however, it often confused with treatment design. Experimental design refers to how the treatments are randomized and organized physically relative to each other. It does not address how the treatments might be related, but only to how they are arranged in the experiment. The simplest experimental design is the Completely Random Design, or CRD. Here all treatments and their replications are assigned random positions within the experiment. There is no imposed structure to the arrangement. A very common modification of this has the treatments randomly assigned to positions within each replication. This randomization changes from replication to replication. This is the Randomized Complete Block or RCB design. Blocks here refer to groups or aggregations of complete sets of treatments. The randomization process is \u0026ldquo;restricted\u0026rdquo; to once per replication. With multiple factors such as a factorial treatment design, this can be extended to two or more restrictions on randomization within each replication leading to Split Plot and Split Block experimental designs. It is important to note, however, that this is independent of the treatment design. That is, a factorial such as the 2x3 above, could be imposed over any of these experimental designs, e.g. CRD or RCB, or Split Plot, etc.\nResponse Design Last category is the response design. This describes how the samples or observations relate to each other. A simple response design, for example, would have one sample/observation per treatment per replication. Or, we might have a sub-sampling response design where there are multiple samples or observations on each treatment in each replication. Another common response design is the repeated measures design where observations are taken sequentially through time on the same sampling unit. Sampling units are what the observations or samples themselves are made on. Examples here might be single pots or plants in a greenhouse study, plots in a field experiment, or animals in a feeding study. A similar concept is the experimental unit. This refers to what level of the experiment we draw inferences to. The experimental unit may be the same as the sampling unit, or it may be an aggregation of sampling units. For example, the experimental unit might be a single plot in a field experiment, however, we might take multiple soil samples from each plot (sampling units) which are then aggregated to a single value for the plot, the experimental unit. Typically this is done to mitigate or account for variability within the experimental unit.\nPutting this together Descriptions of a study for reporting or presentation should include all three levels of design. Doing so informs the audience as to what the treatment structure was, how it was arranged or randomized, and how it was sampled. This can be done in a few sentences. For example, we might say \u0026ldquo;Treatments consisted of Factor A at two levels and Factor B at three levels arranged in a 2x3 factorial treatment design. The experimental design was a randomized complete block with 4 blocks and three samples were taken from each experimental unit.\u0026quot;\nAnalyses of the study design will also incorporate and utilize all three levels of design. The details of this, however, are beyond the scope here and the reader is referred to the referenced article.\nReferences Urquhart, N. S. (1981). The anatomy of a study. HortScience, 16(5), 621-627.\n","date":1660694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"885304123ea650b9ab171d9629059168","permalink":"/posts/experimental-design/","publishdate":"2022-08-17T00:00:00Z","relpermalink":"/posts/experimental-design/","section":"posts","summary":"In describing an experiment, we often use the word *design*. Design, however, can take on several meanings within the structure of a research study. Understanding the types of design is not only useful for reporting on the study, but can also inform the analysis of the associated data.","tags":null,"title":"Design and the anatomy of a study","type":"posts"},{"authors":["William Price"],"categories":["SAS","troubleshooting","documentation"],"content":"You need to get that analysis done so you can write that report/paper/dissertation/thesis/presentation to meet that deadline to please the boss \u0026hellip; BUT \u0026hellip; SAS is giving you heartburn! DON\u0026rsquo;T PANIC.\nAlthough SAS errors look confusing, SAS is really trying to help you find the problem. Really! Look in the Log Window for any error messages. They are marked in RED. Recall that SAS runs sequentially from top to bottom, so any mistakes at the top of the program will propagate more errors down the line. Always start by trying to fix the first error, then rerun the program. You may find that when the initial problems are fixed, the rest of the errors will vanish.\nWhen SAS gives an error message, it also lists a numeric code, like 201 or 76 indicating the line and character in the line where it had trouble. SAS will also underline the offending part of the program and mark it with the numeric code so you can match an error message with the problem area. It can\u0026rsquo;t always find the exact place where the problem occurred, but it usually close by. For more debugging information take a look at the TIPS below on common mistakes and problems.\nGood Luck!\nTips Missing Quotes: If you\u0026rsquo;ve recieved the \u0026ldquo;longer than 200 characters\u0026rdquo; message then the cause of the problem is easy to find. You have left out a quote somewhere. Carefully go back through the program and check for both beginning and ending quotes in any statements that use quoted strings such as TITLE, INFILE, or AXIS statements . Quoted strings in SAS are color coded as a magenta color to make them easier to see. Large blocks of text appearing this color are an indicator of a missing quote. Also note that single quotes are different than double quote characters. If you start a string with one of these and end with the other type, SAS will indicate a missing quote.\nAfter you find the culprit, fix it. But wait! Correcting the problem is not quite that easy (somehow you knew that!). SAS builds up the code you run sequentially. So if you try to run the \u0026ldquo;corrected\u0026rdquo; code, SAS will STILL be looking for an ending quote from last time. The quotes will still be unbalanced and you\u0026rsquo;ll still get an error message. This can be very frustrating, but here is the solution. First, correct the code, but before you run it, open a new editor window. Next, give SAS what it is looking for, the offending single quote. In the new editor window enter this: '; and submit that as a program. This will satisfy SAS (ignore any error messages at this point). Then, go back to the original program and run it. The \u0026ldquo;quote\u0026rdquo; problem should go away.\nMissing Semicolons: Leaving out a semicolon is probably the easiest mistake to make. The error message you get may vary here depending on where the mistake was made. Usually you get a message like \u0026ldquo;variable XXX not found\u0026rdquo; or \u0026ldquo;invalid option\u0026rdquo;. SAS is trying to interpret the errant statement as part of the preceding statement and, therefore, gets it\u0026rsquo;s cyberfeet tangled. The solution is straight forward: insert a ; and resubmit the program.\nMissing RUN Statement: Sometimes while running a program several times you\u0026rsquo;ll notice that the PROC LOTTERY you had at the end of your program is missing from the output listing. So, thinking that SAS maybe just \u0026ldquo;forgot\u0026rdquo; to run it the first time, you run the program again. Now you find it at the top of the output listing instead of the bottom! What the heck is going on!? Remember, SAS runs both sequentially and cumulatively. The first time you ran the program, SAS saw the PROC LOTTERY at the end, but didn\u0026rsquo;t find a RUN; statement. Without that it will hold on to the PROC LOTTERY and wait to execute it when it sees the next DATA or PROC step. When you run the program for the second time SAS first executes the PROC it is holding on to and then runs the submitted program (again, leaving off the last PROC). This can cause confusion to no end (no pun intended!). The solution, of course, is to put in the missing RUN;. By the way, before you go running to your SAS manuals looking for PROC LOTTERY, remember this is only and example!\nLISTING Window/File: Like other portions of SAS, the Listing and Log windows or output files may be cumulative. If you know you have changed your calculations for yield from lbs to kilograms, but SAS seems to keep printing lbs in the output, make sure you have cleared out the Listing window. You are probably looking at an \u0026ldquo;old\u0026rdquo; output created before you changed things. This also applies to the Log window.\nCase problems: SAS is creating too many levels of a variable, for example, when running PROC MIXED, PROC GLIMMIX or PROC FREQ. Go back and check the data very carefully. Alphabetic variables are case sensitive so the value \u0026ldquo;Trt1\u0026rdquo; is different from \u0026ldquo;TRT1\u0026rdquo;. This problem often creeps into programs when data has been combined over several years or locations. If you have this type of data, try to formulate a consistent template for yourself and others to use for entering data. Define what the treatment codes and variable names will be ahead of time. The only way to handle this problem is to change the data or use a data step and the IF statement to correct the problem.\n0 and O problems: SAS keeps insisting that it can\u0026rsquo;t find your variable \u0026ldquo;TRT0\u0026rdquo;, but you know it\u0026rsquo;s there in the INPUT statement! Check things carefully. You may have used O (capital letter oh) instead of 0, a zero. This is very easy to do and is made worse by the fact that the two are physically close together on most keyboards. Similar to problems with case above, this mix up may also show up as a problem when reading in data. The only solution is to change the problem characters to the appropriate values and try again. If this problem occurs a lot, you might consider trying a different font which uses a dot or slash when writing a zero.\nGeneral Debugging Tips: Debugging SAS can be a trying experience. Here are some techniques I use to help the process.\n  Build long complex programs a step at a time. Start by reading in the data. Print it. Check it. Then do any calculations, then add PROC steps and other DATA steps. Don\u0026rsquo;t try to write the whole thing at once. Verify that a piece of code works and then move on.\n  Use comments. Try to isolate problem code by commenting out sections of the program. This is a very useful technique. See Comments.\n  Try writing programs with consistent and readable structure. Use line breaks and indentation where needed and throw in comments to explain what is happening in the program. This will help organize the program flow in your mind and point out where things are going wrong. See Programming Habits.\n  Stay calm! Learning the quirks of SAS takes time. If all else fails let us know!\n  Some SAS Resources for Error Corrections:   Simply copy and paste the error message into a search engine like Google\n  Ask a question on the SAS forums:\n  Types of Errors in SAS\n  Beginning SAS books\n  Tips and Strategies for Mixed Modeling\n  ","date":1660089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"1be7ac4b0cc19a6b2c941b21acf6e0cc","permalink":"/posts/sas-troubleshooting/","publishdate":"2022-08-10T00:00:00Z","relpermalink":"/posts/sas-troubleshooting/","section":"posts","summary":"Common errors you will encounter in SAS and how to fix them.","tags":null,"title":"Common Errors and Solutions in SAS","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","learning R"],"content":"R is a powerful tool, but, like any programming language, it takes an effort to. There is usually some degree of frustration along the way. There is an alternative: graphical user interfaces (GUI\u0026rsquo;s). These are point-and-click interfaces that are generating R code on the fly. Often you can access and save this code generated by R GUI\u0026rsquo;s to run directly on the R command line.\nR GUI\u0026rsquo;s allow you to get things done right away! While they sacrifice the full power of R for ease of usage, for many, the gain in usability is well worth this loss of advanced R functionality. No one is giving out badges and awards for usage of the R command line; that is, there is no shame in choosing to use an R GUI when it meets your needs.\nxkcd comic\nSomeone else already did an extensive review, so take a look at this for a deep dive into the functions and tools enabled by these GUIs. Each has distinct applications it is most well suited to accomplish.\nR GUI\u0026rsquo;s\narranged in alphabetical order\n BlueSky Statistics Deducer jamovi JASP R AnalyticFlow Rattle R Commander R Instat RKWard  ","date":1655942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655942400,"objectID":"099ee7ed74981da02b1a464f7578adaa","permalink":"/posts/r-gui/","publishdate":"2022-06-23T00:00:00Z","relpermalink":"/posts/r-gui/","section":"posts","summary":"R GUI's allow you to get things done right away! While they sacrifice the full power of R for ease of usage, for many, the gain in usability is well worth this loss of advanced R functionality.","tags":null,"title":"Graphical User Interfaces for R","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","learning R"],"content":"R is notoriously difficult, finicky and at times, needlessly challenging (note that I just said the same thing three times, but the repetition is for emphasis). R is also powerful, flexible and extraordinarily nimble. If you want to try out that great new thing, it\u0026rsquo;s probably possible to do it in R without programming it from scratch. But, I\u0026rsquo;m not doing you any favors by pretending that learning R is easy and painless. It is hard. R is a powerful programming language, and as usual, with great power, comes a lot of learning, practice and preparation to know how to use those powers.\nIn time, you may find you truly love solving the various logic puzzles R puts before you. Or not. Regardless, I wish you the best on your journey to learning R. This far from a comprehensive list (that\u0026rsquo;s not the point), but here are a few resources to help get you started.\nResources for the Beginner R User  RStudio Primers R for Excel Users R for Reproducible Research Short Introduction to R YaRrr! The Pirateâs Guide to R EdX short courses  Resources for the Intermediate R User  R 4 Data Science What They Forgot to Teach You (about R)  Miscellaneous very useful resources  RStudio cheatsheets R Graphics Cookbook (a ggplot resource) Rmarkdown: the Definitive Guide Rweekly Blog (great place to learn about updates to the R ecosystem) R Studio Resources (there\u0026rsquo;s so much here!) RStudio YouTube Channel Happy Git with R Advanced R (book, it is rather advanced)  ","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654041600,"objectID":"0e11125c6909b078d1ee496f7b37dada","permalink":"/posts/learning-r/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/posts/learning-r/","section":"posts","summary":"R is notoriously difficult, finicky and at times, needlessly challenging. Many resources have been created to help new users learn and become competent at using R.","tags":null,"title":"Resources for Learning R","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","troubleshooting","documentation"],"content":"What do you do when you need to solve a problem in R? If you have been programming for any amount of time, you have learned that you are likely to encounter errors programming in R and resolving those coding errors can be challenging.\nFor most everyone, goggling an error message is the first step. This is not a bad choice as it often leads you to people who have already encountered the error and solved it. But, it can also lead you to a labyrinth of different, possible conflicting, possibly incomprehensible solutions. Additionally, it is not always the most time efficient method to solve a coding problem.\nSome Alternatives Check the documentation! There\u0026rsquo;s an ancient proverb about the importance of reading documentation: \u0026ldquo;you can spend 2 hours searching the web in order to save 15 minutes of reading the documentation.\u0026rdquo;\nPackage and function documentation can be very helpful. There are two main aspects of documentation:\n Function reference: this describes the arguments a function can take, the expected format for those arguments and information about the function object. It may also contain theoretical details that are needed to understand the argument options and examples. Documentation varies in quality greatly across R packages. It can occasionally be too bare bones to be useful, but often (especially in base R commands and tidyverse packages) the documentation is very detailed and helpful. Submission to CRAN requires that packages have a documentation file that lists all package function documentation in alphabetical order. Vignettes: these are tutorials accompanying how to use a package functions. These usually cover a subset of functions and include text explanations. They are basically long examples. Vignettes can be enormously helpful. They are not required for submission to CRAN, so they are not always available, especially for older legacy packages. You can find these on the package website (if it exists) or its CRAN link. Here are some vignettes from the package {tidyr}.  How do we find documentation? You find function documentation via the R console:\n?par # does an exact search on \u0026quot;par\u0026quot;\r??plot # does a fuzzy match on \u0026quot;plot\u0026quot;\r This will open help files for those items.\nSometimes, you will discover there are multiple options and possibly different help files associated with a function name (just run methods('mean') or methods('anova') to see what I mean).\nThese are functions that act differently depending on the R object type they are called to interact with (e.g. mean(some_dates) will behave differently than mean(some_numbers)). Which leads to the next point: all R objects have a class assigned to them. You can check this with class().\ny \u0026lt;- rnorm(20); x \u0026lt;- y + rnorm(20) m \u0026lt;- lm(y ~ x)\rclass(y)\r ## [1] \u0026quot;numeric\u0026quot;\r class(m)\r ## [1] \u0026quot;lm\u0026quot;\r Understanding this difference between functions will help you understand which documentation files will help you solve your problem.\nMore importantly, once you know the object class, you can search for methods that exist for that class.\nmethods(class = \u0026quot;lm\u0026quot;)\r ## [1] add1 alias anova case.names coerce ## [6] confint cooks.distance deviance dfbeta dfbetas ## [11] drop1 dummy.coef effects extractAIC family ## [16] formula hatvalues influence initialize kappa ## [21] labels logLik model.frame model.matrix nobs ## [26] plot predict print proj qr ## [31] residuals rstandard rstudent show simulate ## [36] slotsFromS3 summary variable.names vcov ## see '?methods' for accessing help and source code\r From this, we can see a special plot() option exists (that provides several diagnostic plots), functions for extracting residuals (rstudent(), residuals()), a version of anova() written for object type \u0026ldquo;lm\u0026rdquo;, and much much more.\nRead your error messages Error messages can be obtuse and confusing (especially if you are new to programming). We have all have experienced (and will experience again) this error message:\nIt is telling us we are trying to subset (extract) information from a \u0026lsquo;closure\u0026rsquo; (which is a function). In essence, there was an attempt to do something like mean$myvar when mean() is a function, not a data.frame.\nHowever, error messages can also be trying to tell you something important. Here\u0026rsquo;s a recent experience of mine:\nOver time, these messages will become comprehensible. It\u0026rsquo;s still the same messages, but your R knowledge will help you understand them. But, becoming fluent in R error messages implies reading them and trying to understand them.\nSpecific places to ask for help Eventually, you may need to search forums or ask for help from kind strangers. If Google fails you, here are some other useful resources:\n  RStudio Community, a helpful forum that is only for R questions. It is run by RStudio and moderated (to an extent). This is one of the more useful sites to search or post on.   R4DS community, a friendly, welcoming community. Join their slack channel and ask a question.\n  Stack overflow, the long-established site of all questions programming. Can often be helpful.   Consider reading the source code This is best for advanced users, but it can hep you resolve very specific questions about a function. Reading source code will also help improve your own coding.\n** How to find source code: **\n Type the function name in the console without parentheses:  mean.default\r ## function (x, trim = 0, na.rm = FALSE, ...) ## {\r## if (!is.numeric(x) \u0026amp;\u0026amp; !is.complex(x) \u0026amp;\u0026amp; !is.logical(x)) {\r## warning(\u0026quot;argument is not numeric or logical: returning NA\u0026quot;)\r## return(NA_real_)\r## }\r## if (isTRUE(na.rm)) ## x \u0026lt;- x[!is.na(x)]\r## if (!is.numeric(trim) || length(trim) != 1L) ## stop(\u0026quot;'trim' must be numeric of length one\u0026quot;)\r## n \u0026lt;- length(x)\r## if (trim \u0026gt; 0 \u0026amp;\u0026amp; n) {\r## if (is.complex(x)) ## stop(\u0026quot;trimmed means are not defined for complex data\u0026quot;)\r## if (anyNA(x)) ## return(NA_real_)\r## if (trim \u0026gt;= 0.5) ## return(stats::median(x, na.rm = FALSE))\r## lo \u0026lt;- floor(n * trim) + 1\r## hi \u0026lt;- n + 1 - lo\r## x \u0026lt;- sort.int(x, partial = unique(c(lo, hi)))[lo:hi]\r## }\r## .Internal(mean(x))\r## }\r## \u0026lt;bytecode: 0x00000236dcfa19e8\u0026gt;\r## \u0026lt;environment: namespace:base\u0026gt;\r  Sometimes this is not informative  c\r ## function (...) .Primitive(\u0026quot;c\u0026quot;)\r subset\r ## function (x, ...) ## UseMethod(\u0026quot;subset\u0026quot;)\r## \u0026lt;bytecode: 0x00000236daf25fb8\u0026gt;\r## \u0026lt;environment: namespace:base\u0026gt;\r `[`\r ## .Primitive(\u0026quot;[\u0026quot;)\r  Use {the package {lookup} to find what you need  remotes::install_github(\u0026quot;jimhester/lookup\u0026quot;)\rlookup::lookup(`[`)\r {lookup} checks CRAN, Bioconductor and GitHub for source code! Prior to {lookup}, finding source code for R functions was challenging. Please note that this \u0026ldquo;lookup\u0026rdquo; is NOT the same same package called \u0026ldquo;lookup\u0026rdquo; found on CRAN. They have zero overlapping functionality.\n","date":1651708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651708800,"objectID":"1f2443a111e0338dce8e25592c866ad2","permalink":"/posts/getting-help-in-r/","publishdate":"2022-05-05T00:00:00Z","relpermalink":"/posts/getting-help-in-r/","section":"posts","summary":"How to find help when you run into trouble using R","tags":null,"title":"Finding Help in R","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","functions"],"content":"You may find yourself needing to do something repeatedly in R. Sure, you can cut-and-paste and change that one thing, or two things, or five things, but this quickly becomes cumbersome. The result can be a very long R file and the likelihood of making a mistake that you don\u0026rsquo;t notice increases (e.g. forgetting to change a variable or an argument).\nThere is the general rule of DRY: don\u0026rsquo;t repeat yourself. In practice, if something has to pasted more than twice, then consider writing a function to accomplish that aim instead.\nIntroduction to Writing Functions R functions follow a general structure:\nmy_function_name \u0026lt;- function(argument1, argument2) {\rfinal_output \u0026lt;- action(argument1, argument2)\rreturn(final_output)\r}\r A classic function example is conversion of temperature from Fahrenheit to celsius:\nfahr_to_cel \u0026lt;- function(fahr) {\r# function that converts temperature in degrees Fahrenheit to celsius\r# input: fahr: numeric value representing temp in degrees fahrenheit\r# output: kelvin: numeric converted temp in celsius\rcelsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius)\r}\r This function takes a numeric value, temperature in Fahrenheit, and outputs another numeric value, that same value converted to celsius.\nFunction usage:\nfahr_to_cel(80)\r ## [1] 26.66667\r This function can be called for a large number of values at once:\n# create a vector of 100 numbers randomly sampled between 1 and 100. x1 \u0026lt;- sample(1:100, 100, replace = TRUE)\rx2 \u0026lt;- fahr_to_cel(x1)\r If you provide the incorrect type of data, the function will not work:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)\r ## Error in fahr - 32: non-numeric argument to binary operator\r A More Complex Example Often we want to do something more complicated. One thing I want to do frequently is build boxplots.\nFirst, simulate some data. This data set has two categorical variables, cat1 and cat2, and 4 different continuous variables generated through data simulation.\nmydata \u0026lt;- data.frame(cat1 = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;), 10),\rcat2 = rep(c(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;), each = 20),\rvar1 = rnorm(40),\rvar2 = runif(40), var3 = rlnorm(40),\rvar4 = rbeta(40, 1, 5))\r Next, write up an example of what you want to do. In this example, let\u0026rsquo;s create a boxplot:\nboxplot(var1 ~ cat1, data = mydata,\rmain = NA, col = \u0026quot;orangered\u0026quot;)\r Now, let\u0026rsquo;s put that in a function. Start with the basic function framework:\nboxplot_func = function() {\r}\r Next, insert the function code. Start by cut-and-pasting the original boxplot command ran above:\nboxplot_func = function() {\rboxplot(var1 ~ cat1, data = mydata,\rmain = NA, col = \u0026quot;orangered\u0026quot;)\r}\r Decide on arguments you want to control and put that inside the function() parentheses. Probably the independent and dependent variable (x and y, respectively), as well as the data frame needed.\nPut those arguments inside function().\nboxplot_func = function(df, x, y) {\rboxplot(var1 ~ cat1, data = mydata,\rmain = NA, col = \u0026quot;orangered\u0026quot;)\r}\r Then indicate where those arguments are used in the function. They must be used in the function (otherwise, why have them?).\nboxplot_func = function(df, x, y) {\rboxplot(y ~ x, data = df,\rmain = NA, col = \u0026quot;orangered\u0026quot;)\r}\r However, if you try to use this function, it won\u0026rsquo;t work. The argument y ~ x is a special class of object in R called \u0026ldquo;formula\u0026rdquo; and the formatting and object type must match. Formulas are used widely in R for linear modelling and follow the exact same convention:\ny ~ x  Note that the information on either side of ~ can become more complicated. (but not in this function).\nSo, create a formula object using the functions formula() and paste() within the function and insert that into the basic boxplot code. If you don\u0026rsquo;t know how to use those function, type ?formula and ?paste into the console to learn more about them.\nboxplot_func = function(df, x, y) {\rf = formula(paste(y, \u0026quot;~\u0026quot;, x))\rboxplot(f, data = df,\rmain = NA, col = \u0026quot;orangered\u0026quot;)\r}\r What if you want the ability to change the color? Insert a new argument and replace it in the function body:\nboxplot_func = function(x, y, color) {\rf = formula(paste(y, \u0026quot;~\u0026quot;, x))\rboxplot(f, data = df,\rmain = NA, col = color)\r}\r If you want the option to set the some options or if you choose not to, have the function choose values automatically as defaults, that can be done by naming the argument in formula().\nboxplot_func = function(df = mydata, x, y, color = \u0026quot;springgreen\u0026quot;) {\rf = formula(paste(y, \u0026quot;~\u0026quot;, x))\rboxplot(f, data = df,\rmain = NA, col = color)\r}\r Next step is to run the function as it is currently written (highlight the function code and click run). Next, make sure you add this function (i.e. boxplot_funct = function(...)) to your R environment by running it in the console. You can check it exists in your R global environment as thus:\nls()\r ## [1] \u0026quot;boxplot_func\u0026quot; \u0026quot;fahr_to_cel\u0026quot; \u0026quot;mydata\u0026quot; \u0026quot;x1\u0026quot; \u0026quot;x2\u0026quot;\r Now, call the function and make sure it does what we want?\nboxplot_func(mydata, \u0026quot;cat1\u0026quot;, \u0026quot;var1\u0026quot;)\r boxplot_func(x = \u0026quot;cat2\u0026quot;, y = \u0026quot;var1\u0026quot;, col = \u0026quot;darkcyan\u0026quot;)\r boxplot_func(mydata, \u0026quot;cat2\u0026quot;, \u0026quot;var4\u0026quot;, col = \u0026quot;khaki\u0026quot;)\r What if it doesn\u0026rsquo;t do what we want? What if you get strange output? No output? Or strange error messages? Herein comes the world of debugging (another blog post for another day).\nError Checking and Error Messages You may have noticed earlier this strange error message:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)\r ## Error in fahr - 32: non-numeric argument to binary operator\r This is a very confusing message. We most certainly provided a \u0026ldquo;non-numeric argument\u0026rdquo;, but what is a \u0026ldquo;binary operator\u0026rdquo;? Turns out that is a programming speak for a standard mathematical operations addition, subtraction, multiplication and division (called \u0026lsquo;binary\u0026rsquo; because they take two inputs). Still, we are likely to encounter more strange error messages written in programmer speak that confuse us or someone else using our functions. We can write custom error messages that are produced when certain errors occur.\nHere is the temperature conversion function again:\nfahr_to_cel \u0026lt;- function(fahr) {\rcelsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius)\r}\r Since they can only take numeric argument, maybe we can start for checking for this? There are a few options in do this. One of the easiest to use is stopifnot(). This functions takes the general form: stopifnot(\u0026quot;my custom error message\u0026quot; = test). What constitutes a \u0026lsquo;test\u0026rsquo; is an R expression that returns a TRUE or FALSE value after being evaluated. Examples of this are is.character(x), is.NA(x), x \u0026gt; 0 and so on. For each of these statements, the expectation is that R will true a TRUE or FALSE. If the test does not do this reliably (e.g. you may not be able to evaluate x \u0026gt; 0 if x is non-numeric), then a different test is needed.\nIn our case, we can use is.numeric().\nfahr_to_cel \u0026lt;- function(fahr) {\rstopifnot(\u0026quot;input is not numeric\u0026quot; = is.numeric(fahr))\rcelsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius)\r}\r Let\u0026rsquo;s run some test cases:\nfahr_to_cel(30)\r ## [1] -1.111111\r fahr_to_cel(\u0026quot;thirty\u0026quot;)\r ## Error in fahr_to_cel(\u0026quot;thirty\u0026quot;): input is not numeric\r As expected, the first one worked and the second generated an error message.\nNaturally, this is a very trivial example, but if you write more complicated functions with the intent of them automatically accomplishing a goal for you, these error messages can be helpful.\nFunctions and Tidy Evaluation If you\u0026rsquo;ve worked with the tidyverse, you know it handles input a bit differently. In summary, quotes are used far less often. This makes writing function quite challening at times and required the use of the double curly braces, {{}} or the \u0026ldquo;bang-bang\u0026rdquo; operator !!.\nWhat if we wanted to do a boxplot function using ggplot?\nHere\u0026rsquo;s what the code would look like:\nlibrary(ggplot2)\rmydata \u0026lt;- data.frame(cat = rep(c(\u0026quot;AA\u0026quot;, \u0026quot;BB\u0026quot;), each = 50), obs = c(rnorm(50), runif(50)))\rggplot(mydata, aes(x = cat, y = obs)) +\rgeom_boxplot(aes(fill = cat), alpha = 0.5) +\rgeom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) +\rtheme_classic()\r But, if you try to write a function following the usual rules, it won\u0026rsquo;t work properly:\ngboxplot_func \u0026lt;- function(x1, y1) {\rggplot(mydata, aes(x = x1, y = y1)) +\rgeom_boxplot(aes(fill = x1), alpha = 0.5) +\rgeom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) +\rtheme_classic()\r}\r gboxplot_func(cat, obs)\r ## Error in FUN(X[[i]], ...): object 'obs' not found\r This one works, but the results are crazy.\ngboxplot_func(\u0026quot;cat\u0026quot;, \u0026quot;obs\u0026quot;)\r Why are the results wonky? Because while this function see \u0026ldquo;mydata\u0026rdquo; has 100 observations, it cannot connect \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;obs\u0026rdquo; to the data frame.\nThis is where the special operators come in:\ngboxplot_func2 \u0026lt;- function(x1, y1) {\rggplot(mydata, aes(x = {{x1}}, y = {{y1}})) +\rgeom_boxplot(aes(fill = {{x1}}), alpha = 0.5) +\rgeom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) +\rtheme_classic()\r}\r gboxplot_func2(cat, obs)\r The curly braces enable us to insert unquoted tidy variables and use ggplot.\nWhat if you have multiple options to specify in a single arguments? You can use the ... notation (in the final argument):\nvar_sum_funct(storms, day, name)[1:5,] # one grouping factor\r ## # A tibble: 5 Ã 3\r## name Mean SD\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AL011993 8.75 13.7 ## 2 AL012000 7.75 0.5 ## 3 AL021992 25.6 0.548\r## 4 AL021994 20.3 0.516\r## 5 AL021999 2.75 0.5\r var_sum_funct(storms, day, name, status)[1:5,] # many grouping factors\r ## `summarise()` has grouped output by 'name'. You can override using the\r## `.groups` argument.\r ## # A tibble: 5 Ã 4\r## # Groups: name [5]\r## name status Mean SD\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 AL011993 tropical depression 8.75 13.7 ## 2 AL012000 tropical depression 7.75 0.5 ## 3 AL021992 tropical depression 25.6 0.548\r## 4 AL021994 tropical depression 20.3 0.516\r## 5 AL021999 tropical depression 2.75 0.5\r var_sum_funct(storms, day) # unusual example!  ## # A tibble: 1 Ã 2\r## Mean SD\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 15.8 8.94\r This is a very brief introduction to tidy evaluation. More information on tidy evaluation is available for ggplot and dplyr.\n","date":1644278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"120fde199d76a6f8e9cbd11b35b8f252","permalink":"/posts/writing-r-functions/","publishdate":"2022-02-08T00:00:00Z","relpermalink":"/posts/writing-r-functions/","section":"posts","summary":"If you have to repeat an action in R, functions are a great way to automate this process and avoid erross from cut-paste-replaCE. Here is a short introduction on how to write these functions.","tags":[],"title":"How to Write Custom Functions in R","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Installing R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstalling RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstalling Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis)\rusethis::use_blank_slate()\r You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIf you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"7c13b3d17036c71cccfda0efef215d4b","permalink":"/posts/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/posts/getting-r-setup/","section":"posts","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":null,"title":"Getting R Set Up","type":"posts"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the usethis package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()\r You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R/.Rmd/.qmd file (Or another file type)\nThis is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*Consider yourself very lucky if this does not happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)\r Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory\râ README.md\râ\rââââdata\râ â file011.txt\râ â file012.txt\râ â\râ ââââspatial_files\râ â file208.dbf\râ â file208.shp\râ â file208.shx\râ ââââscripts\râ â eda.R\râ â analysis.R\râ â plots.R\râ â final_report.Rmd\r|\rââââoutputs\râ â plot1.png\râ â blups.csv\r|\rââââextra\râ some_paper.pdf\râ ...\r I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here)\rmydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))\r This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv)\rrenv::init()\r If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()\r If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this!\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"6b7b58ddcf9bdeab87f90d6dbc845787","permalink":"/posts/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/posts/reproducible-r/","section":"posts","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"posts"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"Short Posts  Notes on Experimental Design Some Comments on P-values  Comprehensive Tutorials  Spatial Analysis for Agricultural Field Experiments (using R and SAS) University of Florida Quantitative Genetics Course Multivariate Statistical Machine Learning Methods for Genomic Prediction Basic Introduction to Linear models USDA SCINet Learning Pathway  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aae7fdbd51c3a04e8216aa279c885255","permalink":"/tutorials/stats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/stats/","section":"tutorials","summary":"Short Posts  Notes on Experimental Design Some Comments on P-values  Comprehensive Tutorials  Spatial Analysis for Agricultural Field Experiments (using R and SAS) University of Florida Quantitative Genetics Course Multivariate Statistical Machine Learning Methods for Genomic Prediction Basic Introduction to Linear models USDA SCINet Learning Pathway  ","tags":null,"title":"General Agricultural Statistics Resources","type":"page"},{"authors":null,"categories":null,"content":"Basics Tutorials  Data Import Data Transformation \u0026amp; Wrangling Exporting Data \u0026amp; Results Reshaping Data and Merging Data Data Aggregation and Summary Creating Publication-Quality Plots in R How to Write an R Function How to Repeat Operation in R  Meta Tutorials  R installation and set-up Intro to Reproducible Research in R and More Resources Finding Help When You\u0026rsquo;re Stuck in R Keeping up With Changes in R  Additional Resources Agricultural Statistics  Rstats4Ag (weed-science oriented) Mixed models in R Data Science for Agriculture in R Mixed Models for Agriculture in R  General R  Tips for Learning R R GUIs RStudio cheatsheets R Graphics Cookbook (a ggplot resource) Rmarkdown: the Definitive Guide Rweekly Blog (great place to learn about updates to the R ecosystem) R Studio Resources (there\u0026rsquo;s so much here!) RStudio YouTube Channel Happy Git with R Advanced R (book, it is rather advanced) Reproducible Research: git, GitHub, SQL, dependency management in R, general R usage  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79f9b87c6510b8be881a90f2f0ab8d47","permalink":"/tutorials/r/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/r/","section":"tutorials","summary":"Basics Tutorials  Data Import Data Transformation \u0026amp; Wrangling Exporting Data \u0026amp; Results Reshaping Data and Merging Data Data Aggregation and Summary Creating Publication-Quality Plots in R How to Write an R Function How to Repeat Operation in R  Meta Tutorials  R installation and set-up Intro to Reproducible Research in R and More Resources Finding Help When You\u0026rsquo;re Stuck in R Keeping up With Changes in R  Additional Resources Agricultural Statistics  Rstats4Ag (weed-science oriented) Mixed models in R Data Science for Agriculture in R Mixed Models for Agriculture in R  General R  Tips for Learning R R GUIs RStudio cheatsheets R Graphics Cookbook (a ggplot resource) Rmarkdown: the Definitive Guide Rweekly Blog (great place to learn about updates to the R ecosystem) R Studio Resources (there\u0026rsquo;s so much here!","tags":null,"title":"R Tutorials \u0026 Resources","type":"page"},{"authors":null,"categories":null,"content":"Tutorials Basics  Data Import and Manipulation Linear Regression Mixed Model ANOVA Mixed Model ANCOVA and Dummy Variable Regression Generalized Mixed Model ANOVA Modeling Categorical Data Nonlinear Modeling Power and Sample Size Estimation Graphics and Plotting  Meta  Troubleshooting SAS Errors  Additional Resources  SAS Documentation SAS Support Community  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"47182c96f9e790b1c9bc06908403b6ef","permalink":"/tutorials/sas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/tutorials/sas/","section":"tutorials","summary":"Tutorials Basics  Data Import and Manipulation Linear Regression Mixed Model ANOVA Mixed Model ANCOVA and Dummy Variable Regression Generalized Mixed Model ANOVA Modeling Categorical Data Nonlinear Modeling Power and Sample Size Estimation Graphics and Plotting  Meta  Troubleshooting SAS Errors  Additional Resources  SAS Documentation SAS Support Community  ","tags":null,"title":"SAS Tutorials","type":"page"}]