[{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1637107200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1637107200,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/author/julia-piaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/julia-piaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\n","date":1636275600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1636275600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/statistical-programs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/statistical-programs/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/author/william-price/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/william-price/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/author/xin-dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-dai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"\r\r Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n\rcheck your system\r\r\rSAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SAS® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n\rbegin the workshop\r\r\rThis workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"5bdfbb99e430a3c1a2a4056386e4765a","permalink":"/workshops/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/","section":"workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"\r\r Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n\rcheck your system\r\r\rSAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SAS® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n\rbegin the workshop\r\r\rThis workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"4a1872e9499ca9ac572da527f1327e4b","permalink":"/draft-workshops/spatial_workshop_new/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/","section":"draft-workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials II","type":"book"},{"authors":null,"categories":null,"content":"\r\r Table of Contents\r What you will learn Program overview Courses in this program Meet your instructor FAQs  \r\rWhat you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program \rPython basics\rBuild a foundation in Python. \r\rVisualization\r\r\rStatistics\rIntroduction to statistics for data science. \r\rMeet your instructor Statistical Programs FAQs Are there prerequisites?\rThere are no prerequisites for the first course.\n How often do the courses run?\rContinuously, at your own pace.\n \rBegin the course\r\r\r","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"3455929a3b128d6d930e25725df43dc4","permalink":"/draft-workshops/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/draft-workshops/example/","section":"draft-workshops","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"📊 Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"\r Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()\r If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()\r If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation\r\u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting\r\u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling\r\u0026quot;SpATS\u0026quot;, # for fitting splines\r\u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;spaMM\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling\rinstall.packages(package_list)\rsapply(package_list, require, character.only = TRUE)\r \rPlease note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. The packages have all been successfully installed if after the sapply(package_list, require, character.only = TRUE) is run, the R output is \u0026ldquo;TRUE\u0026rdquo; for each package. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\r\r\rLibrary Information    package usage     dplyr, tidyr, standard data manipulation   purrr for repeat functions   nlme mixed linear models with options for spatial covariates   lme4 mixed linear models with crossed random effects   ggplot, desplot standard plotting packge and extension for plotting block outlines   SpATS spline-fitting   sp preparation of spatial objects   spdep Moran\u0026rsquo;s I test   gstat for fitting empirical variogram   spaMM fits Matern covariances structure for mixed linear models   emmeans extracts marginal means from linear model objects    ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"15841bfb95305ff58eb70b526b39d8a2","permalink":"/workshops/spatial-workshop/prep-work/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/prep-work/","section":"workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()\r If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()\r If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation\r\u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting\r\u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling\r\u0026quot;SpATS\u0026quot;, # for fitting splines\r\u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling\rinstall.packages(package_list)\rsapply(package_list, require, character.only = TRUE)\r Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please install these in advance of the workshop. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\nDownload files for workshop The following files are used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9a50ee534254c7b08141160344374595","permalink":"/draft-workshops/spatial_workshop_new/prep-work/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/prep-work/","section":"draft-workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n\r Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n\r Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n\r The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.  \r  Error terms are often correlated with each other, invalidating the downstream analysis  \r  high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics \r\ranother distracted boyfriend meme!\r Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"a31a33acbb99d2f0e16a5caf445f69ea","permalink":"/workshops/spatial-workshop/why-spatial/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/why-spatial/","section":"workshops","summary":"Agricultural field trials \r\rUniversity Research Farm\r The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":"\rThis workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n\r While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n\r Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R\r# load libraries\rlibrary(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme)\r# read in data and prepare it\rNin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row)\rNin_na \u0026lt;- filter(Nin, !is.na(rep))\r# make exploratory plot\rggplot(Nin, aes(x = row, y = col)) +\rgeom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) +\rgeom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) +\rlabs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() +\rtheme(axis.text = element_text(size = 12),\raxis.title = element_text(size = 14),\rlegend.title = element_text(size = 14),\rlegend.text = element_text(size = 12))\r## conduct moran's I test ##\r# set neighbors with convenience function for grids\rxy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals\rnin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep,\rdata = Nin, na.action = na.exclude)\rresid_lme \u0026lt;- residuals(nin.lme)\rnames(resid_lme) \u0026lt;- Nin$plot\r# two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude)\rmoran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)\r  SAS\r# read in data\rproc format;\rinvalue has_NA\r'NA' = .;\r;\rfilename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;;\rdata alliance;\rinfile NIN firstobs=2 delimiter=',';\rinformat yield has_NA.;\rinput entry $ rep $ yield col row;\rRow = 4.3*Row;\rCol = 1.2*Col;\rif yield=. then delete;\rrun;\r# heatmap\rproc sgplot data=alliance;\rHEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run;\r# linear mixed model\rproc mixed data=alliance;\rclass Rep Entry;\rmodel Yield = Entry / outp=residuals;\rrandom Rep;\rrun;\r# Moran's I\rproc variogram data=residuals plots(only)=moran ;\rcompute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ;\rcoordinates xc=row yc=col;\rvar resid;\rrun;\r  ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb80fd2ad66cac061f57b377665b0236","permalink":"/workshops/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/diagnosis/","section":"workshops","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis\r\r\rSpatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n\r 1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples?\rLists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive?\rYes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"5c1f7f41e31a8d8e08da3d62bb15dbae","permalink":"/draft-workshops/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/python/","section":"draft-workshops","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n\r\rExample Empirical Variogram\r  range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Exponential Variogram\r Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n\r\rTheoretical Gaussian Variogram\r Matérn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n\r\rEmpirical Matérn Variogram\r There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n\rFor more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.\r\r\rVariogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n\r Not all fitted variogram models are worthy\n\r\rVariogram gone bad\r Code for this section The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(gstat); library(spaMM)\r# set up spatial object\rNin_spatial \u0026lt;- Nin_na\rcoordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte\rclass(Nin_spatial)\r# establish max distance for variogram estimation\rmax_dist = 0.6*max(dist(coordinates(Nin_spatial)))\r# calculate empirical variogram\rresid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist,\rwidth = max_dist/15, # 15 is the number of bins\rdata = Nin_spatial)\rplot(resid_var1) # empirical variogram\r#Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram)\r# starting value for the nugget\rnugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much)\rNin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential\rNin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian\rNin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern\r# actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp)\rNin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau)\rNin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T)\rplot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;)\rplot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;)\rplot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;)\rattr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;)\r# parameters:\rNin_variofit_gau\rnugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error)\rrange \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points\rsill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance\r  SAS\r# calculate semivariance and compute empirical variogram\rproc variogram data=residuals plots(only)=(semivar);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rvar resid;\rrun;\r# fit models to the empirical variogram\rproc variogram data=residuals plots(only)=(fitplot);\rcoordinates xc=Col yc=Row;\rcompute lagd=1.2 maxlags=30;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"cd8303e2c52504ec61991e37816c126c","permalink":"/workshops/spatial-workshop/variograms/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/variograms/","section":"workshops","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"\r 1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful?\rLorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart\rimport plotly.express as px\rdata_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;)\rfig = px.bar(data_canada, x='year', y='pop')\rfig.show()\r ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"dd726e22044eb744e57cecdc3ff1c5b8","permalink":"/draft-workshops/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/visualization/","section":"draft-workshops","summary":"","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR\rlibrary(emmeans); library()\r# (nlme and gstat should already be loaded)\rlibrary(spaMM) # for running `corMatern()`\r# standard linear model\rnin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep,\rdata = Nin,\rna.action = na.exclude)\r# extract the esimated marginal means for variety\rpreds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;))\r# use information from the variogram fitting for intialising the parameters\rnugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill\r# initalise the covariance structure (from the nlme package)\rcor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F,\rtype = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)\r# update the rcbd model\rnin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus)\r# extract predictions for 'gen'\rpreds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;)\r# a similar procedure can be follow for other models\r# but we are going to take a shortcut and not specify the parameters\r# exponential\rcor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_exp \u0026lt;- update(nin_lme, corr = cor.exp)\rpreds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;))\r# Matern structure\rcor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F)\rnin_matern \u0026lt;- update(nin_lme, corr = cor.mat)\rpreds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)\r  SAS\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry ;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_RCBD_means;\rtitle1 'NIN data: RCBD';\rrun;\rproc mixed data=alliance maxiter=150;\rclass entry;\rmodel yield = entry /ddfm=kr;\rrepeated/subject=intercept type=sp(gau) (Row Col) local;\rparms (11) (22) (19);\rlsmeans entry/cl;\rods output LSMeans=NIN_Spatial_means;\rtitle1 'NIN data: Gaussian Spatial Adjustment';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"057408dc907a581ba7ba45d742b73dbd","permalink":"/workshops/spatial-workshop/correlated-error-models/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/correlated-error-models/","section":"workshops","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n\r 1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n\rThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.\r\r\rQuiz What is the parameter $\\mu$?\rThe parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"b99996333fbb4c63e90c11bb44f0be2b","permalink":"/draft-workshops/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/stats/","section":"draft-workshops","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n\r The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR\r# load libraries\rlibrary(lme4)\r# exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;)\rboxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;)\rboxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;)\r## row/column model ##\r# data prep\rNin$rowF = as.factor(Nin$row)\rNin$colF = as.factor(Nin$col)\r# specify model\rnin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF),\rdata = Nin, na.action = na.exclude)\r# extract random effects for row and column\rranef(nin_rc)\r# extract predictions\rnin_rc \u0026lt;- as.data.frame(emmeans(nin.rc, \u0026quot;gen\u0026quot;))\r  SAS\r# exploratory boxplots\rproc sgplot data=alliance;\rvbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\rproc sgplot data=alliance;\rvbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black);\rrun;\r# row/column model\rproc mixed data=alliance ;\rclass entry rep;\rmodel yield = entry row col/ddfm=kr;\rrandom rep;\rlsmeans entry/cl;\rods output LSMeans=NIN_row_col_means;\rtitle1 'NIN data: RCBD';\rrun;\r  Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR\rnin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20),\rdegree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0))\rpreds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)\r  SAS\rproc glimmix data=alliance ;\rclass entry rep;\reffect sp_r = spline(row col);\rmodel yield = entry sp_r/ddfm=kr;\rrandom row col/type=rsmooth;\rlsmeans entry/cl;\rods output LSMeans=NIN_smooth_means;\rtitle1 'NIN data: RCBD';\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"b280dc26c3a20f98ee5ed0cf8a9403e6","permalink":"/workshops/spatial-workshop/trend-modelling/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/trend-modelling/","section":"workshops","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)  \rComparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.\r\r\rUnfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R\rlibrary(tidyr)\r# remove some objects we don't need (and will interfere with downstream processes)\rrm(nin_variofit, nin_vgm)\rrm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect)\r# assemble objects into a list\rnlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern)\rnames(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;)\r# extract log likelihood, AIC, BIC\rdata.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC),\rBIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglik))\r# (higher is better for loglik, lower is better for AIC and BIC)\r# compare post-hoc power\r# conduct ANOVA\ranovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]})\r# bind all the output together\ra \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;, \u0026quot;row-col\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a)\ra\r## compare precision of estimates\rall.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;))\rerrors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;)\rpred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors))\rerror_df \u0026lt;- bind_cols(errors)\rcolnames(error_df) \u0026lt;- pred.names\rboxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;)\r# compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;)\rpreds_df \u0026lt;- bind_cols(preds)\rcolnames(preds_df) \u0026lt;- pred.names\rpreds_df$gen \u0026lt;- preds_exp$gen\r# plot changes in rank\rlev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;)\rpivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) +\rgeom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) +\rgeom_line() +\rylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal()\r  SAS\rdata NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_RCBD_means;\rRCB_est = estimate;\rRCB_se = stderr;\rrun;\rdata NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df);\rset NIN_Spatial_means;\rSp_est = estimate;\rSp_se = stderr;\rrun;\rproc sort data=NIN_RCBD_means;\rby entry;\rrun;\rproc sort data=NIN_Spatial_means;\rby entry;\rrun;\rdata compare;\rmerge NIN_RCBD_means NIN_Spatial_means;\rby entry;\rrun;\rproc rank data=compare out=compare descending;\rvar RCB_est Sp_est;\rranks RCB_Rank Sp_Rank;\rrun;\rproc sort data=compare;\rby Sp_rank;\rrun;\rproc print data=compare(obs=15);\rvar entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank;\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"006196c4ae04f5e62ccfd0b0e4974438","permalink":"/workshops/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/model-comparison/","section":"workshops","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in Burgueño et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (‘name’) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR\r# (if not already loaded)\rlibrary(dplyr); library(nlme); library(ggplot2)\rlibrary(gstat); library(sp)\r# read in data\raug_data_origin \u0026lt;- read.csv(\u0026quot;data/augmented_lind.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed\rmutate(yieldkg = yieldg/1000) # to prevent overflow\r# summarise the genoytypic data by checks/not checks\rgen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when(\rcounts \u0026gt; 1 ~ \u0026quot;check\u0026quot;,\rcounts == 1 ~ \u0026quot;unrep\u0026quot;))\r# need info on just the checks\rchecks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes\rgen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when(\rdelta == \u0026quot;check\u0026quot; ~ gamma,\rdelta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when(\rdelta == \u0026quot;unrep\u0026quot; ~ gamma,\rdelta == \u0026quot;check\u0026quot; ~ gamma))\r# merge original data set with info on treatment levels\raug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldkg, yieldg) %\u0026gt;%\rmutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) ## modelling\raug1 \u0026lt;- lme(fixed = yieldg ~ tau,\rrandom = ~ 1|tau/beta,\rdata = aug_data, na.action = na.exclude)\r# extract residuals\raug_data$res \u0026lt;- residuals(aug1)\r# plot residual chloroepleth map:\rggplot(aug_data, aes(y = row, x = col)) +\rgeom_tile(aes(fill = res)) +\rscale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) +\rscale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) +\rscale_y_continuous(breaks = 1:max(aug_data$col)) +\rcoord_equal() +\rtheme_void() # add spatial covariates\raug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res))\rcoordinates(aug_spatial) \u0026lt;- ~ col + row\rmax_dist = 0.5*max(dist(coordinates(aug_spatial)))\raug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist,\rwidth = max_dist/10, data = aug_spatial)\r# optional to run: nugget_start \u0026lt;- min(aug_vario$gamma)\raug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start)\raug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm)\rplot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;)\rcor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F,\rtype = \u0026quot;exponential\u0026quot;)\raug1_sp \u0026lt;- update(aug1, corr = cor_exp)\r# spatial parameters:\raug1_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug1_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r##### OR #######\r# another formulation\r# delta estimates effects of replicated versus unreplicated genotypes\r# gamma estimates the effecs of all genotypes evaluated in the trial\raug2 \u0026lt;- lme(fixed = yieldkg ~ delta,\rrandom = ~ 1|delta/gamma,\rdata = aug_data, na.action = na.exclude)\raug2_sp \u0026lt;- update(aug2, corr = cor_exp)\r# spatial parameters:\raug2_sp$modelStruct$corStruct\r# extract BLUPs for unreplicated lines:\raug_blups2 \u0026lt;- ranef(aug2_sp)$gamma %\u0026gt;% rename(yieldg = '(Intercept)')\r# look at variance components\rVarCorr(aug1_sp)\r  SAS\rfilename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;;\rPROC IMPORT OUT= WORK.augmented\rDATAFILE= AUG\rDBMS=CSV REPLACE;\rGETNAMES=YES;\rDATAROW=2; RUN;\rdata augmented;\rset augmented;\rif yieldg = 999999 or yieldg=. then delete; /* Remove missing values */\rprow=prow*11.7; /*convert row and column indices to feet */\rpcol=pcol*5.5;\rrun;\rproc freq noprint data=augmented;\rtables name/out=controls;\rrun;\rdata controls;\rset controls;\rif count \u0026gt;1;\rrun;\rproc sort data=controls;\rby name;\rrun;\rproc sort data=augmented;\rby name;\rrun;\rdata augmented;\rmerge augmented controls;\rby name;\rif count=. then d2=2; /* Unreplicated */\relse d2=1; /* Replicated */\ryieldkg=yieldg/1000;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2/noint outp=residuals ddf=229 229;\rlsmeans d2;\r*lsmeans name(d2)/slice = d2;\rrun;\rproc sgplot data=residuals;\rHEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map';\rrun;\rproc variogram data=residuals plots(only)=(fitplot);\rwhere yieldkg ^= .;\rcoordinates xc=pcol yc=pRow;\rcompute lagd=6.6 maxlags=25;\rmodel form=auto(mlist=(gau, exp, pow, sph) nest=1);\rvar resid;\rrun;\rPROC mixed data=augmented;\rclass name d2;\rmodel yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229;\rlsmeans d2;\rrepeated/subject=intercept type=sp(pow)(prow pcol) local;\rods output SolutionR =parms;\rparms (0.074) (0.0051)(0.475) ;\r*lsmeans name(d2)/slice = d2; # alot of output!!\rrun;\r  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"561372141df0a39b09e92d9d036e8b87","permalink":"/workshops/spatial-workshop/augmented/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/augmented/","section":"workshops","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm Matérn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive \r   Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"c08d0a8717c5c9f76fe45f925c14f37a","permalink":"/workshops/spatial-workshop/conclusion/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/conclusion/","section":"workshops","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility. The result is that it is easy to misspecify a model or make other mistakes. The information below is intended to serve as a guide through the R ANOVA wilderness.\nPackages Needed There are many packages to load. Here is a (very) brief summary of what each package does.\n   Package Purpose     car Anova() function to extract type III \u0026amp; II sums of squares   lme4 mixed models   nlme mixed models, non-linear models, alternative covariance structures   emmeans for extracting least squares means and contrasts   lmer test improved summary functions of lmer objects   dplyr data organization   forcats for managing categorical data   agridat has many agricultural data sets   agricolae has options for many common agricultural experimental designs    library(car)\rlibrary(lme4)\rlibrary(nlme)\rlibrary(emmeans) #in older version of R, you may need to install \u0026quot;multcompView\u0026quot; separately to access full functionality of the emmeans package\rlibrary(lmerTest)\rlibrary(dplyr)\rlibrary(forcats)\rlibrary(agridat)\r Formula Notation There are some consistent features across ANOVA methods in R. Formula notation is often used in the R syntax for ANOVA functions. It looks like this: $Y ~ X, where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).\nmy_formula \u0026lt;- formula(Y ~ treatment1 + treatment2)\rclass(my_formula)\r ## [1] \u0026quot;formula\u0026quot;\r my_formula\r ## Y ~ treatment1 + treatment2\r Often the independent variables (i,e, the treatments or the x variables) are expected to be factors, another type of R object:\nmy_var \u0026lt;- c(rep(\u0026quot;low\u0026quot;,5), rep(\u0026quot;high\u0026quot;, 5))\rclass(my_var) #check what variable type it is\r ## [1] \u0026quot;character\u0026quot;\r Although \u0026ldquo;my_var\u0026rdquo; is not type factor, it is type \u0026ldquo;character\u0026rdquo; which is automatically converted to a factor in lm(), lmer(), lme() and many other linear modeling functions. There are some packages that do not follow this convention, so it\u0026rsquo;s helpful to read function documentation, especially if you get unexpected results.\nVariables like year, which are often imported as a number or integer, do need to be converted to a factor or a character variable prior to analysis. Otherwise, they will be interpreted as a number in linear modelling and treated as a covariate, e.g, 2020 would be 2,020. Here is one way to do this conversion:\nmy_factor \u0026lt;- as.character(my_var) # convert to a character\rclass(my_factor) # check variable type to confirm\r ## [1] \u0026quot;character\u0026quot;\r my_factor \u0026lt;- as.factor(my_var) # convert to a factor\rclass(my_factor) # check variable type again to confirm\r ## [1] \u0026quot;factor\u0026quot;\r The choice of whether to convert a categorical variable to a character or factor depends on the comfort of the user with these structures and package requirements.\nSometimes, there is a need to alter the order of treatment levels (that is, how R sees those levels). The default behavior of R is to order categorical levels alphanumerically. However, sometimes there are reasons you may not want this (for example, you want to set a particular reference level as the first factor level).\nBelow is one example of how to reorder factor levels in a variable. The first step is to see which levels are present in the variable and how they are ordered:\nlevels(my_factor)  ## [1] \u0026quot;high\u0026quot; \u0026quot;low\u0026quot;\r Once that is known, you can use that information to manually set the levels and their order. Note that spelling of each level much match what is actually present in the variable. Unmatched levels in the variable will be set to NA automatically by R in the following step.\nmy_factor \u0026lt;- factor(my_factor, levels = c(\u0026quot;low\u0026quot;, \u0026quot;high\u0026quot;)) levels(my_factor) # check the new ordering\r ## [1] \u0026quot;low\u0026quot; \u0026quot;high\u0026quot;\r Knowing the level order is important because in the implementation of ANOVA in R, the first level is treated as the reference level. Manipulating factors is a challenging task in R. The package forcats contains a collection of accessory functions for managing factors (\u0026ldquo;forcats\u0026rdquo; = for categories). The tutorial uses the forcats function fct_drop().\nMore on formulas:\nThe formula first shown, Y ~ treatment1 + treatment2, includes main effects only. Other formula notation includes the symbols : and *, indicating notation for interaction only and main effects plus the interaction term, respectively.\nformula(Y ~ treatment1:treatment2) # interaction only\r ## Y ~ treatment1:treatment2\r formula(Y ~ treatment1*treatment2) # interaction plus main effects\r ## Y ~ treatment1 * treatment2\r These two formulas are equivalent:\nformula(Y ~ treatment1 + treatment2 + treatment1:treatment2) formula(Y ~ treatment1*treatment2)  Perhaps you can see from these examples that formulas are a really just a collections of characters (that is, a string) and exist independent of any data set. Later, we will need to link these formulas to a data set to actually construct a linear model and conduct statistical analysis.\nANOVA for fixed effects models Here is a function for reporting the number of missing data in each column. There are other ways to do this, but I find this function easy enough to write and use.\ncount_na \u0026lt;- function(df) {\rapply(df, 2, function(x) sum(is.na(x))) }\r Completely Randomised design First, load the data set \u0026ldquo;warpbreaks\u0026rdquo; (a data set from base R). This is an old data set with variables for wool type (A and B) and tension on the loom (L, M or H). The response variable is \u0026ldquo;breaks\u0026rdquo;, the number of times the wool thread breaks on industrial looms.\nI always like to have a quick look at the data before running any statistical tests. So, here we go:\ndata(warpbreaks)\rcount_na(warpbreaks)\r ## breaks wool tension ## 0 0 0\r str(warpbreaks)\r ## 'data.frame':\t54 obs. of 3 variables:\r## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ...\r## $ wool : Factor w/ 2 levels \u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ...\r## $ tension: Factor w/ 3 levels \u0026quot;L\u0026quot;,\u0026quot;M\u0026quot;,\u0026quot;H\u0026quot;: 1 1 1 1 1 1 1 1 1 2 ...\r warpbreaks$wool \u0026lt;- factor(warpbreaks$wool, levels = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;))\rtable(warpbreaks$wool, warpbreaks$tension)\r ## ## L M H\r## A 9 9 9\r## B 9 9 9\r## C 0 0 0\r hist(warpbreaks$breaks, col = \u0026quot;gold\u0026quot;)\r boxplot(breaks ~ wool, data = warpbreaks, col = \u0026quot;orangered\u0026quot;)\r boxplot(breaks ~ tension, data = warpbreaks, col = \u0026quot;chartreuse\u0026quot;) #why not have colorful plots?\r This data set has 2 treatments. We don\u0026rsquo;t know if there is an interaction between the variables, yet. A good start is to run a linear model using lm() function, the linear regression function. As a reminder, ANOVA is a special case of the linear regression model where the predictors (the experimental treatments) are categories rather than a continuous variable.\n# run standard linear model for main effects only\rlm.mod1 \u0026lt;- lm(breaks ~ wool + tension, data = warpbreaks)\r# extract type III sums of squares from that model\rAnova(lm.mod1, type = \u0026quot;3\u0026quot;)  ## Anova Table (Type III tests)\r## ## Response: breaks\r## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 20827.0 1 154.3226 \u0026lt; 2.2e-16 ***\r## wool 450.7 1 3.3393 0.073614 . ## tension 2034.3 2 7.5367 0.001378 ** ## Residuals 6747.9 50 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r # run a linear model with main effects and interactions\rlm.mod2 \u0026lt;- lm(breaks ~ wool*tension, data = warpbreaks)\r# ...and type III sums of squares Anova(lm.mod2, type = \u0026quot;III\u0026quot;)\r ## Anova Table (Type III tests)\r## ## Response: breaks\r## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 17866.8 1 149.2757 2.426e-16 ***\r## wool 1200.5 1 10.0301 0.0026768 ** ## tension 2468.5 2 10.3121 0.0001881 ***\r## wool:tension 1002.8 2 4.1891 0.0210442 * ## Residuals 5745.1 48 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r FYI\nfunctions only shown as an example and not actually run.\n# this function runs type II sums of squares: Anova(lm.mod2, type = \u0026quot;II\u0026quot;)\r# this function runs type I sums of squares: anova(lm.mod2)\r A few comments on types of sums of squares: \nAs a reminder, the type of sums of squares used in statistical tests can impact the results and subsequent interpretation. Type I, sums of squares tests for statistical significance by adding one variable to the model at time (and hence is also called \u0026ldquo;sequential\u0026rdquo;). If there is any unbalance in the treatments, the type I sums of squares are dependent on the order variables are added to the model and hence is often not the best choice for many agricultural experiment. Type III sums of squares (also called \u0026ldquo;partial\u0026rdquo; or \u0026ldquo;marginal\u0026rdquo;) evaluates the statistical significance of variable or interaction, assuming that the other variables are in the model. This is a decent default option. The last option is Type II sums of squares, which is the best option when you are sure there are no interactions between variables. If there is complete balance among treatments (each treatment is observed the same number of times with no missing data), then there is no need to concern yourself with these different types of sums of squares.\nCompare Models # conduct an F test comparing the models\ranova(lm.mod1, lm.mod2)\r ## Analysis of Variance Table\r## ## Model 1: breaks ~ wool + tension\r## Model 2: breaks ~ wool * tension\r## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 50 6747.9 ## 2 48 5745.1 2 1002.8 4.1891 0.02104 *\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r # also, consider doing a stepwise approach for finding the best model:\rstep(lm.mod2)\r ## Start: AIC=264.02\r## breaks ~ wool * tension\r## ## Df Sum of Sq RSS AIC\r## \u0026lt;none\u0026gt; 5745.1 264.02\r## - wool:tension 2 1002.8 6747.9 268.71\r ## ## Call:\r## lm(formula = breaks ~ wool * tension, data = warpbreaks)\r## ## Coefficients:\r## (Intercept) woolB tensionM tensionH woolB:tensionM ## 44.56 -16.33 -20.56 -20.00 21.11 ## woolB:tensionH ## 10.56\r Model diagnostics plot(lm.mod2) #this will produce 4 plots of residuals\r shapiro.test(resid(lm.mod2)) #standard shapiro-wilk test.  ## ## Shapiro-Wilk normality test\r## ## data: resid(lm.mod2)\r## W = 0.98686, p-value = 0.8162\r # this variable could be analyzed with a log-normal model instead\r Least squares means \u0026amp; contrasts The emmeans package is a flexible package for extracting the estimated marginal means (in SAS, the \u0026ldquo;least squares means\u0026rdquo;) from different linear models. It is compatible with a large number of R linear modelling packages.\nHere is some code for extracting the marginal means and conducting contrasts.\n# extract least squares means for 'tension'\r(lsm \u0026lt;- emmeans(lm.mod2, ~ tension))\r ## NOTE: Results may be misleading due to involvement in interactions\r ## tension emmean SE df lower.CL upper.CL\r## L 36.4 2.58 48 31.2 41.6\r## M 26.4 2.58 48 21.2 31.6\r## H 21.7 2.58 48 16.5 26.9\r## ## Results are averaged over the levels of: wool ## Confidence level used: 0.95\r emmeans(lm.mod2, \u0026quot;wool\u0026quot;)\r ## NOTE: Results may be misleading due to involvement in interactions\r ## wool emmean SE df lower.CL upper.CL\r## A 31.0 2.11 48 26.8 35.3\r## B 25.3 2.11 48 21.0 29.5\r## ## Results are averaged over the levels of: tension ## Confidence level used: 0.95\r All pairwise comparisons within each level of tension:\ncontrast(lsm, \u0026quot;pairwise\u0026quot;)\r ## contrast estimate SE df t.ratio p.value\r## L - M 10.00 3.65 48 2.742 0.0229\r## L - H 14.72 3.65 48 4.037 0.0006\r## M - H 4.72 3.65 48 1.295 0.4049\r## ## Results are averaged over the levels of: wool ## P value adjustment: tukey method for comparing a family of 3 estimates\r Conduct custom contrasts comparing \u0026lsquo;Low\u0026rsquo; tension versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;High\u0026rsquo; and \u0026lsquo;High\u0026rsquo; versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;Low\u0026rsquo;.\n# see the order of each level in a factor\rlevels(warpbreaks$tension)\r ## [1] \u0026quot;L\u0026quot; \u0026quot;M\u0026quot; \u0026quot;H\u0026quot;\r # construct a list of constructs # each item must be same length as the the number of levels present in the variable tension\r# use numbers and fracions to indicate the contrasting levels\r# the numbers must sum to zero cList \u0026lt;- list(LvMH = c(1, -0.5, -0.5), # low vs high + medium\rHvLM = c(0.5, 0.5, -1)) # high vs low + medium\r# check that each contrast sums to zero\rlapply(cList, sum)\r ## $LvMH\r## [1] 0\r## ## $HvLM\r## [1] 0\r # perform custom contrast and include a Bonferroni adjustment\rsummary(contrast(lsm, cList, adjust = \u0026quot;bonferroni\u0026quot;))\r ## contrast estimate SE df t.ratio p.value\r## LvMH 12.36 3.16 48 3.914 0.0006\r## HvLM 9.72 3.16 48 3.078 0.0069\r## ## Results are averaged over the levels of: wool ## P value adjustment: bonferroni method for 2 tests\r Randomised Complete Block Design (RCBD) - fixed effects model This example uses rapeseed yield data from multiple locations, years and cultivars. Within a single location or year, the replication is often balanced.\nLoad Data and examine:\ndata(shafii.rapeseed) # from the 'agridat' package\rrapeseed1987 \u0026lt;- shafii.rapeseed %\u0026gt;% filter(year == 87) %\u0026gt;% mutate(block = fct_drop(rep), Cv = fct_drop(gen), loc = fct_drop(loc))\rstr(rapeseed1987)\r ## 'data.frame':\t216 obs. of 7 variables:\r## $ year : int 87 87 87 87 87 87 87 87 87 87 ...\r## $ loc : Factor w/ 9 levels \u0026quot;GGA\u0026quot;,\u0026quot;ID\u0026quot;,\u0026quot;MT\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ...\r## $ rep : Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ...\r## $ gen : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ...\r## $ yield: num 961 1329 1781 1698 1605 ...\r## $ block: Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ...\r## $ Cv : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ...\r count_na(rapeseed1987)\r ## year loc rep gen yield block Cv ## 0 0 0 0 0 0 0\r table(rapeseed1987$Cv, rapeseed1987$loc) #experiment has 1 rep per block  ## ## GGA ID MT NC OR SC TGA TX WA\r## Bienvenu 4 4 4 4 4 4 4 4 4\r## Bridger 4 4 4 4 4 4 4 4 4\r## Cascade 4 4 4 4 4 4 4 4 4\r## Dwarf 4 4 4 4 4 4 4 4 4\r## Glacier 4 4 4 4 4 4 4 4 4\r## Jet 4 4 4 4 4 4 4 4 4\r hist(rapeseed1987$yield, col = \u0026quot;gold\u0026quot;)\r boxplot(yield ~ Cv, data = rapeseed1987, col = \u0026quot;orangered\u0026quot;)\r boxplot(yield ~ loc, data = rapeseed1987, col = \u0026quot;chartreuse\u0026quot;)\r Analyse experiment:\n# for this example, the analysis will only be done for a single year\r# block is nested within location\r# if each block had a unique name, 'Error(block)' would suffce\rshaf.aov \u0026lt;- aov(yield ~ Cv*loc + Error(block), data = rapeseed1987)\rsummary(shaf.aov)\r ## ## Error: block\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F)\r## Residuals 3 336565 112188 ## ## Error: Within\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Cv 5 3203992 640798 2.645 0.025111 * ## loc 8 318197192 39774649 164.165 \u0026lt; 2e-16 ***\r## Cv:loc 40 22707425 567686 2.343 0.000103 ***\r## Residuals 159 38523267 242285 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r emmeans(shaf.aov, ~ Cv | loc)\r ## Note: re-fitting model with sum-to-zero contrasts\r ## loc = GGA:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 1442 245 161 959 1926\r## Bridger 1363 245 161 880 1847\r## Cascade 1505 245 161 1021 1988\r## Dwarf 1295 245 161 811 1779\r## Glacier 1681 245 161 1197 2164\r## Jet 1091 245 161 607 1575\r## ## loc = ID:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 1242 245 161 759 1726\r## Bridger 947 245 161 463 1430\r## Cascade 773 245 161 290 1257\r## Dwarf 932 245 161 448 1415\r## Glacier 1111 245 161 627 1595\r## Jet 1064 245 161 580 1548\r## ## loc = MT:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 2616 245 161 2132 3100\r## Bridger 2828 245 161 2345 3312\r## Cascade 2916 245 161 2433 3400\r## Dwarf 3452 245 161 2968 3935\r## Glacier 3307 245 161 2823 3790\r## Jet 3660 245 161 3177 4144\r## ## loc = NC:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 1001 245 161 517 1485\r## Bridger 1064 245 161 581 1548\r## Cascade 745 245 161 262 1229\r## Dwarf 1014 245 161 530 1497\r## Glacier 1229 245 161 746 1713\r## Jet 1674 245 161 1190 2157\r## ## loc = OR:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 4556 245 161 4072 5039\r## Bridger 2530 245 161 2046 3013\r## Cascade 3336 245 161 2852 3819\r## Dwarf 3932 245 161 3448 4415\r## Glacier 4185 245 161 3702 4669\r## Jet 3220 245 161 2736 3703\r## ## loc = SC:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 2500 245 161 2016 2983\r## Bridger 2705 245 161 2221 3189\r## Cascade 2119 245 161 1635 2602\r## Dwarf 1894 245 161 1410 2377\r## Glacier 2717 245 161 2234 3201\r## Jet 2833 245 161 2349 3316\r## ## loc = TGA:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 1258 245 161 774 1741\r## Bridger 1868 245 161 1384 2351\r## Cascade 1708 245 161 1224 2191\r## Dwarf 873 245 161 389 1356\r## Glacier 1453 245 161 970 1937\r## Jet 954 245 161 470 1438\r## ## loc = TX:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 838 245 161 354 1322\r## Bridger 1069 245 161 585 1553\r## Cascade 735 245 161 251 1218\r## Dwarf 988 245 161 505 1472\r## Glacier 952 245 161 468 1435\r## Jet 1408 245 161 925 1892\r## ## loc = WA:\r## Cv emmean SE df lower.CL upper.CL\r## Bienvenu 4375 245 161 3891 4859\r## Bridger 4604 245 161 4120 5087\r## Cascade 4464 245 161 3981 4948\r## Dwarf 3974 245 161 3490 4458\r## Glacier 4740 245 161 4256 5224\r## Jet 4344 245 161 3861 4828\r## ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95\r ANOVA for mixed models (models with random and fixed effects)\nRandom effects are often those treatments levels drawn from a large population of possible treatment levels and there is interest in understanding the distribution and variance of that population. This in contrast to fixed effects, where the inferences are restricted to the treatment levels tested.\nBlocking factors and Year are often considered random factors because a researcher is not interested in particular years or a blocking factor. When there is unbalanced replication, the variance components should be estimated with maximum likelihood or REML, which implies use of the packages \u0026ldquo;lmer\u0026rdquo; and/or \u0026ldquo;nlme\u0026rdquo;.\nRandomised Complete Block Design (RCBD) - mixed effects The \u0026ldquo;shafii.rapeseed\u0026rdquo; data set will be used for this section.\nAnalyse experiment using a mixed model:\nThis uses the function lme() from the package \u0026ldquo;nlme\u0026rdquo;. Functionally, it is very similar to calling lme4::lmer(). The degrees of freedom are different (lmer() is using Satterthwaite\u0026rsquo;s approximation), but the p-values are the same.\n# turn year into the factor \u0026quot;Year\u0026quot;\rshafii.rapeseed$Year \u0026lt;- as.factor(shafii.rapeseed$year)\r# create a blocking variable that is unique for each location-by-year combination\r# so R doesn't conflate \u0026quot;R1\u0026quot; from one location/year with another location/year\rshafii.rapeseed$Rep \u0026lt;- as.factor(paste(shafii.rapeseed$loc, shafii.rapeseed$year, shafii.rapeseed$rep, sep = \u0026quot;_\u0026quot;))\rshaf.lme \u0026lt;- lme(fixed = yield ~ gen*loc + Year,\rrandom = ~ 1|Rep,\rdata = shafii.rapeseed, method = \u0026quot;REML\u0026quot;)\r# view sum of squares table # when anova() is called for an lme object, the function called is actually anova.lme()\ranova(shaf.lme, type = \u0026quot;marginal\u0026quot;) # \u0026quot;marginal\u0026quot; is equivalent to type III sums of squares\r ## numDF denDF F-value p-value\r## (Intercept) 1 470 16.204597 0.0001\r## gen 5 470 1.092341 0.3637\r## loc 13 92 13.074492 \u0026lt;.0001\r## Year 2 92 2.035054 0.1365\r## gen:loc 65 470 2.575753 \u0026lt;.0001\r Anova(shaf.lme, type = \u0026quot;3\u0026quot;)\r ## Analysis of Deviance Table (Type III tests)\r## ## Response: yield\r## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 ***\r## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 ***\r## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r # FYI: use \u0026quot;anova(model.lme)\u0026quot; for type I sums of squares\r# lmer notation\rshaf.lmer \u0026lt;- lmer(yield ~ gen*loc + Year + (1|Rep),\rdata = shafii.rapeseed, REML = T)\ranova(shaf.lmer, type = \u0026quot;marginal\u0026quot;)\r ## Marginal Analysis of Variance Table with Satterthwaite's method\r## Sum Sq Mean Sq NumDF DenDF F value Pr(\u0026gt;F) ## gen 1860586 372117 5 470.00 1.0923 0.3637 ## loc 57901484 4453960 13 159.37 13.0745 \u0026lt; 2.2e-16 ***\r## Year 1386524 693262 2 92.00 2.0351 0.1365 ## gen:loc 57034691 877457 65 470.00 2.5758 5.499e-09 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r Anova(shaf.lmer, type = \u0026quot;3\u0026quot;)\r ## Analysis of Deviance Table (Type III Wald chisquare tests)\r## ## Response: yield\r## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 ***\r## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 ***\r## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r Diagnostics, model building plot(shaf.lme)\r qqnorm(shaf.lme, abline = c(0, 1))\r Least squares means # for cultivar (lme.means.cv \u0026lt;- emmeans(shaf.lme, \u0026quot;gen\u0026quot;))\r ## NOTE: Results may be misleading due to involvement in interactions\r ## gen emmean SE df lower.CL upper.CL\r## Bienvenu 2432 112 92 2211 2654\r## Bridger 2314 112 92 2092 2536\r## Cascade 2184 112 92 1962 2406\r## Dwarf 2308 112 92 2087 2530\r## Glacier 2463 112 92 2242 2685\r## Jet 2304 112 92 2082 2525\r## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95\r # for location\r(lme.means.loc \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;))\r ## NOTE: Results may be misleading due to involvement in interactions\r ## loc emmean SE df lower.CL upper.CL\r## GGA 1682 329 92 1030 2335\r## ID 4217 261 92 3698 4736\r## KS 1120 476 92 174 2066\r## MS 2204 476 92 1258 3150\r## MT 3339 474 92 2398 4280\r## NC 1328 329 92 676 1981\r## NY 3139 476 92 2193 4085\r## OR 3292 329 92 2640 3945\r## SC 1819 261 92 1300 2338\r## TGA 1028 261 92 509 1547\r## TN 2543 476 92 1597 3490\r## TX 827 329 92 174 1479\r## VA 2282 328 92 1631 2932\r## WA 3861 261 92 3342 4380\r## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95\r # for cultivar means within each location\rlme.means.int \u0026lt;- emmeans(shaf.lme, ~ gen | loc + Year)\r# this code would produce location means within each cultivar # emmeans(model.lme, ~ loc | gen))\r# also: # emmeans(model.lme, ~ loc | gen)) provides the same estimates as 'emmeans(model.lme, ~ gen | loc))'\r Pairwise Contrasts: # all pairwise\rpairs(lme.means.cv)\r ## contrast estimate SE df t.ratio p.value\r## Bienvenu - Bridger 118.57 87.6 470 1.353 0.7548\r## Bienvenu - Cascade 248.34 87.6 470 2.834 0.0539\r## Bienvenu - Dwarf 124.11 87.6 470 1.417 0.7170\r## Bienvenu - Glacier -31.00 87.6 470 -0.354 0.9993\r## Bienvenu - Jet 128.70 87.6 470 1.469 0.6843\r## Bridger - Cascade 129.77 87.6 470 1.481 0.6765\r## Bridger - Dwarf 5.54 87.6 470 0.063 1.0000\r## Bridger - Glacier -149.57 87.6 470 -1.707 0.5277\r## Bridger - Jet 10.13 87.6 470 0.116 1.0000\r## Cascade - Dwarf -124.23 87.6 470 -1.418 0.7161\r## Cascade - Glacier -279.34 87.6 470 -3.188 0.0190\r## Cascade - Jet -119.64 87.6 470 -1.366 0.7477\r## Dwarf - Glacier -155.10 87.6 470 -1.770 0.4861\r## Dwarf - Jet 4.59 87.6 470 0.052 1.0000\r## Glacier - Jet 159.70 87.6 470 1.823 0.4521\r## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: tukey method for comparing a family of 6 estimates\r # plot results\rplot(lme.means.cv, comparison = T)\r plot(lme.means.loc, comparison = T, horizontal = F) # rotate plots to vertical position\r ## Warning: Comparison discrepancy in group \u0026quot;1\u0026quot;, GGA - OR:\r## Target overlap = 0.0083, overlap on graph = -0.0111\r # blue bars = lsmeans confidence 95% confidence intervals\r# red arrows. pairwise differences (overlapping arrows = not significantly different)\r For those who want the letters assigned to treatments based on all pairwise comparisons, it\u0026rsquo;s an unwieldy road:\nlibrary(multcomp) # this will need to be installed if you do not already have it\rtukey \u0026lt;- glht(shaf.lme, linfct = mcp(loc = \u0026quot;Tukey\u0026quot;))\r### extract information\rcld_tukey \u0026lt;- cld(tukey)\rprint(cld_tukey)\r ## GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ac\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;bc\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;bc\u0026quot;\r Interaction plots can also be done:\n(but, it gets unwieldy)\nplot(lme.means.int, comparison = T, adjust = \u0026quot;tukey\u0026quot;)\r Other pre-set contrasts # compare to a control, e.g. \u0026quot;Bridger\u0026quot;\rlevels(shafii.rapeseed$gen)\r ## [1] \u0026quot;Bienvenu\u0026quot; \u0026quot;Bridger\u0026quot; \u0026quot;Cascade\u0026quot; \u0026quot;Dwarf\u0026quot; \u0026quot;Glacier\u0026quot; \u0026quot;Jet\u0026quot;\r # Bridger is listed in position 2 of the factor 'shafii.rapeseed$gen'\r# so '2' is set as the reference level in the following contrast statement: # \u0026quot;trt.vs.ctrlk\u0026quot; (treatment versus control treatment k) is a specific option to compare all treatment levels to a user-defined level\r# by default, it will use the last level as the reference level\rcontrast(lme.means.cv, \u0026quot;trt.vs.ctrlk\u0026quot;, ref = 2)  ## contrast estimate SE df t.ratio p.value\r## Bienvenu - Bridger 118.57 87.6 470 1.353 0.5118\r## Cascade - Bridger -129.77 87.6 470 -1.481 0.4315\r## Dwarf - Bridger -5.54 87.6 470 -0.063 0.9998\r## Glacier - Bridger 149.57 87.6 470 1.707 0.3034\r## Jet - Bridger -10.13 87.6 470 -0.116 0.9990\r## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: dunnettx method for 5 tests\r Search ?contrast.emmGrid to see full list of options for preset contrasts.\nCustom contrasts # example: contrast Western locations versus Southern locations\r# first, find out what levels are present\runique(shafii.rapeseed$loc)\r ## [1] GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## Levels: GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA\r # next create a contrast list # this is a list of coefficients as long your list of treatment levels\r# indicating what coefficients to give each treatment level\r# in this example, levels \u0026quot;ID\u0026quot;, \u0026quot;MT\u0026quot;, \u0026quot;OR\u0026quot;, and \u0026quot;WA\u0026quot; are contrasted versus\r# \u0026quot;NC\u0026quot;, \u0026quot;SC\u0026quot;, \u0026quot;MS\u0026quot;, \u0026quot;TN\u0026quot;, \u0026quot;TX\u0026quot; and \u0026quot;VA\u0026quot;\rcList \u0026lt;- list(West_V_South = c(0, 1/4, 0, -1/6, 1/4, -1/6, 0, 1/4, -1/6, 0, -1/6, -1/6, -1/6, 1/4))\r# check that each contrast sums to zero:\rlapply(cList, sum)\r ## $West_V_South\r## [1] 5.551115e-17\r lme.means.loc2 \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;, contr = cList)\r ## NOTE: Results may be misleading due to involvement in interactions\r summary(lme.means.loc2)\r ## $emmeans\r## loc emmean SE df lower.CL upper.CL\r## GGA 1682 329 92 1030 2335\r## ID 4217 261 92 3698 4736\r## KS 1120 476 92 174 2066\r## MS 2204 476 92 1258 3150\r## MT 3339 474 92 2398 4280\r## NC 1328 329 92 676 1981\r## NY 3139 476 92 2193 4085\r## OR 3292 329 92 2640 3945\r## SC 1819 261 92 1300 2338\r## TGA 1028 261 92 509 1547\r## TN 2543 476 92 1597 3490\r## TX 827 329 92 174 1479\r## VA 2282 328 92 1631 2932\r## WA 3861 261 92 3342 4380\r## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1843 233 92 7.910 \u0026lt;.0001\r## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment\r # same contrast can also be done within each level of 'gen':\remmeans(shaf.lme, ~ loc | gen, contr = cList)\r ## $emmeans\r## gen = Bienvenu:\r## loc emmean SE df lower.CL upper.CL\r## GGA 1785 379 92 1032.31 2537\r## ID 4742 303 92 4140.13 5345\r## KS 1179 546 92 94.60 2263\r## MS 2455 546 92 1371.47 3539\r## MT 2825 544 92 1745.38 3904\r## NC 1330 379 92 577.36 2082\r## NY 2934 546 92 1849.69 4018\r## OR 4118 379 92 3365.98 4870\r## SC 1844 303 92 1241.42 2446\r## TGA 893 303 92 290.99 1496\r## TN 2965 546 92 1880.59 4049\r## TX 919 379 92 167.04 1671\r## VA 2124 378 92 1373.34 2875\r## WA 3943 303 92 3340.44 4545\r## ## gen = Bridger:\r## loc emmean SE df lower.CL upper.CL\r## GGA 1470 379 92 718.17 2223\r## ID 3591 303 92 2989.15 4194\r## KS 1091 546 92 7.35 2175\r## MS 2478 546 92 1393.89 3562\r## MT 3037 544 92 1957.63 4117\r## NC 1479 379 92 727.28 2232\r## NY 3130 546 92 2045.60 4214\r## OR 2564 379 92 1811.99 3316\r## SC 2282 303 92 1679.58 2884\r## TGA 1603 303 92 1000.66 2205\r## TN 2485 546 92 1401.33 3569\r## TX 851 379 92 99.08 1604\r## VA 2397 378 92 1646.76 3148\r## WA 3935 303 92 3332.27 4537\r## ## gen = Cascade:\r## loc emmean SE df lower.CL upper.CL\r## GGA 1758 379 92 1006.25 2511\r## ID 4081 303 92 3479.04 4684\r## KS 891 546 92 -193.40 1975\r## MS 1598 546 92 514.04 2682\r## MT 3125 544 92 2045.63 4205\r## NC 1062 379 92 309.61 1814\r## NY 2586 546 92 1502.21 3670\r## OR 2806 379 92 2053.82 3558\r## SC 1982 303 92 1379.70 2584\r## TGA 1492 303 92 889.83 2094\r## TN 2006 546 92 922.37 3090\r## TX 796 379 92 43.59 1548\r## VA 2191 378 92 1440.56 2942\r## WA 4203 303 92 3600.69 4805\r## ## gen = Dwarf:\r## loc emmean SE df lower.CL upper.CL\r## GGA 1538 379 92 785.71 2290\r## ID 4326 303 92 3723.81 4928\r## KS 1208 546 92 123.85 2292\r## MS 1966 546 92 881.69 3050\r## MT 3661 544 92 2581.14 4740\r## NC 1321 379 92 568.53 2073\r## NY 3645 546 92 2561.26 4729\r## OR 3594 379 92 2841.40 4346\r## SC 1292 303 92 690.10 1895\r## TGA 451 303 92 -151.81 1053\r## TN 2688 546 92 1603.57 3771\r## TX 654 379 92 -98.64 1406\r## VA 2250 378 92 1499.12 3000\r## WA 3726 303 92 3123.52 4328\r## ## gen = Glacier:\r## loc emmean SE df lower.CL upper.CL\r## GGA 2031 379 92 1278.35 2783\r## ID 4299 303 92 3696.61 4901\r## KS 1268 546 92 183.85 2352\r## MS 2861 546 92 1776.82 3945\r## MT 3516 544 92 2436.14 4595\r## NC 1452 379 92 699.82 2204\r## NY 3301 546 92 2217.49 4385\r## OR 3472 379 92 2719.36 4224\r## SC 2025 303 92 1422.97 2628\r## TGA 1109 303 92 506.90 1712\r## TN 2265 546 92 1180.58 3348\r## TX 720 379 92 -31.85 1473\r## VA 2363 378 92 1612.64 3114\r## WA 3807 303 92 3205.02 4410\r## ## gen = Jet:\r## loc emmean SE df lower.CL upper.CL\r## GGA 1511 379 92 758.95 2263\r## ID 4262 303 92 3659.68 4864\r## KS 1082 546 92 -2.40 2166\r## MS 1866 546 92 781.68 2950\r## MT 3869 544 92 2789.89 4949\r## NC 1326 379 92 573.58 2078\r## NY 3237 546 92 2152.80 4321\r## OR 3199 379 92 2446.70 3951\r## SC 1488 303 92 886.13 2091\r## TGA 622 303 92 19.27 1224\r## TN 2853 546 92 1768.63 3937\r## TX 1020 379 92 267.99 1772\r## VA 2364 378 92 1613.85 3115\r## WA 3554 303 92 2952.19 4157\r## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts\r## gen = Bienvenu:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1968 267 92 7.359 \u0026lt;.0001\r## ## gen = Bridger:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1286 267 92 4.811 \u0026lt;.0001\r## ## gen = Cascade:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1948 267 92 7.286 \u0026lt;.0001\r## ## gen = Dwarf:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 2132 267 92 7.972 \u0026lt;.0001\r## ## gen = Glacier:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1826 267 92 6.828 \u0026lt;.0001\r## ## gen = Jet:\r## contrast estimate SE df t.ratio p.value\r## West_V_South 1902 267 92 7.112 \u0026lt;.0001\r## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment\r To perform custom contrasts on a another variable, a cList and emmeans call for that variable is required.\nANCOVA (analysis of covariance) From a R programming perspective, this is no different than running a standard linear model. A data set from agridat, \u0026ldquo;theobald.covariate\u0026rdquo; comparing corn silage yields across multiple years, locations and cultivars. The data set includes a covariate, \u0026ldquo;chu\u0026rdquo; (corn heat units, a bit like growing degree days).\nLoad data and examine:\ndata(theobald.covariate)\rstr(theobald.covariate)\r ## 'data.frame':\t256 obs. of 5 variables:\r## $ year : int 1990 1990 1990 1990 1990 1991 1991 1991 1991 1991 ...\r## $ env : Factor w/ 7 levels \u0026quot;E1\u0026quot;,\u0026quot;E2\u0026quot;,\u0026quot;E3\u0026quot;,..: 1 2 3 4 7 1 2 3 4 5 ...\r## $ gen : Factor w/ 10 levels \u0026quot;G01\u0026quot;,\u0026quot;G02\u0026quot;,\u0026quot;G03\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ...\r## $ yield: num 6.27 5.57 8.45 7.35 6.5 6.71 5.59 8.36 7.25 8.09 ...\r## $ chu : num 2.57 2.53 2.72 2.72 2.48 2.44 2.55 2.75 2.75 2.61 ...\r count_na(theobald.covariate)\r ## year env gen yield chu ## 0 0 0 0 0\r Exploratory plots:\n# distributions of continuous variables\rhist(theobald.covariate$yield, col = \u0026quot;gold\u0026quot;)\r hist(theobald.covariate$chu, col = \u0026quot;gray70\u0026quot;)\r # relationship between reponse variable and covariate:\rwith(theobald.covariate, plot(chu, yield))\r length(unique(theobald.covariate$chu))\r ## [1] 21\r # the usual boxplots: boxplot(yield ~ env, data = theobald.covariate, col = \u0026quot;orangered\u0026quot;)\r boxplot(yield ~ year, data = theobald.covariate, col = \u0026quot;chartreuse\u0026quot;)\r boxplot(yield ~ gen, data = theobald.covariate, col = \u0026quot;darkcyan\u0026quot;)\r Check the extent of replication:\ntheobald.covariate$Year \u0026lt;- as.factor(theobald.covariate$year)\rreplications(yield ~ Year + env + gen, data = theobald.covariate)\r ## $Year\r## Year\r## 1990 1991 1992 1993 1994 ## 40 63 60 45 48 ## ## $env\r## env\r## E1 E2 E3 E4 E5 E6 E7 ## 35 35 44 36 36 36 34 ## ## $gen\r## gen\r## G01 G02 G03 G04 G05 G06 G07 G08 G09 G10 ## 29 29 29 29 22 29 23 18 24 24\r # with(theobald.covariate, table(gen, env, Year)) # lots of useful output\r The treatments are not fully crossed, so a fully specified model of the form yield ~ Year*env*gen*chu cannot be tested. The treatments and interactions were tested in reduced models and compared (not shown). The final \u0026ldquo;best\u0026rdquo; model is shown below.\n# the covariate, chu, is added in like any other effect. theobald.lm2 \u0026lt;- lm(yield ~ Year + env*chu, data = theobald.covariate)\rAnova(theobald.lm2, type = \u0026quot;III\u0026quot;)\r ## Anova Table (Type III tests)\r## ## Response: yield\r## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 4.309 1 6.8321 0.009524 ** ## Year 76.589 4 30.3607 \u0026lt; 2.2e-16 ***\r## env 13.473 6 3.5607 0.002138 ** ## chu 11.831 1 18.7596 2.187e-05 ***\r## env:chu 13.376 6 3.5350 0.002268 ** ## Residuals 150.096 238 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r # how to extract the covariate slope(s): emtrends(theobald.lm2, ~ env, \u0026quot;chu\u0026quot;)\r ## env chu.trend SE df lower.CL upper.CL\r## E1 7.015 1.62 238 3.82 10.21\r## E2 0.979 4.44 238 -7.76 9.72\r## E3 4.099 3.15 238 -2.11 10.31\r## E4 -2.884 3.54 238 -9.87 4.10\r## E5 8.222 2.70 238 2.90 13.54\r## E6 3.425 2.72 238 -1.93 8.78\r## E7 -0.359 2.55 238 -5.38 4.66\r## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95\r # emmeans extracted as usual:\remmeans(theobald.lm2, ~ env)\r ## NOTE: Results may be misleading due to involvement in interactions\r ## env emmean SE df lower.CL upper.CL\r## E1 6.67 0.175 238 6.32 7.01\r## E2 5.13 0.256 238 4.63 5.64\r## E3 6.66 0.482 238 5.71 7.61\r## E4 7.22 0.508 238 6.22 8.22\r## E5 6.61 0.138 238 6.34 6.88\r## E6 6.43 0.236 238 5.97 6.90\r## E7 6.32 0.397 238 5.54 7.10\r## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95\r emmeans(theobald.lm2, ~ Year)\r ## Year emmean SE df lower.CL upper.CL\r## 1990 6.97 0.189 238 6.60 7.34\r## 1991 6.75 0.170 238 6.41 7.08\r## 1992 7.07 0.187 238 6.70 7.44\r## 1993 5.39 0.208 238 4.98 5.80\r## 1994 6.00 0.218 238 5.57 6.43\r## ## Results are averaged over the levels of: env ## Confidence level used: 0.95\r Split-plot Load \u0026ldquo;Oats\u0026rdquo; from nlme. Nitrogen level (\u0026ldquo;nitro\u0026rdquo;) is the main plot, cultivar (\u0026ldquo;Variety\u0026rdquo;) is the sub-plot and \u0026ldquo;Block\u0026rdquo; describes the blocking layout.\ndata(Oats) str(Oats)\r ## Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':\t72 obs. of 4 variables:\r## $ Block : Ord.factor w/ 6 levels \u0026quot;VI\u0026quot;\u0026lt;\u0026quot;V\u0026quot;\u0026lt;\u0026quot;III\u0026quot;\u0026lt;..: 6 6 6 6 6 6 6 6 6 6 ...\r## $ Variety: Factor w/ 3 levels \u0026quot;Golden Rain\u0026quot;,..: 3 3 3 3 1 1 1 1 2 2 ...\r## $ nitro : num 0 0.2 0.4 0.6 0 0.2 0.4 0.6 0 0.2 ...\r## $ yield : num 111 130 157 174 117 114 161 141 105 140 ...\r## - attr(*, \u0026quot;formula\u0026quot;)=Class 'formula' language yield ~ nitro | Block\r## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## - attr(*, \u0026quot;labels\u0026quot;)=List of 2\r## ..$ y: chr \u0026quot;Yield\u0026quot;\r## ..$ x: chr \u0026quot;Nitrogen concentration\u0026quot;\r## - attr(*, \u0026quot;units\u0026quot;)=List of 2\r## ..$ y: chr \u0026quot;(bushels/acre)\u0026quot;\r## ..$ x: chr \u0026quot;(cwt/acre)\u0026quot;\r## - attr(*, \u0026quot;inner\u0026quot;)=Class 'formula' language ~Variety\r## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt;\r count_na(Oats)\r ## Block Variety nitro yield ## 0 0 0 0\r Oats$N \u0026lt;- as.factor(Oats$nitro)\rreplications(yield ~ Variety*N*Block, data = Oats)\r ## Variety N Block Variety:N Variety:Block ## 24 18 12 6 4 ## N:Block Variety:N:Block ## 3 1\r table(Oats$Variety, Oats$N)\r ## ## 0 0.2 0.4 0.6\r## Golden Rain 6 6 6 6\r## Marvellous 6 6 6 6\r## Victory 6 6 6 6\r hist(Oats$yield, col = \u0026quot;gold\u0026quot;)\r boxplot(yield ~ N, data = Oats, col = \u0026quot;dodgerblue1\u0026quot;)\r boxplot(yield ~ Variety, data = Oats, col = \u0026quot;red3\u0026quot;)\r Balanced Trial Analysis\nThe format for specifying split-plot error terms is Error(blocking factor/main plot).\n#contrasts(\u0026quot;contr.sum\u0026quot;)\rspl.oats \u0026lt;- aov(yield ~ Variety*N + Error(Block:N), data = Oats) summary(spl.oats)\r ## ## Error: Block:N\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## N 3 20020 6673 7.556 0.00143 **\r## Residuals 20 17663 883 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Error: Within\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Variety 2 1786 893.2 2.930 0.0649 .\r## Variety:N 6 322 53.6 0.176 0.9818 ## Residuals 40 12194 304.8 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r emmeans(spl.oats, \u0026quot;N\u0026quot;)  ## Note: re-fitting model with sum-to-zero contrasts\r ## NOTE: Results may be misleading due to involvement in interactions\r ## N emmean SE df lower.CL upper.CL\r## 0 79.4 7 20 64.8 94\r## 0.2 98.9 7 20 84.3 114\r## 0.4 114.2 7 20 99.6 129\r## 0.6 123.4 7 20 108.8 138\r## ## Results are averaged over the levels of: Variety ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95\r emmeans(spl.oats, ~ Variety)  ## Note: re-fitting model with sum-to-zero contrasts\r## NOTE: Results may be misleading due to involvement in interactions\r ## Variety emmean SE df lower.CL upper.CL\r## Golden Rain 104.5 4.55 46.1 95.3 114\r## Marvellous 109.8 4.55 46.1 100.6 119\r## Victory 97.6 4.55 46.1 88.5 107\r## ## Results are averaged over the levels of: N ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95\r Unbalanced Trial Analysis\nspl.oats2 \u0026lt;- lmer(yield ~ N*Variety + (1|Block:N), data = Oats) Anova(spl.oats2, type = \u0026quot;3\u0026quot;)\r ## Analysis of Deviance Table (Type III Wald chisquare tests)\r## ## Response: yield\r## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 77.1670 1 \u0026lt; 2.2e-16 ***\r## N 13.9028 3 0.003041 ** ## Variety 2.2747 2 0.320663 ## N:Variety 1.0554 6 0.983423 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r emmeans(spl.oats2, \u0026quot;N\u0026quot;)  ## NOTE: Results may be misleading due to involvement in interactions\r ## N emmean SE df lower.CL upper.CL\r## 0 79.4 7 20 64.8 94\r## 0.2 98.9 7 20 84.3 114\r## 0.4 114.2 7 20 99.6 129\r## 0.6 123.4 7 20 108.8 138\r## ## Results are averaged over the levels of: Variety ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95\r emmeans(spl.oats2, ~ Variety)  ## NOTE: Results may be misleading due to involvement in interactions\r ## Variety emmean SE df lower.CL upper.CL\r## Golden Rain 104.5 4.55 46.1 95.3 114\r## Marvellous 109.8 4.55 46.1 100.6 119\r## Victory 97.6 4.55 46.1 88.5 107\r## ## Results are averaged over the levels of: N ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95\r Other Designs There are many other experimental designs commonly used in agricultural trials (split-split plot, split-block, alpha lattice, etc). We have written an online resource for routine incorporation of spatial covariates into field trial analysis that includes information on how to analyze different designs. You could also consider using the agricolae package.\nExtra Functions for extracting model parameters, diagnostics and other model information\nThese work differently with different R object types. That is, different output will result depending on if a \u0026ldquo;lm\u0026rdquo;, \u0026ldquo;lme\u0026rdquo; or \u0026ldquo;merMod\u0026rdquo; (lmer) object is used in the function call.\n# extract model summary\rsummary()\r#extract coefficients:\rcoef()\r#extract residuals\rresid()\rrstudent()\rresiduals()\r# extract predicted values\rfits()\r# make diagnostic plots\rplot()\r# extract influence measures:\rinfluence.measures()\r#other fir diagnostics:\rcooks.distance()\rdffits()\rdfbeta()\rhat()\r To see the all functions available for a particular type of linear model object, use:\nmethods(class = \u0026quot;lm\u0026quot;) # for lm objects\r ## [1] add1 addterm alias ## [4] anova Anova attrassign ## [7] avPlot Boot bootCase ## [10] boxcox boxCox brief ## [13] case.names ceresPlot coerce ## [16] concordance confidenceEllipse confint ## [19] Confint cooks.distance crPlot ## [22] deltaMethod deviance dfbeta ## [25] dfbetaPlots dfbetas dfbetasPlots ## [28] drop1 dropterm dummy.coef ## [31] durbinWatsonTest effects emm_basis ## [34] extractAIC family formula ## [37] hatvalues hccm infIndexPlot ## [40] influence influencePlot initialize ## [43] inverseResponsePlot kappa labels ## [46] leveneTest leveragePlot linearHypothesis ## [49] logLik logtrans mcPlot ## [52] mmp model.frame model.matrix ## [55] ncvTest nextBoot nobs ## [58] outlierTest plot powerTransform ## [61] predict Predict print ## [64] proj qqnorm qqPlot ## [67] qr recover_data residualPlot ## [70] residualPlots residuals rstandard ## [73] rstudent S show ## [76] sigmaHat simulate slotsFromS3 ## [79] spreadLevelPlot summary symbox ## [82] variable.names vcov ## see '?methods' for accessing help and source code\r methods(class = \u0026quot;lme\u0026quot;) # for lme4 objects\r ## [1] ACF anova Anova augPred ## [5] coef comparePred confint Confint ## [9] deltaMethod deviance emm_basis extractAIC ## [13] fitted fixef formula getData ## [17] getGroups getGroupsFormula getResponse getVarCov ## [21] influence intervals linearHypothesis logLik ## [25] matchCoefs nobs pairs plot ## [29] predict print qqnorm ranef ## [33] recover_data residuals S sigma ## [37] simulate summary update VarCorr ## [41] Variogram vcov ## see '?methods' for accessing help and source code\r methods(class = \u0026quot;merMod\u0026quot;) # for nlme objects  ## [1] anova Anova as.function coef ## [5] confint cooks.distance deltaMethod deviance ## [9] df.residual drop1 emm_basis extractAIC ## [13] family fitted fixef formula ## [17] getData getL getME hatvalues ## [21] influence isGLMM isLMM isNLMM ## [25] isREML linearHypothesis logLik matchCoefs ## [29] model.frame model.matrix na.action ngrps ## [33] nobs plot predict print ## [37] profile ranef recover_data refit ## [41] refitML rePCA residuals rstudent ## [45] show sigma simulate summary ## [49] terms update VarCorr vcov ## [53] vif weights ## see '?methods' for accessing help and source code\r The package emmeans also supports a large number of models.\n","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"a1d58233f436a629f860db1f7ebc5c18","permalink":"/post/anova-in-r/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/post/anova-in-r/","section":"post","summary":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility.","tags":["ANOVA","linear models","lme4","emmeans"],"title":"Applied ANOVA in R","type":"post"},{"authors":["Statistical Programs"],"categories":null,"content":"Here is the slide set for the workshop. A more extensive resource is also available [here](Spatial Recipes for Field Trials).\n","date":1636275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636275600,"objectID":"2f9bb2ee0e0bf534d0410e8050c5d500","permalink":"/draft-workshops/spatial_workshop_new/slide-set/","publishdate":"2021-11-07T09:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/slide-set/","section":"draft-workshops","summary":"A one-day introductory workshop on how to integrate spatial covariates into analysis of field trials laid out in a lattice pattern using SAS and R.","tags":["spatial statistics","field experiments"],"title":"Slide set of spatial recipes","type":"draft-workshops"},{"authors":null,"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"6871f3ea19b37a37cb6962d10767b6a6","permalink":"/project/spatial-stats-field-trials/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/project/spatial-stats-field-trials/","section":"project","summary":"Our extended resource on including spatial models in gridded field trials","tags":["Demo"],"title":"Incorporating Spatial Covariates into Planned Field Experienments","type":"project"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Install R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstall RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstall Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis)\rusethis::use_blank_slate()\r You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIf you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8b70804b6afa070131995606b8772ebd","permalink":"/post/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/getting-r-setup/","section":"post","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":["R","reproducible research"],"title":"Getting R Set Up","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the \u0026lsquo;usethis\u0026rsquo; package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()\r You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R or .Rmd file This is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*This is almost guaranteed to happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)\r Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory\r│ README.md\r│\r└───data\r│ │ file011.txt\r│ │ file012.txt\r│ │\r│ └───spatial_files\r│ │ file208.dbf\r│ │ file208.shp\r│ │ file208.shx\r│ └───scripts\r│ │ eda.R\r│ │ analysis.R\r│ │ plots.R\r│ │ final_report.Rmd\r|\r└───outputs\r│ │ plot1.png\r│ │ blups.csv\r|\r└───extra\r│ some_paper.pdf\r│ ...\r I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here)\rmydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))\r This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv)\rrenv::init()\r If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()\r If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this! three-elk-FARM-2001\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"32bd8072205d33315c2c1a506db82c8c","permalink":"/post/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/reproducible-r/","section":"post","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"post"},{"authors":null,"categories":null,"content":"","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"91e1b7a95d5e6b1ad524b3787bac8fdc","permalink":"/project/reproducible-research/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/project/reproducible-research/","section":"project","summary":"Gosh, reproducible research is important. Let's do more of it!","tags":["Reproducible Research"],"title":"Reproducible Research","type":"project"},{"authors":["Statistical Programs"],"categories":null,"content":"","date":1617982200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617982200,"objectID":"7a023a5229b3c480b15defa8bfb626a1","permalink":"/talks/spatial_seminar_20210409/","publishdate":"2021-04-09T15:30:00Z","relpermalink":"/talks/spatial_seminar_20210409/","section":"talks","summary":"A brief introduction into how to integrate spatial covariates into ANOVA-based analysis of field trials laid out in a lattice pattern.","tags":["spatial statistics","field experiments"],"title":"Routine Incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"talks"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"=\rul {\rcolor: #282828;\rfont-size: 40px;\r}\r\r A Road in Auvers After the Rain by Vincent Van Gogh\n\r  ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data. (you don\u0026rsquo;t have to be a geospatial statistics expert)\n\rWhere to Find This Information This Presentation:\nhttps://github.com/IdahoAgStats/lattice-spatial-analysis-talk\r A longer tutorial:\nhttps://idahoagstats.github.io/guide-to-field-trial-spatial-analysis\r What Are Barriers to Using Spatial Stats?  Perceived lack of need Unsure of benefits No training in the topic/intimidated by the statistical methodology Limited time to devote to statistical analysis Unclear what would happen to blocking if spatial stats are used very few resources for easy implementation  Spatial Variation in Agricultural Fields Univeristy of Idaho's Parker Farm (Moscow, Idaho)\n\rSpatial Variation in Agricultural Fields Blocking in Agricultural Fields Blocking versus Spatial Analysis This is not how this works. Blocking is compatible with spatial analysis and recommended for most (all?) field trials.\nThere Are Many Spatial Methods Available    areal data correlated error models     row and column trend exponential   nearest neighbor spherical   separable ARxAR models Gaussian   spatial error model Matern   spatial lag model Cauchy   ARIMA power   splines linear   GAMs many more\u0026hellip;    These Methods Work These Methods Can Be Complex  \u0026hellip;.But\nYou can also integrate spatial methods into gridded field trials without:\n having to know anything about map projections, shapefiles or other geospatial terminology possessing a deep understanding of linear modeling techniques or empirical variograms being an R or SAS programming expert  Knowing these things is helpful, but not essential.\nA Typical Experiment  Experimental treatments fully crossed effects Blocking scheme along the expected direction of field variation  Analysis A typical linear model: $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nResponse = trial mean + treatment effect + block effect + leftover error\nWe Assume:  The error terms, or residuals, are independent of another with a shared distribution:  $$\\epsilon_i \\sim N(0,\\sigma_e)$$\nEach block captures variation unique to that block and there is no other variation related to spatial position of the experimental plots.   **How often is #2 evaluated?** \rExample Analysis Average Yield by Row, Column and Block Standard Analysis of Kimberly, 2013 Wheat Variety Trial  36 soft white winter wheat cultivars 4 blocks 2 missing data points the linear model:  $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nlibrary(nlme)\rlm1 \u0026lt;- lme(yield ~ cultivar, random = ~ 1|block, data = mydata, na.action = na.exclude)\r What Do The Residuals Look Like? plot(lm1)\r What Do The Residuals Look Like Spatially? What Do The Residuals Look Like Spatially? Global Moran\u0026rsquo;s Test for Spatial Autocorrelation $H_0$: There is no spatial autocorrelation $H_a:$ There is spatial autocorrelation!\nThis uses a simple weighting matrix that weights all neighbors that share a plot border (the chess-based \u0026ldquo;rook\u0026rdquo; formation) equally.\n## ## Monte-Carlo simulation of Moran I\r## ## data: mydata$residuals ## weights: weights ## omitted: 88, 97 ## number of simulations + 1: 1000 ## ## statistic = 0.15869, observed rank = 997, p-value = 0.003\r## alternative hypothesis: greater\r Handling Spatial Autocorrelation in Areal Data Areal data = finite region divided into discrete sub-regions (plots) with aggregated outcomes\nOptions:\n model row and column trends  good for known gradients (hill slope, salinity patterns)   assume plots close together are more similar than plots far apart. The errors terms can be modelled based on proximity, but there is no trial-wide trend  autoregressive models (AR) models utilizing \u0026ldquo;gaussian random fields\u0026rdquo; for continuously varying data (e.g. point data) Smoothing splines nearest neighbor    Basic Linear Model $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$ $$\\epsilon_i \\sim N(0,\\sigma)$$\nIf N = 4:\n$$e_i ~\\sim N \\Bigg( 0, \\left[ {\\begin{array}{ccc} \\sigma^2 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; \\sigma^2 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; 0 \u0026amp; \\sigma^2 \u0026amp; 0\\\n0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\sigma^2 \\end{array} } \\right] \\Bigg) $$\nThe variance-covariance matrix indicates a shared variance and all off-diagonals are zero, that is, the errors are uncorrelated.\nLinear Model with Autoregressive (AR) Errors Same linear model: $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$\nDifferent variance structure:\n$$e_i ~\\sim N \\Bigg( 0, = \\sigma^2 \\left[ {\\begin{array}{cc} 1 \u0026amp; \\rho \u0026amp; \\rho^2 \u0026amp; \\rho^3 \\\n\\rho \u0026amp; 1 \u0026amp; \\rho \u0026amp; \\rho^2 \\\n\\rho^2 \u0026amp; \\rho \u0026amp; 1 \u0026amp; \\rho \\\n\\rho^3 \u0026amp; \\rho^2 \u0026amp; \\rho \u0026amp; 1 \\ \\end{array} } \\right] \\Bigg) $$\n $\\rho$ is a correlation parameter ranging from -1 to 1 where 0 is no correlation and values approaching 1 indicate spatial correlation. The \u0026ldquo;one\u0026rdquo; in AR1 means that only the next most adjacent point is considered. There can be AR2, AR3, \u0026hellip;, ARn models.  The Separable AR1 x AR1 model   AR1xAR1 assumes correlation in two directions, row and column. It estimates $\\sigma$, $\\rho_{column}$, and $\\rho_{row}$ often a good choice since plot are rectangular and hence autocorrelation will differ by direction (\u0026ldquo;anistropy\u0026rdquo;)  More Notes on Separable AR1xAR1  From a statistical standpoint, it\u0026rsquo;s one of the more intuitive models The implementation in R is a little shaky  several packages, all hard to use and incompatible with other R packages   It is implemented in SAS Some proprietary software implements this (AsREML), others do not (Agrobase)  Semivariance and Empirical Variograms A measure of spatial correlation based on all pairwise correlations in a data set, binned by distance apart:\n$\\gamma^2(h) = \\frac{1}{2} Var[Z(s+h)-Z(s)]$\n$Z(s)$ = observed data at point $s$.\n$Z(s)$ = observed data at another point $h$ distance from point $s$.\nFor a data set with $N$ observation, there are this many pairwise points:\n$\\frac{N(N-1)}{2}$\nEmpirical Variogram This uses semivariance to mathematically relate spatial correlations with distance\nrange = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance\nSemivariance \u0026amp; Empirical Variograms  There are many difference mathematical models for explaining semivariance:  exponential, Gaussian, Matérn, spherical, \u0026hellip;   It is usually used for kriging, or prediction of a new point through spatial interpolation It can also be used in a linear model where local observations are used to predict a data point in addition to treatment effects Bonus: R and SAS are really good at this!  Adding Semivariance to a Linear Model Copy data into new object so we can assign it a new class (and remove missing data):\nlibrary(gstat); library(sp); library(dplyr)\rmydata_sp \u0026lt;- mydata %\u0026gt;% filter(!is.na(yield))\r Establish coordinates for data set to make it an sp object (\u0026ldquo;spatial points\u0026rdquo;):\ncoordinates(mydata_sp) \u0026lt;- ~ row + range\r Set the maximum distance for looking at pairwise correlations:\nmax_dist \u0026lt;- 0.5*max(dist(coordinates(mydata_sp)))\r Adding Semivariance to a Linear Model Calculate a sample variogram:\nsemivar \u0026lt;- variogram(yield ~ block + cultivar, data = mydata_sp,\rcutoff = max_dist, width = max_dist/12)\rnugget_start \u0026lt;- min(semivar$gamma)\r Adding Semivariance to a Linear Model The empirical variogram:\nplot(semivar)\r Adding Semivariance to a Linear Model Set up models for fitting variograms:\nvgm1 \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential\rvgm2 \u0026lt;- vgm(model = \u0026quot;Sph\u0026quot;, nugget = nugget_start) # spherical\rvgm3 \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian\r Fit the variogram models to the data:\nvariofit1 \u0026lt;- fit.variogram(semivar, vgm1)\rvariofit2 \u0026lt;- fit.variogram(semivar, vgm2)\rvariofit3 \u0026lt;- fit.variogram(semivar, vgm3)\r Adding Semivariance to a Linear Model Look at the error terms to see which model is the best at minimizing error.\n## [1] \u0026quot;exponential: 26857.3\u0026quot;\r ## [1] \u0026quot;spherical: 26058.3\u0026quot;\r ## [1] \u0026quot;Gaussian: 41861.0\u0026quot;\r The spherical model is the best at minimizing error.\nAdding Semivariance to a Linear Model plot(semivar, variofit2, main = \u0026quot;Spherical model\u0026quot;)\r Adding Semivariance to a Linear Model Extract the nugget and sill information from the spherical variogram:\nnugget \u0026lt;- variofit2$psill[1] range \u0026lt;- variofit2$range[2] sill \u0026lt;- sum(variofit2$psill) nugget.effect \u0026lt;- nugget/sill # the nugget/sill ratio\r Adding Semivariance to a Linear Model Build a correlation structure in nlme:\ncor.sph \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row + range, nugget = T, fixed = F,\rtype = \u0026quot;spherical\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)\r Update the Model:\nlm_sph \u0026lt;- update(lm1, corr = cor.sph)\r Compare Models - Log likelihood logLik(lm1)\r ## 'log Lik.' -489.0572 (df=38)\r logLik(lm_sph)\r ## 'log Lik.' -445.4782 (df=40)\r Compare Models - Post-hoc Power anova(lm1)[2,]\r ## numDF denDF F-value p-value\r## cultivar 35 103 1.6411 0.029\r anova(lm_sph)[2,]\r ## numDF denDF F-value p-value\r## cultivar 35 103 2.054749 0.0028\r Compare Model Predictions library(emmeans)\rlme_preds \u0026lt;- as.data.frame(emmeans(lm1, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model\u0026quot;)\rsph_preds \u0026lt;- as.data.frame(emmeans(lm_sph, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model + spatial\u0026quot;)\rpreds \u0026lt;- rbind(lme_preds, sph_preds)\r Compare Model Predictions Highest yielding wheat: \u0026lsquo;Stephens\u0026rsquo; (released in 1977)\nWhere Was Stephens Located in the Trial? More Notes  When models omit blocking, the predictions may be unchanged or they may worsen. This varies by the agronomic field, but in general, blocking a field trial and including block in the statistical model improves your experimental power and controls experimental error. There is no single spatial model that fits all However, using any spatial model is usually better than none at all When you use spatial covariates, your estimates are better and more precise. This really does help you!  What\u0026rsquo;s Next:  Track row and range information in your trial data set. Look at the tutorial! (we will also add SAS code) Try out a few models and see how it impacts your results.  The Seminar Was Brought to you by\u0026hellip;Statistical Programs!!! Statistical consulting to support the College of Agriculture and Life Sciences.\nBill Price, Director, bprice@uidaho.edu, AgSci307\nJulia Piaskowski, jpiaskowski@uidaho.edu, AgSci 305\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"207697934e1ff33ee3b594c2b0f9405a","permalink":"/slides/spatial_seminar_20210409/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial_seminar_20210409/","section":"slides","summary":"=\rul {\rcolor: #282828;\rfont-size: 40px;\r}\r\r A Road in Auvers After the Rain by Vincent Van Gogh\n\r  ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data.","tags":null,"title":"Routine incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"slides"}]