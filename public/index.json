[{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\n","date":1636275600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1636275600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/statistical-programs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/statistical-programs/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1618444800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1618444800,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/author/julia-piaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/julia-piaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/author/william-price/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/william-price/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/author/xin-dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-dai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"   Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n check your system   SAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SAS® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n begin the workshop   This workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"5bdfbb99e430a3c1a2a4056386e4765a","permalink":"/workshops/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/","section":"workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"   Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n check your system   SAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SAS® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n begin the workshop   This workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"4a1872e9499ca9ac572da527f1327e4b","permalink":"/draft-workshops/spatial_workshop_new/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/","section":"draft-workshops","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials II","type":"book"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization   Statistics Introduction to statistics for data science.   Meet your instructor Statistical Programs FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"3455929a3b128d6d930e25725df43dc4","permalink":"/draft-workshops/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/draft-workshops/example/","section":"draft-workshops","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"📊 Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()  If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()  If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation \u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting \u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling \u0026quot;SpATS\u0026quot;, # for fitting splines \u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling install.packages(package_list) sapply(package_list, require, character.only = TRUE)  Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please install these in advance of the workshop. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.\nDownload files for workshop The following files are used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9a50ee534254c7b08141160344374595","permalink":"/draft-workshops/spatial_workshop_new/prep-work/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/prep-work/","section":"draft-workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"  Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()  If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()  If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation \u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting \u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling \u0026quot;SpATS\u0026quot;, # for fitting splines \u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling install.packages(package_list) sapply(package_list, require, character.only = TRUE)   Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"15841bfb95305ff58eb70b526b39d8a2","permalink":"/workshops/spatial-workshop/prep-work/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/prep-work/","section":"workshops","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n  Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n  Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n  The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.     Error terms are often correlated with each other, invalidating the downstream analysis     high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics   another distracted boyfriend meme!  Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"a31a33acbb99d2f0e16a5caf445f69ea","permalink":"/workshops/spatial-workshop/why-spatial/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/why-spatial/","section":"workshops","summary":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":" This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n  While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n  Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R # load libraries library(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme) # read in data and prepare it Nin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row) Nin_na \u0026lt;- filter(Nin, !is.na(rep)) # make exploratory plot ggplot(Nin, aes(x = row, y = col)) + geom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) + geom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) + labs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() + theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14), legend.title = element_text(size = 14), legend.text = element_text(size = 12)) ## conduct moran's I test ## # set neighbors with convenience function for grids xy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals nin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) resid_lme \u0026lt;- residuals(nin.lme) names(resid_lme) \u0026lt;- Nin$plot # two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude) moran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)   SAS # read in data proc format; invalue has_NA 'NA' = .; ; filename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;; data alliance; infile NIN firstobs=2 delimiter=','; informat yield has_NA.; input entry $ rep $ yield col row; Row = 4.3*Row; Col = 1.2*Col; if yield=. then delete; run; # heatmap proc sgplot data=alliance; HEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run; # linear mixed model proc mixed data=alliance; class Rep Entry; model Yield = Entry / outp=residuals; random Rep; run; # Moran's I proc variogram data=residuals plots(only)=moran ; compute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ; coordinates xc=row yc=col; var resid; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb80fd2ad66cac061f57b377665b0236","permalink":"/workshops/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/diagnosis/","section":"workshops","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"5c1f7f41e31a8d8e08da3d62bb15dbae","permalink":"/draft-workshops/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/python/","section":"draft-workshops","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n  Example Empirical Variogram   range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Exponential Variogram  Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Gaussian Variogram  Matérn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n  Empirical Matérn Variogram  There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n For more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.   Variogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n  Not all fitted variogram models are worthy\n  Variogram gone bad  Code for this section The following scripts build upon work done in previous section(s).\nR # load libraries library(gstat) # set up spatial object Nin_spatial \u0026lt;- Nin_na coordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte class(Nin_spatial) # establish max distance for variogram estimation max_dist = 0.5*max(dist(coordinates(Nin_spatial))) # calculate empirical variogram resid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist, width = max_dist/20, # 20 is the number of bins data = Nin_spatial) plot(resid_var1) # empirical variogram #Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram) # starting value for the nugget nugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much) Nin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential Nin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian Nin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern # actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp) Nin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau) Nin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T) plot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;) plot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;) plot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;) # parameters: Nin_variofit_gau nugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error) range \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points sill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance   SAS # calculate semivariance and compute empirical variogram proc variogram data=residuals plots(only)=(semivar); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; var resid; run; # fit models to the empirical variogram proc variogram data=residuals plots(only)=(fitplot); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"cd8303e2c52504ec61991e37816c126c","permalink":"/workshops/spatial-workshop/variograms/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/variograms/","section":"workshops","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"dd726e22044eb744e57cecdc3ff1c5b8","permalink":"/draft-workshops/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/visualization/","section":"draft-workshops","summary":"","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR library(emmeans); library() # (nlme and gstat should already be loaded) library(spaMM) # for running `corMatern()` # standard linear model nin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) # extract the esimated marginal means for variety preds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;)) # use information from the variogram fitting for intialising the parameters nugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill # initalise the covariance structure (from the nlme package) cor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F, type = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;) # update the rcbd model nin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus) # extract predictions for 'gen' preds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;) # a similar procedure can be follow for other models # but we are going to take a shortcut and not specify the parameters # exponential cor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F) nin_exp \u0026lt;- update(nin_lme, corr = cor.exp) preds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;)) # Matern structure cor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F) nin_matern \u0026lt;- update(nin_lme, corr = cor.mat) preds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)   SAS proc mixed data=alliance ; class entry rep; model yield = entry ; random rep; lsmeans entry/cl; ods output LSMeans=NIN_RCBD_means; title1 'NIN data: RCBD'; run; proc mixed data=alliance maxiter=150; class entry; model yield = entry /ddfm=kr; repeated/subject=intercept type=sp(gau) (Row Col) local; parms (11) (22) (19); lsmeans entry/cl; ods output LSMeans=NIN_Spatial_means; title1 'NIN data: Gaussian Spatial Adjustment'; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"057408dc907a581ba7ba45d742b73dbd","permalink":"/workshops/spatial-workshop/correlated-error-models/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/correlated-error-models/","section":"workshops","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"b99996333fbb4c63e90c11bb44f0be2b","permalink":"/draft-workshops/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/draft-workshops/example/stats/","section":"draft-workshops","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n  The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR # load libraries library(lme4) # exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;) boxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;) boxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;) ## row/column model ## # data prep Nin$rowF = as.factor(Nin$row) Nin$colF = as.factor(Nin$col) # specify model nin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF), data = Nin, na.action = na.exclude) # extract random effects for row and column ranef(nin_rc)   SAS # exploratory boxplots proc sgplot data=alliance; vbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; # row/column model proc mixed data=alliance ; class entry rep; model yield = entry row col/ddfm=kr; random rep; lsmeans entry/cl; ods output LSMeans=NIN_row_col_means; title1 'NIN data: RCBD'; run;   Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR nin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20), degree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0)) preds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)   SAS proc glimmix data=alliance ; class entry rep; effect sp_r = spline(row col); model yield = entry sp_r/ddfm=kr; random row col/type=rsmooth; lsmeans entry/cl; ods output LSMeans=NIN_smooth_means; title1 'NIN data: RCBD'; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"b280dc26c3a20f98ee5ed0cf8a9403e6","permalink":"/workshops/spatial-workshop/trend-modelling/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/trend-modelling/","section":"workshops","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)   Comparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.   Unfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R library(tidyr) # remove some objects we don't need (and will interfere with downstream processes) rm(nin_variofit, nin_vgm) rm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect) # assemble objects into a list nlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern) names(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;) # extract log likelihood, AIC, BIC data.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC), BIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglikelihood)) # (higher is better for loglik, lower is better for AIC and BIC) # compare post-hoc power # conduct ANOVA anovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]}) # bind all the output together a \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a) a[6,2:5] \u0026lt;- anova(nin_trend)[3:6] a ## compare precision of estimates all.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;)) errors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;) pred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors)) error_df \u0026lt;- bind_cols(errors) colnames(error_df) \u0026lt;- pred.names boxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;) # compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;) preds_df \u0026lt;- bind_cols(preds) colnames(preds_df) \u0026lt;- pred.names preds_df$gen \u0026lt;- preds_exp$gen # plot changes in rank lev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;) pivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) + geom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) + geom_line() + ylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal(base_size = 14)   SAS data NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_RCBD_means; RCB_est = estimate; RCB_se = stderr; run; data NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_Spatial_means; Sp_est = estimate; Sp_se = stderr; run; proc sort data=NIN_RCBD_means; by entry; run; proc sort data=NIN_Spatial_means; by entry; run; data compare; merge NIN_RCBD_means NIN_Spatial_means; by entry; run; proc rank data=compare out=compare descending; var RCB_est Sp_est; ranks RCB_Rank Sp_Rank; run; proc sort data=compare; by Sp_rank; run; proc print data=compare(obs=15); var entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"006196c4ae04f5e62ccfd0b0e4974438","permalink":"/workshops/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/workshops/spatial-workshop/model-comparison/","section":"workshops","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in Burgueño et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (‘name’) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR # (if not already loaded) library(dplyr); library(nlme); library(ggplot2) library(gstat); library(sp) # read in data aug_data_origin \u0026lt;- read.csv(\u0026quot;data/AB19F5_LIND.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed mutate(yieldkg = yieldg/1000) # to prevent overflow # summarise the genoytypic data by checks/not checks gen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when( counts \u0026gt; 1 ~ \u0026quot;check\u0026quot;, counts == 1 ~ \u0026quot;unrep\u0026quot;)) # need info on just the checks checks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes gen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when( delta == \u0026quot;check\u0026quot; ~ gamma, delta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when( delta == \u0026quot;unrep\u0026quot; ~ gamma, delta == \u0026quot;check\u0026quot; ~ NA_integer_)) # or maybe \u0026quot;check\u0026quot; # merge original data set with info on treatment levels aug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldg) %\u0026gt;% mutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) # calculate denominator degrees of freedom ddf = sum(checks$counts) - nrow(checks) - 1 ## initial linear model: # tau estimates effects of checks versus all unreplicated genotypes) # beta predicts effects each unreplicated genotypes, nested in tau aug1 \u0026lt;- lme(fixed = yieldg ~ tau, random = ~ 1|tau/beta, data = aug_data, na.action = na.exclude) # another formulation # delta estimates effects of replicated versus unreplicated genotypes # gamma estimates the effecs of all genotypes evaluated in the trial aug2 \u0026lt;- lme(fixed = yieldg ~ delta, random = ~ 1|delta/gamma, data = aug_data, na.action = na.exclude) # extract residuals aug_data$res \u0026lt;- residuals(aug1) # plot residual chlorpleth map: ggplot(aug_data, aes(y = row, x = col)) + geom_tile(aes(fill = res)) + scale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) + scale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) + scale_y_continuous(breaks = 1:max(aug_data$col)) + coord_equal() + theme_void() # add spatial covariates aug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res)) coordinates(aug_spatial) \u0026lt;- ~ col + row max_dist = 0.5*max(dist(coordinates(aug_spatial))) aug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist, width = max_dist/10, # 20 is the number of bins data = aug_spatial) plot(aug_vario) # optional to run: # nugget_start \u0026lt;- min(aug_vario$gamma) # aug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # aug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm) # plot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;) cor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F, type = \u0026quot;exponential\u0026quot;) aug1_sp \u0026lt;- update(aug1, corr = cor_exp) # spatial parameters: aug1_sp$modelStruct$corStruct # extract BLUPs for unreplicated lines: aug_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)') # look at variance components VarCorr(aug1_sp)   SAS filename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;; PROC IMPORT OUT= WORK.augmented DATAFILE= AUG DBMS=CSV REPLACE; GETNAMES=YES; DATAROW=2; RUN; data augmented; set augmented; if yieldg = 999999 or yieldg=. then delete; /* Remove missing values */ prow=prow*11.7; /*convert row and column indices to feet */ pcol=pcol*5.5; run; proc freq noprint data=augmented; tables name/out=controls; run; data controls; set controls; if count \u0026gt;1; run; proc sort data=controls; by name; run; proc sort data=augmented; by name; run; data augmented; merge augmented controls; by name; if count=. then d2=2; /* Unreplicated */ else d2=1; /* Replicated */ yieldkg=yieldg/1000; run; PROC mixed data=augmented; class name d2; model yieldkg = d2/noint outp=residuals ddf=229 229; lsmeans d2; *lsmeans name(d2)/slice = d2; run; proc sgplot data=residuals; HEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map'; run; proc variogram data=residuals plots(only)=(fitplot); where yieldkg ^= .; coordinates xc=pcol yc=pRow; compute lagd=6.6 maxlags=25; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run; PROC mixed data=augmented; class name d2; model yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229; lsmeans d2; repeated/subject=intercept type=sp(pow)(prow pcol) local; ods output SolutionR =parms; parms (0.074) (0.0051)(0.475) ; *lsmeans name(d2)/slice = d2; # alot of output!! run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"561372141df0a39b09e92d9d036e8b87","permalink":"/workshops/spatial-workshop/augmented/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/augmented/","section":"workshops","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm Matérn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive     Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"c08d0a8717c5c9f76fe45f925c14f37a","permalink":"/workshops/spatial-workshop/conclusion/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/workshops/spatial-workshop/conclusion/","section":"workshops","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["Statistical Programs"],"categories":null,"content":"Here is the slide set for the workshop. A more extensive resource is also available [here](Spatial Recipes for Field Trials).\n","date":1636275600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636275600,"objectID":"2f9bb2ee0e0bf534d0410e8050c5d500","permalink":"/draft-workshops/spatial_workshop_new/slide-set/","publishdate":"2021-11-07T09:00:00Z","relpermalink":"/draft-workshops/spatial_workshop_new/slide-set/","section":"draft-workshops","summary":"A one-day introductory workshop on how to integrate spatial covariates into analysis of field trials laid out in a lattice pattern using SAS and R.","tags":["spatial statistics","field experiments"],"title":"Slide set of spatial recipes","type":"draft-workshops"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Install R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstall RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstall Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis) usethis::use_blank_slate()  You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIF you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8b70804b6afa070131995606b8772ebd","permalink":"/post/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/getting-r-setup/","section":"post","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":["R","reproducible research"],"title":"Getting R Set Up","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the \u0026lsquo;usethis\u0026rsquo; package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()  You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R or .Rmd file This is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*This is almost guaranteed to happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)  Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory │ README.md │ └───data │ │ file011.txt │ │ file012.txt │ │ │ └───spatial_files │ │ file208.dbf │ │ file208.shp │ │ file208.shx │ └───scripts │ │ eda.R │ │ analysis.R │ │ plots.R │ │ final_report.Rmd | └───outputs │ │ plot1.png │ │ blups.csv | └───extra │ some_paper.pdf │ ...  I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here) mydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))  This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv) renv::init()  If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()  If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this! three-elk-FARM-2001\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"32bd8072205d33315c2c1a506db82c8c","permalink":"/post/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/reproducible-r/","section":"post","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"post"},{"authors":null,"categories":null,"content":"Gosh, reproducible research is important. Let\u0026rsquo;s do more of it!\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8793122cbd284a2b1eee00d22d3422f6","permalink":"/project/reproducible-research/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/project/reproducible-research/","section":"project","summary":"An example of using the in-built project page.","tags":["Reproducible Research"],"title":"Reproducible Research","type":"project"},{"authors":["Statistical Programs"],"categories":null,"content":"","date":1617982200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617982200,"objectID":"7a023a5229b3c480b15defa8bfb626a1","permalink":"/talks/spatial_seminar_20210409/","publishdate":"2021-04-09T15:30:00Z","relpermalink":"/talks/spatial_seminar_20210409/","section":"talks","summary":"A brief introduction into how to integrate spatial covariates into ANOVA-based analysis of field trials laid out in a lattice pattern.","tags":["spatial statistics","field experiments"],"title":"Routine Incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"talks"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"},{"authors":null,"categories":null,"content":"= ul { color: #282828; font-size: 40px; }   A Road in Auvers After the Rain by Vincent Van Gogh\n   ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data. (you don\u0026rsquo;t have to be a geospatial statistics expert)\n Where to Find This Information This Presentation:\nhttps://github.com/IdahoAgStats/lattice-spatial-analysis-talk  A longer tutorial:\nhttps://idahoagstats.github.io/guide-to-field-trial-spatial-analysis  What Are Barriers to Using Spatial Stats?  Perceived lack of need Unsure of benefits No training in the topic/intimidated by the statistical methodology Limited time to devote to statistical analysis Unclear what would happen to blocking if spatial stats are used very few resources for easy implementation  Spatial Variation in Agricultural Fields Univeristy of Idaho's Parker Farm (Moscow, Idaho)\n Spatial Variation in Agricultural Fields Blocking in Agricultural Fields Blocking versus Spatial Analysis This is not how this works. Blocking is compatible with spatial analysis and recommended for most (all?) field trials.\nThere Are Many Spatial Methods Available    areal data correlated error models     row and column trend exponential   nearest neighbor spherical   separable ARxAR models Gaussian   spatial error model Matern   spatial lag model Cauchy   ARIMA power   splines linear   GAMs many more\u0026hellip;    These Methods Work These Methods Can Be Complex  \u0026hellip;.But\nYou can also integrate spatial methods into gridded field trials without:\n having to know anything about map projections, shapefiles or other geospatial terminology possessing a deep understanding of linear modeling techniques or empirical variograms being an R or SAS programming expert  Knowing these things is helpful, but not essential.\nA Typical Experiment  Experimental treatments fully crossed effects Blocking scheme along the expected direction of field variation  Analysis A typical linear model: $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nResponse = trial mean + treatment effect + block effect + leftover error\nWe Assume:  The error terms, or residuals, are independent of another with a shared distribution:  $$\\epsilon_i \\sim N(0,\\sigma_e)$$\nEach block captures variation unique to that block and there is no other variation related to spatial position of the experimental plots.   **How often is #2 evaluated?**  Example Analysis Average Yield by Row, Column and Block Standard Analysis of Kimberly, 2013 Wheat Variety Trial  36 soft white winter wheat cultivars 4 blocks 2 missing data points the linear model:  $Y_{ij} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$\nlibrary(nlme) lm1 \u0026lt;- lme(yield ~ cultivar, random = ~ 1|block, data = mydata, na.action = na.exclude)  What Do The Residuals Look Like? plot(lm1)  What Do The Residuals Look Like Spatially? What Do The Residuals Look Like Spatially? Global Moran\u0026rsquo;s Test for Spatial Autocorrelation $H_0$: There is no spatial autocorrelation $H_a:$ There is spatial autocorrelation!\nThis uses a simple weighting matrix that weights all neighbors that share a plot border (the chess-based \u0026ldquo;rook\u0026rdquo; formation) equally.\n## ## Monte-Carlo simulation of Moran I ## ## data: mydata$residuals ## weights: weights ## omitted: 88, 97 ## number of simulations + 1: 1000 ## ## statistic = 0.15869, observed rank = 997, p-value = 0.003 ## alternative hypothesis: greater  Handling Spatial Autocorrelation in Areal Data Areal data = finite region divided into discrete sub-regions (plots) with aggregated outcomes\nOptions:\n model row and column trends  good for known gradients (hill slope, salinity patterns)   assume plots close together are more similar than plots far apart. The errors terms can be modelled based on proximity, but there is no trial-wide trend  autoregressive models (AR) models utilizing \u0026ldquo;gaussian random fields\u0026rdquo; for continuously varying data (e.g. point data) Smoothing splines nearest neighbor    Basic Linear Model $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$ $$\\epsilon_i \\sim N(0,\\sigma)$$\nIf N = 4:\n$$e_i ~\\sim N \\Bigg( 0, \\left[ {\\begin{array}{ccc} \\sigma^2 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; \\sigma^2 \u0026amp; 0 \u0026amp; 0\\ 0 \u0026amp; 0 \u0026amp; \\sigma^2 \u0026amp; 0\\\n0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\sigma^2 \\end{array} } \\right] \\Bigg) $$\nThe variance-covariance matrix indicates a shared variance and all off-diagonals are zero, that is, the errors are uncorrelated.\nLinear Model with Autoregressive (AR) Errors Same linear model: $$Y_{ij} = \\mu + A_i + \\epsilon_{ij}$$\nDifferent variance structure:\n$$e_i ~\\sim N \\Bigg( 0, = \\sigma^2 \\left[ {\\begin{array}{cc} 1 \u0026amp; \\rho \u0026amp; \\rho^2 \u0026amp; \\rho^3 \\\n\\rho \u0026amp; 1 \u0026amp; \\rho \u0026amp; \\rho^2 \\\n\\rho^2 \u0026amp; \\rho \u0026amp; 1 \u0026amp; \\rho \\\n\\rho^3 \u0026amp; \\rho^2 \u0026amp; \\rho \u0026amp; 1 \\ \\end{array} } \\right] \\Bigg) $$\n $\\rho$ is a correlation parameter ranging from -1 to 1 where 0 is no correlation and values approaching 1 indicate spatial correlation. The \u0026ldquo;one\u0026rdquo; in AR1 means that only the next most adjacent point is considered. There can be AR2, AR3, \u0026hellip;, ARn models.  The Separable AR1 x AR1 model   AR1xAR1 assumes correlation in two directions, row and column. It estimates $\\sigma$, $\\rho_{column}$, and $\\rho_{row}$ often a good choice since plot are rectangular and hence autocorrelation will differ by direction (\u0026ldquo;anistropy\u0026rdquo;)  More Notes on Separable AR1xAR1  From a statistical standpoint, it\u0026rsquo;s one of the more intuitive models The implementation in R is a little shaky  several packages, all hard to use and incompatible with other R packages   It is implemented in SAS Some proprietary software implements this (AsREML), others do not (Agrobase)  Semivariance and Empirical Variograms A measure of spatial correlation based on all pairwise correlations in a data set, binned by distance apart:\n$\\gamma^2(h) = \\frac{1}{2} Var[Z(s+h)-Z(s)]$\n$Z(s)$ = observed data at point $s$.\n$Z(s)$ = observed data at another point $h$ distance from point $s$.\nFor a data set with $N$ observation, there are this many pairwise points:\n$\\frac{N(N-1)}{2}$\nEmpirical Variogram This uses semivariance to mathematically relate spatial correlations with distance\nrange = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance\nSemivariance \u0026amp; Empirical Variograms  There are many difference mathematical models for explaining semivariance:  exponential, Gaussian, Matérn, spherical, \u0026hellip;   It is usually used for kriging, or prediction of a new point through spatial interpolation It can also be used in a linear model where local observations are used to predict a data point in addition to treatment effects Bonus: R and SAS are really good at this!  Adding Semivariance to a Linear Model Copy data into new object so we can assign it a new class (and remove missing data):\nlibrary(gstat); library(sp); library(dplyr) mydata_sp \u0026lt;- mydata %\u0026gt;% filter(!is.na(yield))  Establish coordinates for data set to make it an sp object (\u0026ldquo;spatial points\u0026rdquo;):\ncoordinates(mydata_sp) \u0026lt;- ~ row + range  Set the maximum distance for looking at pairwise correlations:\nmax_dist \u0026lt;- 0.5*max(dist(coordinates(mydata_sp)))  Adding Semivariance to a Linear Model Calculate a sample variogram:\nsemivar \u0026lt;- variogram(yield ~ block + cultivar, data = mydata_sp, cutoff = max_dist, width = max_dist/12) nugget_start \u0026lt;- min(semivar$gamma)  Adding Semivariance to a Linear Model The empirical variogram:\nplot(semivar)  Adding Semivariance to a Linear Model Set up models for fitting variograms:\nvgm1 \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential vgm2 \u0026lt;- vgm(model = \u0026quot;Sph\u0026quot;, nugget = nugget_start) # spherical vgm3 \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian  Fit the variogram models to the data:\nvariofit1 \u0026lt;- fit.variogram(semivar, vgm1) variofit2 \u0026lt;- fit.variogram(semivar, vgm2) variofit3 \u0026lt;- fit.variogram(semivar, vgm3)  Adding Semivariance to a Linear Model Look at the error terms to see which model is the best at minimizing error.\n## [1] \u0026quot;exponential: 26857.3\u0026quot;  ## [1] \u0026quot;spherical: 26058.3\u0026quot;  ## [1] \u0026quot;Gaussian: 41861.0\u0026quot;  The spherical model is the best at minimizing error.\nAdding Semivariance to a Linear Model plot(semivar, variofit2, main = \u0026quot;Spherical model\u0026quot;)  Adding Semivariance to a Linear Model Extract the nugget and sill information from the spherical variogram:\nnugget \u0026lt;- variofit2$psill[1] range \u0026lt;- variofit2$range[2] sill \u0026lt;- sum(variofit2$psill) nugget.effect \u0026lt;- nugget/sill # the nugget/sill ratio  Adding Semivariance to a Linear Model Build a correlation structure in nlme:\ncor.sph \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row + range, nugget = T, fixed = F, type = \u0026quot;spherical\u0026quot;, metric = \u0026quot;euclidean\u0026quot;)  Update the Model:\nlm_sph \u0026lt;- update(lm1, corr = cor.sph)  Compare Models - Log likelihood logLik(lm1)  ## 'log Lik.' -489.0572 (df=38)  logLik(lm_sph)  ## 'log Lik.' -445.4782 (df=40)  Compare Models - Post-hoc Power anova(lm1)[2,]  ## numDF denDF F-value p-value ## cultivar 35 103 1.6411 0.029  anova(lm_sph)[2,]  ## numDF denDF F-value p-value ## cultivar 35 103 2.054749 0.0028  Compare Model Predictions library(emmeans) lme_preds \u0026lt;- as.data.frame(emmeans(lm1, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model\u0026quot;) sph_preds \u0026lt;- as.data.frame(emmeans(lm_sph, \u0026quot;cultivar\u0026quot;)) %\u0026gt;% mutate(model = \u0026quot;mixed model + spatial\u0026quot;) preds \u0026lt;- rbind(lme_preds, sph_preds)  Compare Model Predictions Highest yielding wheat: \u0026lsquo;Stephens\u0026rsquo; (released in 1977)\nWhere Was Stephens Located in the Trial? More Notes  When models omit blocking, the predictions may be unchanged or they may worsen. This varies by the agronomic field, but in general, blocking a field trial and including block in the statistical model improves your experimental power and controls experimental error. There is no single spatial model that fits all However, using any spatial model is usually better than none at all When you use spatial covariates, your estimates are better and more precise. This really does help you!  What\u0026rsquo;s Next:  Track row and range information in your trial data set. Look at the tutorial! (we will also add SAS code) Try out a few models and see how it impacts your results.  The Seminar Was Brought to you by\u0026hellip;Statistical Programs!!! Statistical consulting to support the College of Agriculture and Life Sciences.\nBill Price, Director, bprice@uidaho.edu, AgSci307\nJulia Piaskowski, jpiaskowski@uidaho.edu, AgSci 305\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"207697934e1ff33ee3b594c2b0f9405a","permalink":"/slides/spatial_seminar_20210409/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/spatial_seminar_20210409/","section":"slides","summary":"= ul { color: #282828; font-size: 40px; }   A Road in Auvers After the Rain by Vincent Van Gogh\n   ### Goal: Make everyone feel more comfortable using spatial stats when analyzing field experimental data.","tags":null,"title":"Routine incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"slides"}]