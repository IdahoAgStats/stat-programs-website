[{"authors":null,"categories":null,"content":"Julia Piaskowski is a consulting statistician at the University of Idaho.\n","date":1644278400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1637107200,"objectID":"ea76c2c583835370cabcc577a3ff91a8","permalink":"/author/julia-piaskowski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/julia-piaskowski/","section":"authors","summary":"Julia Piaskowski is a consulting statistician at the University of Idaho.","tags":null,"title":"Julia Piaskowski","type":"authors"},{"authors":null,"categories":null,"content":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.\n","date":1617982200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1617982200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/statistical-programs/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/statistical-programs/","section":"authors","summary":"Statistical Programs is a unit located within the Idaho Agricultural Experimental Station serving the College of Agriculture and Life Sciences at the University of Idaho.","tags":null,"title":"Statistical Programs","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cc81c076516116756d3f5a4a4fb9202","permalink":"/author/william-price/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/william-price/","section":"authors","summary":"","tags":null,"title":"William Price","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb2df55dd082c12a119ff230831261b","permalink":"/author/xin-dai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xin-dai/","section":"authors","summary":"","tags":null,"title":"Xin Dai","type":"authors"},{"authors":null,"categories":null,"content":"   Location Sunday, November 7\n9:00am - 4:00pm\nSalt Palace Convention Center, 250D\nWhat you will learn  how to diagnose spatial covariance in field trial data how to model spatial covariance in a linear model in R and SAS how to model an empirical variogram how to pick the \u0026ldquo;best\u0026rdquo; spatial model  Workshop overview Agricultural field experiments commonly employ standard experimental designs such as randomized complete block to control for field heterogeneity. However, there is often substantial spatial variation not fully captured by blocking, particularly in large experiments. Although spatial statistics have demonstrated effectiveness in controlling localized spatial variation, they are rarely integrated into analysis of agricultural field experiments. The purpose of this workshop is to provide tools for diagnosing with-field spatial variation and accounting for that spatial variation in statistical analysis of trial data. The workshop draws heavily from our book on this subject.\nIntended Audience This workshop is open to scientists, students, technicians and anyone else who conducts planned field experiments that are arranged in a regular gridded layout. Attendees will need a laptop with R or SAS installed. Some knowledge of programming in R (if you follow the R track) or SAS (if you follow the SAS track) is assumed: setting a working directory, importing files, loading libraries, calling functions. Familiarity with randomized complete block design and how to analyze that design is also assumed.\nHow to Prepare R\nYou will need a recent version of R, available free through the Comprehensive R Archive Network (CRAN). While this is sufficient for running R scripts, You may also find it helpful to use RStudio, which provides a nice graphical user interface for R. RStudio can be downloaded here. Additionally, there are several package to download:\n check your system   SAS\nIn order to run the SAS portion of this tutorial, a valid copy of SAS Base and Stat products and a current SAS license are required. This tutorial was built using SAS 9.4 (TS1M5). Although older versions of SAS may also work, we have not evaluated this. Users can also consider downloading and using a free version of SASÂ® On Demand for Academics: Studio.\nThe workshop will use Rstudio and the standard SAS interface for R and SAS code demonstrations, respectively.\nData sets\nThe following files will be used in the workshop:\nNebraska Interstate Nursery, a wheat variety trial arranged in a randomised complete block design with 4 blocks. This data set was first described by W. Stroup (2004) and has been used extensively for spatial analysis.\nLind, a winter wheat variety trial from Washington using an augmented design. This data set was kindly donated by Kimberly Garland Campbell of the USDA-ARS.\nPlease download these in advance so you can run the R and/or SAS scripts in the workshop.\nDraft Schedule    Time Topic     9:00 welcome/intro   9:20 diagnosing spatial autocorrelation   10:00 10-minute break   10:10 code demo   11:05 row-by-column designs   11:30 empirical variograms   12:00 1-hour lunch   13:00 questions   13:15 code demo   14:00 10-minute break   14:10 splines + code demo   14:40 model compariso + code demo   15:00 augmented designn + code demo   16:00 Adjourn    This schedule may be adjusted as the workshop unfolds.\nMeet Your Instructors Julia Piaskowski is an agricultural statistician at the University of Idaho, Software Carpentry Certified Instructor and long-time R programmer.\nXin Dai is consulting statistician at Utah Agricultural Experiment Station, Utah State University with 12 years of experience in SAS programming.\n begin the workshop   This workshop is licensed under the Creative Commons Attribution-NonCommercial 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc/4.0/.\n","date":1635724800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1635724800,"objectID":"8d6c9227a908a3b2f4bfffbd8e7660e4","permalink":"/tutorials/spatial-workshop/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/","section":"tutorials","summary":"Routine inclusion of spatial statistics in planned field experiments","tags":null,"title":"Spatial Recipes for Field Trials","type":"book"},{"authors":null,"categories":null,"content":"  Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:\nR.Version()  If the version printed is not 4.0 or newer, please upgrade R.\nThis step is not required if you do not use RStudio. Open RStudio and run this code to check what version of RStudio is installed on your system:\nrstudioapi::versionInfo()  If the version printed is not 1.4 or newer, please upgrade Rstudio.\nInstall workshop packages Open R and run this script:\npackage_list \u0026lt;- c(\u0026quot;dplyr\u0026quot;, \u0026quot;tidyr\u0026quot;, \u0026quot;purrr\u0026quot;, # for standard data manipulation \u0026quot;ggplot2\u0026quot;, \u0026quot;desplot\u0026quot;, # for plotting \u0026quot;nlme\u0026quot;, \u0026quot;lme4\u0026quot;, \u0026quot;emmeans\u0026quot;, # for linear modelling \u0026quot;SpATS\u0026quot;, # for fitting splines \u0026quot;sp\u0026quot;, \u0026quot;spdep\u0026quot;, \u0026quot;gstat\u0026quot;, \u0026quot;spaMM\u0026quot;, \u0026quot;sf\u0026quot;) # for spatial modelling install.packages(package_list) sapply(package_list, require, character.only = TRUE)   Please note that the spatial packages may take awhile to install, and you may run into problems with the installation. Please attempt installation in advance of the workshop. The packages have all been successfully installed if after the sapply(package_list, require, character.only = TRUE) is run, the R output is \u0026ldquo;TRUE\u0026rdquo; for each package. If you have problems installing and/or loading any of these packages that you are not able to resolve, contact us so we can help you, preferably before the workshop.   Library Information    package usage     dplyr, tidyr, standard data manipulation   purrr for repeat functions   nlme mixed linear models with options for spatial covariates   lme4 mixed linear models with crossed random effects   ggplot, desplot standard plotting packge and extension for plotting block outlines   SpATS spline-fitting   sp preparation of spatial objects   spdep Moran\u0026rsquo;s I test   gstat for fitting empirical variogram   spaMM fits Matern covariances structure for mixed linear models   emmeans extracts marginal means from linear model objects    ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"560438cfbc510098ce3796b273c9291d","permalink":"/tutorials/spatial-workshop/prep-work/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/prep-work/","section":"tutorials","summary":"Here are instructions for how check your R installation and install packages needed for the workshop.\nCheck software versions Open R and run this code to check what version of R your system is running:","tags":null,"title":"Computational Set-up","type":"book"},{"authors":null,"categories":null,"content":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.\n  Field variation Agricultural field trials often employ popular experimental designs such as randomized complete block design to account for environmental heterogeneity. However, those techniques are quite often inadequate to fully account for spatial heterogeneity that arises due to field position, soil conditions, disease, wildlife impacts and more.\nHere is the a map of a wheat variety trial conducted in Idaho with a chloropleth map indicating plot yield. Each square represents a plot.\n  Blocking in a field trial Block is not always sufficient to account for spatial variation. Here is the same Idaho variety trial with block information overlaid:\n  The block arrangement is clearly not aligning with the field variation.\nWhen Spatial Variation is not Fully Accounted For  The treatment rankings can be wrong. Here is a plot of the cultivar ranks for yield from the Idaho variety trial when analysed with a standard linear mixed model and the same model augmented with spatial covariates.     Error terms are often correlated with each other, invalidating the downstream analysis     high error/low precision/wide confidence intervals/low experimental power  Blocking versus spatial statistics   another distracted boyfriend meme!  Researchers do not have to abandon blocking when incorporating spatial covariates into analysis of a field trial. In fact, using blocking or other experimental designs combined with spatial modelling has been shown improve the quality of the final estimates.\n","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"689de5052b0586aa34d90677aeb6b67a","permalink":"/tutorials/spatial-workshop/why-spatial/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/why-spatial/","section":"tutorials","summary":"Agricultural field trials   University Research Farm  The goal of many agricultural field trials is to provide information about crop response to a set a treatments such as soil treatments, disease pressure or crop genetic variation.","tags":null,"title":"Why Care about Spatial Variation?","type":"book"},{"authors":null,"categories":null,"content":" This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another. That correlation is expected to decline with distance. Note that is different from experiment-wide gradients, such as a salinity gradient or position on a slope.\nPlotting One of the easiest ways to diagnose spatial autocorrelation is by plotting data by its spatial position and using a heat map to indicate values of a response variable.\n  While there is always ambiguity associated with using plots for decision making, early exploration of these plots can be helpful in understanding the extent of spatial correlation.\nMoran\u0026rsquo;s I Moran\u0026rsquo;s I, sometimes called \u0026ldquo;Global Moran\u0026rsquo;s I\u0026rdquo; can be used for conducting a hypothesis test on whether there is correlation between spatial units located adjacent to one another.\n$$ I = \\frac{N}{W}\\frac{\\sum_i \\sum_j w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i(x_i - \\bar{x})^2} \\qquad i \\neq j$$\nWhere N is total number of spatial locations indexed by $i$ and $j$, x is the variable of interest, $w_{ij}$ are a spatial weights between each $i$ and $j$, and W is the sum of all weights.\nThe expected values of Moran\u0026rsquo;s I is $-1/(N-1)$. Values greater than the expected value indicate positive spatial correlation (areas close to each other are similar), while values less than that indicate dissimilarity as spatial distance between points decreases.\nDefining Neighbors There are several options for defining adjacent neighbors and how to weight each neighbor\u0026rsquo;s influence. The two common configurations for defining neighbors are the rook and queen configurations. These are exactly what their chess analogy suggests: \u0026ldquo;rook\u0026rdquo; defines neighbors in an row/column fashion, while \u0026ldquo;queen\u0026rdquo; defines neighbors in a row/column configuration an also neighbors located diagonally at a 45 degree angle from the row/column neighbors. Determining this can be complicated when working with irregularly-placed data (e.g. counties), but is quite unambiguous for lattice data common in planned field experiments.\n  Setting the values for weights is a function of both how neighbors are defined and their proximity to the unit of interest. However, a very popular method is to define each neighbor as equal fractions that sum to one, e.g. in rook formation, each neighbor is weighted 0.25 (assuming an interior plot with 4 neighbors).\nEmpirical variogram This is one of the most useful methods of determining the extent of spatial variability and will be covered in the following sections.\nCode for this section R # load libraries library(dplyr); library(ggplot2); library(desplot); library(spdep); library(sf); library(nlme) # read in data and prepare it Nin \u0026lt;- read.csv(\u0026quot;stroup_nin_wheat.csv\u0026quot;) %\u0026gt;% mutate(col.width = col * 1.2, row.length = row * 4.3) %\u0026gt;% mutate(name = case_when(is.na(as.character(rep)) ~ NA_character_, TRUE ~ as.character(gen))) %\u0026gt;% arrange(col, row) Nin_na \u0026lt;- filter(Nin, !is.na(rep)) # make exploratory plot ggplot(Nin, aes(x = row, y = col)) + geom_tile(aes(fill = yield), col = \u0026quot;white\u0026quot;) + geom_tileborder(aes(group = 1, grp = rep), lwd = 1.2) + labs(x = \u0026quot;row\u0026quot;, y = \u0026quot;column\u0026quot;, title = \u0026quot;field plot layout\u0026quot;) + theme_classic() + theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14), legend.title = element_text(size = 14), legend.text = element_text(size = 12)) ## conduct moran's I test ## # set neighbors with convenience function for grids xy_rook \u0026lt;- cell2nb(nrow = max(Nin$row), ncol = max(Nin$col), type=\u0026quot;rook\u0026quot;, torus = FALSE, legacy = FALSE) # run linear mixed model and extract residuals nin.lme \u0026lt;- lme(fixed = yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) resid_lme \u0026lt;- residuals(nin.lme) names(resid_lme) \u0026lt;- Nin$plot # two version of the Moran's I test: moran.test(resid_lme, nb2listw(xy_rook), na.action = na.exclude) moran.mc(resid_lme, nb2listw(xy_rook), 999, na.action = na.exclude)   SAS # read in data proc format; invalue has_NA 'NA' = .; ; filename NIN url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/stroup_nin_wheat.csv\u0026quot;; data alliance; infile NIN firstobs=2 delimiter=','; informat yield has_NA.; input entry $ rep $ yield col row; Row = 4.3*Row; Col = 1.2*Col; if yield=. then delete; run; # heatmap proc sgplot data=alliance; HEATMAPPARM y=Row x=Col COLORRESPONSE=yield/ colormodel=(blue yellow green); run; # linear mixed model proc mixed data=alliance; class Rep Entry; model Yield = Entry / outp=residuals; random Rep; run; # Moran's I proc variogram data=residuals plots(only)=moran ; compute lagd=1.2 maxlag=30 novariogram autocorr(assum=nor) ; coordinates xc=row yc=col; var resid; run;   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"08ba8f989a275150ef8b304f2ff276ec","permalink":"/tutorials/spatial-workshop/diagnosis/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/diagnosis/","section":"tutorials","summary":"This workshop is concerned with areal data, that is, data that occurs in discrete units (plots, in most cases). This attribute of trial data impacts many aspects of spatial analysis   Spatial autocorrelation refers to similarity between points that are close to one another.","tags":null,"title":"Diagnosing Spatial Autocorrelation","type":"book"},{"authors":null,"categories":null,"content":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):\n$$\\gamma(h) = \\frac{1}{2|N(h)|}\\sum_{N(h)}(x_i - x_j)^2$$\nSemivariances are binned for distance intervals. The average values for semivariance and distance interval can be fit to mathematical models designed to explain how semivariance changes over distance.\nThree important concepts of an empirical variogram are nugget, sill and range\n  Example Empirical Variogram   range = distance up to which is there is spatial correlation sill = uncorrelated variance of the variable of interest nugget = measurement error, or short-distance spatial variance and other unaccounted for variance  2 other concepts:\n partial sill = sill - nugget nugget effect = the nugget/sill ratio, interpreted opposite of $r^2$ (the closer it is to 1, the less the amount of spatial autocorrelation)  Correlated Error Models Many equations exist for modelling semivariance patterns. A deep knowledge of these is not required to fit an empirical variogram to a model. Here are a few popular examples.\nExponential\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0 \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r}) } \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Exponential Variogram  Gaussian\n(a squared version of the exponential model)\n$$ \\gamma (h) = \\begin{cases}0 \u0026amp; \\text{if }h=0, \\\\\nC_0+C_1 \\left [ 1-e^{-(\\frac{h}{r})^2} \\right] \u0026amp; \\text{if } h\u0026gt;0 \\end{cases}$$\nwhere\n$$ C_0 = nugget $$ $$ C_1 = partial : sill $$ $$ r = range $$\n  Theoretical Gaussian Variogram  MatÃ©rn\n\u0026lt;/An extremely complicated mathematical model/\u0026gt;\n  Empirical MatÃ©rn Variogram  There are many more models: Cauchy, logistic, spherical, sine, \u0026hellip;.\n For more information on these models, see this workshop\u0026rsquo;s accompanying online book on this topic and additional SAS resources.   Variogram fitting Picking the right model is done both by comparing the sum of squares of error for different models and by\nNot all variables have spatial autocorrelation\n  Not all fitted variogram models are worthy\n  Variogram gone bad  Code for this section The following scripts build upon work done in previous section(s).\nR # load libraries library(gstat); library(spaMM) # set up spatial object Nin_spatial \u0026lt;- Nin_na coordinates(Nin_spatial) \u0026lt;- ~ col.width + row.length # add attribte class(Nin_spatial) # establish max distance for variogram estimation max_dist = 0.6*max(dist(coordinates(Nin_spatial))) # calculate empirical variogram resid_var1 \u0026lt;- gstat::variogram(yield ~ rep + gen, cutoff = max_dist, width = max_dist/15, # 15 is the number of bins data = Nin_spatial) plot(resid_var1) # empirical variogram #Note: To fit a large number of models, the function 'autofitVariogram()' from the package automap can be used (is it calling gstat::variogram) # starting value for the nugget nugget_start \u0026lt;- min(resid_var1$gamma) # initialise the model (this does not do much) Nin_vgm_exp \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) # exponential Nin_vgm_gau \u0026lt;- vgm(model = \u0026quot;Gau\u0026quot;, nugget = nugget_start) # Gaussian Nin_vgm_mat \u0026lt;- vgm(model = \u0026quot;Mat\u0026quot;, nugget = nugget_start) # Matern # actually do some fitting! Nin_variofit_exp \u0026lt;- fit.variogram(resid_var1, Nin_vgm_exp) Nin_variofit_gau \u0026lt;- fit.variogram(resid_var1, Nin_vgm_gau) Nin_variofit_mat \u0026lt;- fit.variogram(resid_var1, Nin_vgm_mat, fit.kappa = T) plot(resid_var1, Nin_variofit_exp, main = \u0026quot;Exponential model\u0026quot;) plot(resid_var1, Nin_variofit_gau, main = \u0026quot;Gaussian model\u0026quot;) plot(resid_var1, Nin_variofit_mat, main = \u0026quot;Matern model\u0026quot;) attr(Nin_variofit_exp, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_gau, \u0026quot;SSErr\u0026quot;) attr(Nin_variofit_mat, \u0026quot;SSErr\u0026quot;) # parameters: Nin_variofit_gau nugget \u0026lt;- Nin_variofit_gau$psill[1] # measurement error (other random error) range \u0026lt;- Nin_variofit_gau$range[2] # distance to establish independence between data points sill \u0026lt;- sum(Nin_variofit_gau$psill) # maximum semivariance   SAS # calculate semivariance and compute empirical variogram proc variogram data=residuals plots(only)=(semivar); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; var resid; run; # fit models to the empirical variogram proc variogram data=residuals plots(only)=(fitplot); coordinates xc=Col yc=Row; compute lagd=1.2 maxlags=30; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"23ef7960956bfe73c013ddd0ae065781","permalink":"/tutorials/spatial-workshop/variograms/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/variograms/","section":"tutorials","summary":"The empirical variogram is a visual tool for quantifying spatial covariance. It uses semivariance ($\\gamma$), which is a measure of covariance between points or units ($i$ and $j$) as a function of distance ($h$):","tags":null,"title":"Empirical Variograms","type":"book"},{"authors":null,"categories":null,"content":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model. In RCBD design, often the treatments are treated as fixed and the block effect as random.\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ block\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)$\nHere is an expanded version of the last term:\n$$ \\epsilon_{ij} ~\\sim N \\Bigg( 0, \\left[ { \\begin{array}{ccc} \\sigma \u0026amp; \\cdots \u0026amp; 0 \\\\\n\\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\n0 \u0026amp; \\cdots \u0026amp; \\sigma \\end{array} } \\right] \\Bigg) $$\nThis is a mathematically representation of iid, independent and identically distributed, an assumption of linear models. When there is spatial autocorrelation, observations closer to one another are correlated, so the off-diagonals in the variance-covariance matrix are not zero.\nSpatial models seek to mathematically model this covariance so it is properly accounted for during hypothesis testing and prediction.\nCode for this section The following scripts build upon work done in previous section(s).\nR library(emmeans); library() # (nlme and gstat should already be loaded) library(spaMM) # for running `corMatern()` # standard linear model nin_lme \u0026lt;- lme(yield ~ gen, random = ~1|rep, data = Nin, na.action = na.exclude) # extract the esimated marginal means for variety preds_lme \u0026lt;- as.data.frame(emmeans(nin_lme, \u0026quot;gen\u0026quot;)) # use information from the variogram fitting for intialising the parameters nugget \u0026lt;- Nin_variofit_gau$psill[1] range \u0026lt;- Nin_variofit_gau$range[2] sill \u0026lt;- sum(Nin_variofit_gau$psill) nugget.effect \u0026lt;- nugget/sill # initalise the covariance structure (from the nlme package) cor.gaus \u0026lt;- corSpatial(value = c(range, nugget.effect), form = ~ row.length + col.width, nugget = T, fixed = F, type = \u0026quot;gaussian\u0026quot;, metric = \u0026quot;euclidean\u0026quot;) # update the rcbd model nin_gaus \u0026lt;- update(nin_lme, corr = cor.gaus) # extract predictions for 'gen' preds_gaus \u0026lt;- as.data.frame(emmeans(nin_gaus, \u0026quot;gen\u0026quot;) # a similar procedure can be follow for other models # but we are going to take a shortcut and not specify the parameters # exponential cor.exp \u0026lt;- corSpatial(form = ~ row.length + col.width, nugget = T, fixed = F) nin_exp \u0026lt;- update(nin_lme, corr = cor.exp) preds_exp \u0026lt;- as.data.frame(emmeans(nin_exp, \u0026quot;gen\u0026quot;)) # Matern structure cor.mat \u0026lt;- corMatern(form = ~ row.length + col.width, nugget = T, fixed = F) nin_matern \u0026lt;- update(nin_lme, corr = cor.mat) preds_mat \u0026lt;- as.data.frame(emmeans(nin_matern, \u0026quot;gen\u0026quot;)   SAS proc mixed data=alliance ; class entry rep; model yield = entry ; random rep; lsmeans entry/cl; ods output LSMeans=NIN_RCBD_means; title1 'NIN data: RCBD'; run; proc mixed data=alliance maxiter=150; class entry; model yield = entry /ddfm=kr; repeated/subject=intercept type=sp(gau) (Row Col) local; parms (11) (22) (19); lsmeans entry/cl; ods output LSMeans=NIN_Spatial_means; title1 'NIN data: Gaussian Spatial Adjustment'; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"1f334ed878f21376cf002826a7cd2176","permalink":"/tutorials/spatial-workshop/correlated-error-models/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/correlated-error-models/","section":"tutorials","summary":"Now that we have a sense of how to model spatial variation, the next step is to incorporate that into a linear model. The starting point is the linear mixed model.","tags":null,"title":"Linear Models with Correlated Errors","type":"book"},{"authors":null,"categories":null,"content":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation. However, sometimes that is accurately describe a field trial. There can be experiment-wide gradients due to position on a slope, proximity to an influential environmental factor (e.g. a road), and so on. In these instances, those gradients should be modelled as a trend.\nBlocking Blocking is one example of modelling an experiment wide-trend:\n  The expectation is that each block will capture and model existing variation within it. This becomes difficult to justify as blocks become large.\nRows \u0026amp; Ranges Recall the RCBD model from the previous section:\n$$Y_ij = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ij}$$\nTrials rows and ranges can likewise be modelled directly through expansion of that model (and omitting block since it full represented by column):\n$$Y_ijk = \\mu + \\alpha_i + \\beta_j + \\gamma_k + \\epsilon_{ijk}$$\n$Y_ij$ is the independent variable\n$\\mu$ is the overall mean\n$\\alpha_i$ is the effect due to the $i^{th}$ treatment\n$\\beta_j$ is the effect due to the $j^{th}$ row $\\gamma_k$ is the effect due to the $k^{th}$ range (or column)\n$\\epsilon_{ij}$ are the error terms distributed as $N ~\\sim (0,\\sigma)\nCode for Trends The following scripts build upon work done in previous section(s).\nR # load libraries library(lme4) # exploratory plots boxplot(yield ~ rep, data = Nin, xlab = \u0026quot;block\u0026quot;, col = \u0026quot;red2\u0026quot;) boxplot(yield ~ row, data = Nin, xlab = \u0026quot;row\u0026quot;, col = \u0026quot;dodgerblue2\u0026quot;) boxplot(yield ~ col, data = Nin, xlab = \u0026quot;column\u0026quot;, col = \u0026quot;gold\u0026quot;) ## row/column model ## # data prep Nin$rowF = as.factor(Nin$row) Nin$colF = as.factor(Nin$col) # specify model nin.rc \u0026lt;- lmer(yield ~ gen + (1|colfF) + (1|rowF), data = Nin, na.action = na.exclude) # extract random effects for row and column ranef(nin_rc) # extract predictions nin_rc \u0026lt;- as.data.frame(emmeans(nin.rc, \u0026quot;gen\u0026quot;))   SAS # exploratory boxplots proc sgplot data=alliance; vbox yield/category=rep FILLATTRS=(color=red) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Col FILLATTRS=(color=yellow) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; proc sgplot data=alliance; vbox yield/category=Row FILLATTRS=(color=blue) LINEATTRS=(color=black) WHISKERATTRS=(color=black); run; # row/column model proc mixed data=alliance ; class entry rep; model yield = entry row col/ddfm=kr; random rep; lsmeans entry/cl; ods output LSMeans=NIN_row_col_means; title1 'NIN data: RCBD'; run;   Splines Polynomial splines are an additional method for spatial adjustment and represent a more non-parametric method that does not rely on estimation or modeling of variograms. Instead, it uses the raw data and residuals to fit a surface to the spatial data and adjust the variance covariance matrix accordingly.\nCode for Splines The following scripts build upon work done in previous section(s).\nR nin_spline \u0026lt;- SpATS(response = \u0026quot;yield\u0026quot;, spatial = ~ PSANOVA(col, row, nseg = c(10,20), degree = 3, pord = 2), genotype = \u0026quot;gen\u0026quot;, random = ~ rep, # + rowF + colF, data = Nin, control = list(tolerance = 1e-03, monitoring = 0)) preds_spline \u0026lt;- predict(nin_spline, which = \u0026quot;gen\u0026quot;) %\u0026gt;% dplyr::select(gen, emmean = \u0026quot;predicted.values\u0026quot;, SE = \u0026quot;standard.errors\u0026quot;)   SAS proc glimmix data=alliance ; class entry rep; effect sp_r = spline(row col); model yield = entry sp_r/ddfm=kr; random row col/type=rsmooth; lsmeans entry/cl; ods output LSMeans=NIN_smooth_means; title1 'NIN data: RCBD'; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"2302b9cbb0105016d563cd9dea408057","permalink":"/tutorials/spatial-workshop/trend-modelling/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/trend-modelling/","section":"tutorials","summary":"The spatial models introduced in this workshop assume that spatial variation is localised and within a trial, plots located sufficiently far apart are independent of each other with no apparent spatial correlation.","tags":null,"title":"Modelling Spatial Trends","type":"book"},{"authors":null,"categories":null,"content":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model. Some approaches include:\n Comparing model fitness (e.g. AIC, BIC, log likelihood). Although the methods are not nested, hence precluding a log likelihood ratio test, we can compare raw values for each fit statistic. Be careful doing this in R since linear modelling packages use different estimation procedures for maximum likelihood and REML estimation that are not comparable. Comparing post-hoc power (that is, the p-values for the treatments) Comparing standard error of the estimates (i.e. precision)   Comparing changes in the coefficient of variation (CV, $\\sigma/\\mu$) is not recommended because in many spatial models, field variation has been re-partitioned to the error term when it was (erroneously) absorbed by the other experimental effects. As a result, the CV can increase in spatial models even when inclusion of spatial covariates results in better model fit.   Unfortunately, there is no one method for unambiguously returning the the best estimates and true ranks of the treatments. Likewise, there is no one spatial method that works best in all situations and field trials.\nCode for this section R library(tidyr) # remove some objects we don't need (and will interfere with downstream processes) rm(nin_variofit, nin_vgm) rm(nin_vgm, nin_variofit, nugget, sill, range, nugget.effect) # assemble objects into a list nlme_mods \u0026lt;- list(nin_lme, nin_exp, nin_gaus, nin_matern) names(nlme_mods) \u0026lt;- c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;) # extract log likelihood, AIC, BIC data.frame(loglik = sapply(nlme_mods, logLik), AIC = sapply(nlme_mods, AIC), BIC = sapply(nlme_mods, AIC, k = log(nrow(Nin_na)))) %\u0026gt;% arrange(desc(loglik)) # (higher is better for loglik, lower is better for AIC and BIC) # compare post-hoc power # conduct ANOVA anovas \u0026lt;- lapply(nlme_mods[-7], function(x){ aov \u0026lt;- as.data.frame(anova(x))[2,]}) # bind all the output together a \u0026lt;- bind_rows(anovas) %\u0026gt;% mutate(model = c(\u0026quot;LMM\u0026quot;, \u0026quot;exponential\u0026quot;, \u0026quot;gaussian\u0026quot;, \u0026quot;matern\u0026quot;, \u0026quot;row-col\u0026quot;)) %\u0026gt;% arrange(desc(`p-value`)) %\u0026gt;% select(c(model, 1:4)) rownames(a) \u0026lt;- 1:nrow(a) a ## compare precision of estimates all.preds \u0026lt;- mget(ls(pattern = \u0026quot;^preds_*\u0026quot;)) errors \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;SE\u0026quot;) pred.names \u0026lt;- gsub(\u0026quot;preds_\u0026quot;, \u0026quot;\u0026quot;, names(errors)) error_df \u0026lt;- bind_cols(errors) colnames(error_df) \u0026lt;- pred.names boxplot(error_df, ylab = \u0026quot;standard errors\u0026quot;, xlab = \u0026quot;linear model\u0026quot;, col = \u0026quot;dodgerblue3\u0026quot;) # compare predictions preds \u0026lt;- lapply(all.preds, \u0026quot;[\u0026quot;, \u0026quot;emmean\u0026quot;) preds_df \u0026lt;- bind_cols(preds) colnames(preds_df) \u0026lt;- pred.names preds_df$gen \u0026lt;- preds_exp$gen # plot changes in rank lev \u0026lt;- c(\u0026quot;lme\u0026quot;, \u0026quot;exp\u0026quot;, \u0026quot;gaus\u0026quot;, \u0026quot;mat\u0026quot;) pivot_longer(preds_df, cols = !gen, names_to = \u0026quot;model\u0026quot;, values_to = \u0026quot;emmeans\u0026quot;) %\u0026gt;% mutate(model = factor(model, levels = lev)) %\u0026gt;% ggplot(aes(x = model, y = emmeans, group = gen)) + geom_point(size = 5, alpha = 0.5, col = \u0026quot;navy\u0026quot;) + geom_line() + ylab(\u0026quot;yield means for gen\u0026quot;) + theme_minimal()   SAS data NIN_RCBD_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_RCBD_means; RCB_est = estimate; RCB_se = stderr; run; data NIN_Spatial_means (drop=tvalue probt alpha estimate stderr lower upper df); set NIN_Spatial_means; Sp_est = estimate; Sp_se = stderr; run; proc sort data=NIN_RCBD_means; by entry; run; proc sort data=NIN_Spatial_means; by entry; run; data compare; merge NIN_RCBD_means NIN_Spatial_means; by entry; run; proc rank data=compare out=compare descending; var RCB_est Sp_est; ranks RCB_Rank Sp_Rank; run; proc sort data=compare; by Sp_rank; run; proc print data=compare(obs=15); var entry rcb_est Sp_est rcb_se sp_se rcb_rank sp_rank; run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"ff5c09d1e0d47b213293120cb7478b45","permalink":"/tutorials/spatial-workshop/model-comparison/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/model-comparison/","section":"tutorials","summary":"Now that we have built these spatial models, how do we pick the right one? Unfortunately, there is no one model that works best in all circumstances. In addition, there is no single way for choosing the best model.","tags":null,"title":"Comparing Models","type":"book"},{"authors":null,"categories":null,"content":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible. Often, the primary goal of the studies using this design is to rank or select genotypes.\nAugmented models are analyzed in a fundamentally different method than RCBD models due to the large number of unreplicated observations. To adjust for the lack of replication, only a select set of treatments, usually of known performance, are replicated in the experiments. The error estimated from these replicated treatments is used in the analysis to evaluate the remaining genotypes.\nThere are multiple way to specify an augmented model depending on what the researcher wants to know.\nModel specification #1 $$ Y_{ij} = \\tau_i + \\beta(\\tau)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\tau_i$ is the effect of each check and the average effect of all unreplicated treatments $ \\beta(\\tau)_{ij}$ is is the effect of the $j^{th}$ unreplicated treatment nested within the overall effect of unreplicated treatments  This model evaluates:\n The difference between all checks and the average of the unreplicated treatments. The difference between the unreplicated treatments.  Model specification #2 $$ Y_{ij} = \\delta_i + \\gamma(\\delta)_{ij} $$\nwhere:\n $ Y_{ij}$ is the response variable $ \\delta_i$ is the average effect of all checks and the average effect of all unreplicated treatments (so there are only 2 treatment levels) $ \\gamma(\\delta)_{ij}$ is is the effect of the $j^{th}$ treatment nested within the either unreplicated treatments or the check observations  This model evaluates:\n The difference between the average of the checks and the average of the unreplicated treatments The difference between all treatments  These models are described more in depth in BurgueÃ±o et al, 2018, along with a helpful discussion on when to treat any of these effects as fixed or random\nThe data used here refer to a wheat genotype evaluation study carried out near Lind Washington. The study looked at 922 unreplicated genotypes (ânameâ) accompanied by 9 replicated check wheat cultivars.\nCode for this section The following scripts build upon work done in previous section(s).\nR # (if not already loaded) library(dplyr); library(nlme); library(ggplot2) library(gstat); library(sp) # read in data aug_data_origin \u0026lt;- read.csv(\u0026quot;data/augmented_lind.csv\u0026quot;, na.strings = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;999999\u0026quot;)) %\u0026gt;% slice(-1) %\u0026gt;% # first line not needed mutate(yieldkg = yieldg/1000) # to prevent overflow # summarise the genoytypic data by checks/not checks gen_sum \u0026lt;- group_by(aug_data_origin, name) %\u0026gt;% summarise(counts = n()) %\u0026gt;% mutate(delta = case_when( counts \u0026gt; 1 ~ \u0026quot;check\u0026quot;, counts == 1 ~ \u0026quot;unrep\u0026quot;)) # need info on just the checks checks \u0026lt;- gen_sum %\u0026gt;% filter(delta == \u0026quot;check\u0026quot;) # more summarise steps for different augmented modes gen_sum2 \u0026lt;- gen_sum %\u0026gt;% mutate(gamma = name) %\u0026gt;% mutate(tau = case_when( delta == \u0026quot;check\u0026quot; ~ gamma, delta == \u0026quot;unrep\u0026quot; ~ \u0026quot;unreplicate_obs\u0026quot;)) %\u0026gt;% mutate(beta = case_when( delta == \u0026quot;unrep\u0026quot; ~ gamma, delta == \u0026quot;check\u0026quot; ~ gamma)) # merge original data set with info on treatment levels aug_data \u0026lt;- aug_data_origin %\u0026gt;% select(name, prow, pcol, yieldkg, yieldg) %\u0026gt;% mutate(row = prow*11.7, col = pcol*5.5) %\u0026gt;% full_join(gen_sum2, by = \u0026quot;name\u0026quot;) ## modelling aug1 \u0026lt;- lme(fixed = yieldg ~ tau, random = ~ 1|tau/beta, data = aug_data, na.action = na.exclude) # extract residuals aug_data$res \u0026lt;- residuals(aug1) # plot residual chloroepleth map: ggplot(aug_data, aes(y = row, x = col)) + geom_tile(aes(fill = res)) + scale_fill_gradient(low = \u0026quot;yellow\u0026quot;, high = \u0026quot;black\u0026quot;) + scale_x_continuous(breaks = seq(1,max(aug_data$row), 1)) + scale_y_continuous(breaks = 1:max(aug_data$col)) + coord_equal() + theme_void() # add spatial covariates aug_spatial \u0026lt;- aug_data %\u0026gt;% filter(!is.na(res)) coordinates(aug_spatial) \u0026lt;- ~ col + row max_dist = 0.5*max(dist(coordinates(aug_spatial))) aug_vario \u0026lt;- gstat::variogram(res ~ 1, cutoff = max_dist, width = max_dist/10, data = aug_spatial) # optional to run: nugget_start \u0026lt;- min(aug_vario$gamma) aug_vgm \u0026lt;- vgm(model = \u0026quot;Exp\u0026quot;, nugget = nugget_start) aug_variofit \u0026lt;- fit.variogram(aug_vario, aug_vgm) plot(aug_vario, aug_variofit, main = \u0026quot;Exponential model\u0026quot;) cor_exp \u0026lt;- corSpatial(form = ~ row + col, nugget = T, fixed = F, type = \u0026quot;exponential\u0026quot;) aug1_sp \u0026lt;- update(aug1, corr = cor_exp) # spatial parameters: aug1_sp$modelStruct$corStruct # extract BLUPs for unreplicated lines: aug1_blups \u0026lt;- ranef(aug1_sp)$beta %\u0026gt;% rename(yieldg = '(Intercept)') # look at variance components VarCorr(aug1_sp) ##### OR ####### # another formulation # delta estimates effects of replicated versus unreplicated genotypes # gamma estimates the effecs of all genotypes evaluated in the trial aug2 \u0026lt;- lme(fixed = yieldkg ~ delta, random = ~ 1|delta/gamma, data = aug_data, na.action = na.exclude) aug2_sp \u0026lt;- update(aug2, corr = cor_exp) # spatial parameters: aug2_sp$modelStruct$corStruct # extract BLUPs for unreplicated lines: aug_blups2 \u0026lt;- ranef(aug2_sp)$gamma %\u0026gt;% rename(yieldg = '(Intercept)') # look at variance components VarCorr(aug1_sp)   SAS filename AUG url \u0026quot;https://raw.githubusercontent.com/IdahoAgStats/guide-to-field-trial-spatial-analysis/master/data/AB19F5_LIND.csv\u0026quot;; PROC IMPORT OUT= WORK.augmented DATAFILE= AUG DBMS=CSV REPLACE; GETNAMES=YES; DATAROW=2; RUN; data augmented; set augmented; if yieldg = 999999 or yieldg=. then delete; /* Remove missing values */ prow=prow*11.7; /*convert row and column indices to feet */ pcol=pcol*5.5; run; proc freq noprint data=augmented; tables name/out=controls; run; data controls; set controls; if count \u0026gt;1; run; proc sort data=controls; by name; run; proc sort data=augmented; by name; run; data augmented; merge augmented controls; by name; if count=. then d2=2; /* Unreplicated */ else d2=1; /* Replicated */ yieldkg=yieldg/1000; run; PROC mixed data=augmented; class name d2; model yieldkg = d2/noint outp=residuals ddf=229 229; lsmeans d2; *lsmeans name(d2)/slice = d2; run; proc sgplot data=residuals; HEATMAPPARM y=pRow x=pCol COLORRESPONSE=resid/ colormodel=(cx014458 cx1E8C6E cxE1FE01); title1 'Field Map'; run; proc variogram data=residuals plots(only)=(fitplot); where yieldkg ^= .; coordinates xc=pcol yc=pRow; compute lagd=6.6 maxlags=25; model form=auto(mlist=(gau, exp, pow, sph) nest=1); var resid; run; PROC mixed data=augmented; class name d2; model yieldkg = d2 name(d2)/outp=adjresiduals ddf=229 229; lsmeans d2; repeated/subject=intercept type=sp(pow)(prow pcol) local; ods output SolutionR =parms; parms (0.074) (0.0051)(0.475) ; *lsmeans name(d2)/slice = d2; # alot of output!! run;   ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"a80942952d1d93f00f9edee58c345366","permalink":"/tutorials/spatial-workshop/augmented/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/augmented/","section":"tutorials","summary":"The augmented experimental design is a special design where there is a large number of unreplicated plots interspersed with frequent checks that are replicated. This type of model is useful when the number of treatments is very large and/or replication is either impossible or unfeasible.","tags":null,"title":"Augmented Designs","type":"book"},{"authors":null,"categories":null,"content":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time. However, investigating spatial correlation in a field trial and controlling for it if necessary using any of the methods developed for this is recommended over doing nothing.\nThere is no denying that work is needed to develop scripts that automate this process so researchers can routinely incorporate spatial covariance into field trial analysis. Many current R tools are unwieldy to use and have insufficient options to support variety trial analysis.\nUntil this situation is improved, it is probably wisest to focus on using spatial models that are well-supported at this time. Any of the options implemented in the nlme package (or that work with that package) are decent choices with excellent support for extracting least-squares means, running ANOVA, and standard model diagnostics. Furthermore, nlme supports generalized linear models. INLA is established is supported by a large and growing user base, and breedR is likewise well established.\nOther resources   Incorporating Spatial Analysis into Agricultural Field Experiments, a more comprehensive version of this tutorial\n  CRAN task view on analysis of spatial data\n  Other R packages\n     package usage     breedR mixed modelling with AR1xAR1 estimation   inla Bayesian modelling with options for spatial covariance structure   Mcspatial nonparametric spatial analysis, (no longer on CRAN)   ngspatial spatial models with a focus on generalized linear models   sommer mixed models, including an AR1xAr1 model   spamm MatÃ©rn covariance structure   spANOVA spatial lag models for field trials   spatialreg spatial functions for areal data    The package sommer implements a version of the AR1xAR1 covariance structure. However, it does not estimate the parameter $\\rho$. The user must specify the $\\rho$ and that value is not optimized in the restricted maximum likelihood estimation. Both BreedR and inla implement an AR1xAR1 covariance structure. Additional, SAS and the proprietary software asreml can implement a mixed model with this covariance structure.\nBooks for the deep dive     Statistics for Spatial Data\n  Applied Spatial Data Analysis with R, available for free\n  Spatio-Temporal Statistics With R (also free)\n  Spatial Data Analysis in Ecology and Agriculture Using R\n  ","date":1636243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636243200,"objectID":"4b22d71fa46669752a29a46e057cd574","permalink":"/tutorials/spatial-workshop/conclusion/","publishdate":"2021-11-07T00:00:00Z","relpermalink":"/tutorials/spatial-workshop/conclusion/","section":"tutorials","summary":"Spatial analysis can be challenging, but I think it is worth the effort to learn and implement in analysis of field trials. Incorporating spatial statistics into analysis of feel trials can be overwhelming at time.","tags":null,"title":"Final thoughts","type":"book"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"You may find yourself needing to do something repeatedly in R. Sure, you can cut-and-paste and change that one thing, or two things, or five things, but this quickly becomes cumbersome. The result can be a very long R file and the likelihood of making a mistake that you don\u0026rsquo;t notice increases (e.g. forgetting to change a variable or an argument).\nThere is the general rule of DRY: don\u0026rsquo;t repeat yourself. In practice, if something has to pasted more than twice, then consider writing a function to accomplish that aim instead.\nIntroduction to Writing Functions R functions follow a general structure:\nmy_function_name \u0026lt;- function(argument1, argument2) { final_output \u0026lt;- action(argument1, argument2) return(final_output) }  A classic function example is conversion of temperature from Fahrenheit to celsius:\nfahr_to_cel \u0026lt;- function(fahr) { # function that converts temperature in degrees Fahrenheit to celsius # input: fahr: numeric value representing temp in degrees fahrenheit # output: kelvin: numeric converted temp in celsius celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  This function takes a numeric value, temperature in Fahrenheit, and outputs another numeric value, that same value converted to celsius.\nFunction usage:\nfahr_to_cel(80)  ## [1] 26.66667  This function can be called for a large number of values at once:\n# create a vector of 100 numbers randomly sampled between 1 and 100. x1 \u0026lt;- sample(1:100, 100, replace = TRUE) x2 \u0026lt;- fahr_to_cel(x1)  If you provide the incorrect type of data, the function will not work:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr - 32: non-numeric argument to binary operator  A More Complex Example Often we want to do something more complicated. One thing I want to do frequently is build boxplots.\nFirst, simulate some data. This data set has two categorical variables, cat1 and cat2, and 4 different continuous variables generated through data simulation.\nmydata \u0026lt;- data.frame(cat1 = rep(c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;D\u0026quot;), 10), cat2 = rep(c(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;), each = 20), var1 = rnorm(40), var2 = runif(40), var3 = rlnorm(40), var4 = rbeta(40, 1, 5))  Next, write up an example of what you want to do. In this example, let\u0026rsquo;s create a boxplot:\nboxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;)  Now, let\u0026rsquo;s put that in a function. Start with the basic function framework:\nboxplot_func = function() { }  Next, insert the function code. Start by cut-and-pasting the original boxplot command ran above:\nboxplot_func = function() { boxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;) }  Decide on arguments you want to control and put that inside the function() parentheses. Probably the independent and dependent variable (x and y, respectively), as well as the data frame needed.\nPut those arguments inside function().\nboxplot_func = function(df, x, y) { boxplot(var1 ~ cat1, data = mydata, main = NA, col = \u0026quot;orangered\u0026quot;) }  Then indicate where those arguments are used in the function. They must be used in the function (otherwise, why have them?).\nboxplot_func = function(df, x, y) { boxplot(y ~ x, data = df, main = NA, col = \u0026quot;orangered\u0026quot;) }  However, if you try to use this function, it won\u0026rsquo;t work. The argument y ~ x is a special class of object in R called \u0026ldquo;formula\u0026rdquo; and the formatting and object type must match. Formulas are used widely in R for linear modelling and follow the exact same convention:\ny ~ x  Note that the information on either side of ~ can become more complicated. (but not in this function).\nSo, create a formula object using the functions formula() and paste() within the function and insert that into the basic boxplot code. If you don\u0026rsquo;t know how to use those function, type ?formula and ?paste into the console to learn more about them.\nboxplot_func = function(df, x, y) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = \u0026quot;orangered\u0026quot;) }  What if you want the ability to change the color? Insert a new argument and replace it in the function body:\nboxplot_func = function(x, y, color) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = color) }  If you want the option to set the some options or if you choose not to, have the function choose values automatically as defaults, that can be done by naming the argument in formula().\nboxplot_func = function(df = mydata, x, y, color = \u0026quot;springgreen\u0026quot;) { f = formula(paste(y, \u0026quot;~\u0026quot;, x)) boxplot(f, data = df, main = NA, col = color) }  Next step is to run the function as it is currently written (highlight the function code and click run). Next, make sure you add this function (i.e. boxplot_funct = function(...)) to your R environment by running it in the console. You can check it exists in your R global environment as thus:\nls()  ## [1] \u0026quot;boxplot_func\u0026quot; \u0026quot;fahr_to_cel\u0026quot; \u0026quot;mydata\u0026quot; \u0026quot;x1\u0026quot; \u0026quot;x2\u0026quot;  Now, call the function and make sure it does what we want?\nboxplot_func(mydata, \u0026quot;cat1\u0026quot;, \u0026quot;var1\u0026quot;)  boxplot_func(x = \u0026quot;cat2\u0026quot;, y = \u0026quot;var1\u0026quot;, col = \u0026quot;darkcyan\u0026quot;)  boxplot_func(mydata, \u0026quot;cat2\u0026quot;, \u0026quot;var4\u0026quot;, col = \u0026quot;khaki\u0026quot;)  What if it doesn\u0026rsquo;t do what we want? What if you get strange output? No output? Or strange error messages? Herein comes the world of debugging (another blog post for another day).\nError Checking and Error Messages You may have noticed earlier this strange error message:\nfahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr - 32: non-numeric argument to binary operator  This is a very confusing message. We most certainly provided a \u0026ldquo;non-numeric argument\u0026rdquo;, but what is a \u0026ldquo;binary operator\u0026rdquo;? Turns out that is a programming speak for a standard mathematical operations addition, subtraction, multiplication and division (called \u0026lsquo;binary\u0026rsquo; because they take two inputs). Still, we are likely to encounter more strange error messages written in programmer speak that confuse us or someone else using our functions. We can write custom error messages that are produced when certain errors occur.\nHere is the temperature conversion function again:\nfahr_to_cel \u0026lt;- function(fahr) { celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  Since they can only take numeric argument, maybe we can start for checking for this? There are a few options in do this. One of the easiest to use is stopifnot(). This functions takes the general form: stopifnot(\u0026quot;my custom error message\u0026quot; = test). What constitutes a \u0026lsquo;test\u0026rsquo; is an R expression that returns a TRUE or FALSE value after being evaluated. Examples of this are is.character(x), is.NA(x), x \u0026gt; 0 and so on. For each of these statements, the expectation is that R will true a TRUE or FALSE. If the test does not do this reliably (e.g. you may not be able to evaluate x \u0026gt; 0 if x is non-numeric), then a different test is needed.\nIn our case, we can use is.numeric().\nfahr_to_cel \u0026lt;- function(fahr) { stopifnot(\u0026quot;input is not numeric\u0026quot; = is.numeric(fahr)) celsius \u0026lt;- ((fahr - 32) * (5 / 9)) return(celsius) }  Let\u0026rsquo;s run some test cases:\nfahr_to_cel(30)  ## [1] -1.111111  fahr_to_cel(\u0026quot;thirty\u0026quot;)  ## Error in fahr_to_cel(\u0026quot;thirty\u0026quot;): input is not numeric  As expected, the first one worked and the second generated an error message.\nNaturally, this is a very trivial example, but if you write more complicated functions with the intent of them automatically accomplishing a goal for you, these error messages can be helpful.\nFunctions and Tidy Evaluation If you\u0026rsquo;ve worked with the tidyverse, you know it handles input a bit differently. In summary, quotes are used far less often. This makes writing function quite challening at times and required the use of the double curly braces, {{}} or the \u0026ldquo;bang-bang\u0026rdquo; operator !!.\nWhat if we wanted to do a boxplot function using ggplot?\nHere\u0026rsquo;s what the code would look like:\nlibrary(ggplot2) mydata \u0026lt;- data.frame(cat = rep(c(\u0026quot;AA\u0026quot;, \u0026quot;BB\u0026quot;), each = 50), obs = c(rnorm(50), runif(50))) ggplot(mydata, aes(x = cat, y = obs)) + geom_boxplot(aes(fill = cat), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic()  But, if you try to write a function following the usual rules, it won\u0026rsquo;t work properly:\ngboxplot_func \u0026lt;- function(x1, y1) { ggplot(mydata, aes(x = x1, y = y1)) + geom_boxplot(aes(fill = x1), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic() }  gboxplot_func(cat, obs)  ## Error in FUN(X[[i]], ...): object 'obs' not found  This one works, but the results are crazy.\ngboxplot_func(\u0026quot;cat\u0026quot;, \u0026quot;obs\u0026quot;)  Why are the results wonky? Because while this function see \u0026ldquo;mydata\u0026rdquo; has 100 observations, it cannot connect \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;obs\u0026rdquo; to the data frame.\nThis is where the special operators come in:\ngboxplot_func2 \u0026lt;- function(x1, y1) { ggplot(mydata, aes(x = {{x1}}, y = {{y1}})) + geom_boxplot(aes(fill = {{x1}}), alpha = 0.5) + geom_jitter(height = 0, width = 0.2, alpha = 0.6, color = \u0026quot;black\u0026quot;) + guides(fill = \u0026quot;none\u0026quot;) + theme_classic() }  gboxplot_func2(cat, obs)  The curly braces enable us to insert unquoted tidy variables and use ggplot.\nThis is a very brief introduction to tidy evaluation. More information on tidy evaluation is available for ggplot and dplyr.\n","date":1644278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"5928faba22c787e9eb2af9367751510c","permalink":"/post/writing-r-functions/","publishdate":"2022-02-08T00:00:00Z","relpermalink":"/post/writing-r-functions/","section":"post","summary":"You may find yourself needing to do something repeatedly in R. Sure, you can cut-and-paste and change that one thing, or two things, or five things, but this quickly becomes cumbersome.","tags":["functions"],"title":"How to Write Custom Functions in R","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R"],"content":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility. The result is that it is easy to misspecify a model or make other mistakes. The information below is intended to serve as a guide through the R ANOVA wilderness.\nPackages Needed There are many packages to load. Here is a (very) brief summary of what each package does.\n   Package Purpose     car Anova() function to extract type III \u0026amp; II sums of squares   lme4 mixed models   nlme mixed models, non-linear models, alternative covariance structures   emmeans for extracting least squares means and contrasts   lmer test improved summary functions of lmer objects   dplyr data organization   forcats for managing categorical data   agridat has many agricultural data sets   agricolae has options for many common agricultural experimental designs    library(car) library(lme4) library(nlme) library(emmeans) #in older version of R, you may need to install \u0026quot;multcompView\u0026quot; separately to access full functionality of the emmeans package library(lmerTest) library(dplyr) library(forcats) library(agridat)  Formula Notation There are some consistent features across ANOVA methods in R. Formula notation is often used in the R syntax for ANOVA functions. It looks like this: $Y ~ X, where Y is the dependent variable (the response) and X is/are the independent variable(s) (e.g. the experimental treatments).\nmy_formula \u0026lt;- formula(Y ~ treatment1 + treatment2) class(my_formula)  ## [1] \u0026quot;formula\u0026quot;  my_formula  ## Y ~ treatment1 + treatment2  Often the independent variables (i,e, the treatments or the x variables) are expected to be factors, another type of R object:\nmy_var \u0026lt;- c(rep(\u0026quot;low\u0026quot;,5), rep(\u0026quot;high\u0026quot;, 5)) class(my_var) #check what variable type it is  ## [1] \u0026quot;character\u0026quot;  Although \u0026ldquo;my_var\u0026rdquo; is not type factor, it is type \u0026ldquo;character\u0026rdquo; which is automatically converted to a factor in lm(), lmer(), lme() and many other linear modeling functions. There are some packages that do not follow this convention, so it\u0026rsquo;s helpful to read function documentation, especially if you get unexpected results.\nVariables like year, which are often imported as a number or integer, do need to be converted to a factor or a character variable prior to analysis. Otherwise, they will be interpreted as a number in linear modelling and treated as a covariate, e.g, 2020 would be 2,020. Here is one way to do this conversion:\nmy_factor \u0026lt;- as.character(my_var) # convert to a character class(my_factor) # check variable type to confirm  ## [1] \u0026quot;character\u0026quot;  my_factor \u0026lt;- as.factor(my_var) # convert to a factor class(my_factor) # check variable type again to confirm  ## [1] \u0026quot;factor\u0026quot;  The choice of whether to convert a categorical variable to a character or factor depends on the comfort of the user with these structures and package requirements.\nSometimes, there is a need to alter the order of treatment levels (that is, how R sees those levels). The default behavior of R is to order categorical levels alphanumerically. However, sometimes there are reasons you may not want this (for example, you want to set a particular reference level as the first factor level).\nBelow is one example of how to reorder factor levels in a variable. The first step is to see which levels are present in the variable and how they are ordered:\nlevels(my_factor)  ## [1] \u0026quot;high\u0026quot; \u0026quot;low\u0026quot;  Once that is known, you can use that information to manually set the levels and their order. Note that spelling of each level much match what is actually present in the variable. Unmatched levels in the variable will be set to NA automatically by R in the following step.\nmy_factor \u0026lt;- factor(my_factor, levels = c(\u0026quot;low\u0026quot;, \u0026quot;high\u0026quot;)) levels(my_factor) # check the new ordering  ## [1] \u0026quot;low\u0026quot; \u0026quot;high\u0026quot;  Knowing the level order is important because in the implementation of ANOVA in R, the first level is treated as the reference level. Manipulating factors is a challenging task in R. The package forcats contains a collection of accessory functions for managing factors (\u0026ldquo;forcats\u0026rdquo; = for categories). The tutorial uses the forcats function fct_drop().\nMore on formulas:\nThe formula first shown, Y ~ treatment1 + treatment2, includes main effects only. Other formula notation includes the symbols : and *, indicating notation for interaction only and main effects plus the interaction term, respectively.\nformula(Y ~ treatment1:treatment2) # interaction only  ## Y ~ treatment1:treatment2  formula(Y ~ treatment1*treatment2) # interaction plus main effects  ## Y ~ treatment1 * treatment2  These two formulas are equivalent:\nformula(Y ~ treatment1 + treatment2 + treatment1:treatment2) formula(Y ~ treatment1*treatment2)  Perhaps you can see from these examples that formulas are a really just a collections of characters (that is, a string) and exist independent of any data set. Later, we will need to link these formulas to a data set to actually construct a linear model and conduct statistical analysis.\nANOVA for fixed effects models Here is a function for reporting the number of missing data in each column. There are other ways to do this, but I find this function easy enough to write and use.\ncount_na \u0026lt;- function(df) { apply(df, 2, function(x) sum(is.na(x))) }  Completely Randomised design First, load the data set \u0026ldquo;warpbreaks\u0026rdquo; (a data set from base R). This is an old data set with variables for wool type (A and B) and tension on the loom (L, M or H). The response variable is \u0026ldquo;breaks\u0026rdquo;, the number of times the wool thread breaks on industrial looms.\nI always like to have a quick look at the data before running any statistical tests. So, here we go:\ndata(warpbreaks) count_na(warpbreaks)  ## breaks wool tension ## 0 0 0  str(warpbreaks)  ## 'data.frame':\t54 obs. of 3 variables: ## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... ## $ wool : Factor w/ 2 levels \u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tension: Factor w/ 3 levels \u0026quot;L\u0026quot;,\u0026quot;M\u0026quot;,\u0026quot;H\u0026quot;: 1 1 1 1 1 1 1 1 1 2 ...  warpbreaks$wool \u0026lt;- factor(warpbreaks$wool, levels = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;)) table(warpbreaks$wool, warpbreaks$tension)  ## ## L M H ## A 9 9 9 ## B 9 9 9 ## C 0 0 0  hist(warpbreaks$breaks, col = \u0026quot;gold\u0026quot;)  boxplot(breaks ~ wool, data = warpbreaks, col = \u0026quot;orangered\u0026quot;)  boxplot(breaks ~ tension, data = warpbreaks, col = \u0026quot;chartreuse\u0026quot;) #why not have colorful plots?  This data set has 2 treatments. We don\u0026rsquo;t know if there is an interaction between the variables, yet. A good start is to run a linear model using lm() function, the linear regression function. As a reminder, ANOVA is a special case of the linear regression model where the predictors (the experimental treatments) are categories rather than a continuous variable.\n# run standard linear model for main effects only lm.mod1 \u0026lt;- lm(breaks ~ wool + tension, data = warpbreaks) # extract type III sums of squares from that model Anova(lm.mod1, type = \u0026quot;3\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: breaks ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 20827.0 1 154.3226 \u0026lt; 2.2e-16 *** ## wool 450.7 1 3.3393 0.073614 . ## tension 2034.3 2 7.5367 0.001378 ** ## Residuals 6747.9 50 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # run a linear model with main effects and interactions lm.mod2 \u0026lt;- lm(breaks ~ wool*tension, data = warpbreaks) # ...and type III sums of squares Anova(lm.mod2, type = \u0026quot;III\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: breaks ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 17866.8 1 149.2757 2.426e-16 *** ## wool 1200.5 1 10.0301 0.0026768 ** ## tension 2468.5 2 10.3121 0.0001881 *** ## wool:tension 1002.8 2 4.1891 0.0210442 * ## Residuals 5745.1 48 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  FYI\nfunctions only shown as an example and not actually run.\n# this function runs type II sums of squares: Anova(lm.mod2, type = \u0026quot;II\u0026quot;) # this function runs type I sums of squares: anova(lm.mod2)  A few comments on types of sums of squares: \nAs a reminder, the type of sums of squares used in statistical tests can impact the results and subsequent interpretation. Type I, sums of squares tests for statistical significance by adding one variable to the model at time (and hence is also called \u0026ldquo;sequential\u0026rdquo;). If there is any unbalance in the treatments, the type I sums of squares are dependent on the order variables are added to the model and hence is often not the best choice for many agricultural experiment. Type III sums of squares (also called \u0026ldquo;partial\u0026rdquo; or \u0026ldquo;marginal\u0026rdquo;) evaluates the statistical significance of variable or interaction, assuming that the other variables are in the model. This is a decent default option. The last option is Type II sums of squares, which is the best option when you are sure there are no interactions between variables. If there is complete balance among treatments (each treatment is observed the same number of times with no missing data), then there is no need to concern yourself with these different types of sums of squares.\nCompare Models # conduct an F test comparing the models anova(lm.mod1, lm.mod2)  ## Analysis of Variance Table ## ## Model 1: breaks ~ wool + tension ## Model 2: breaks ~ wool * tension ## Res.Df RSS Df Sum of Sq F Pr(\u0026gt;F) ## 1 50 6747.9 ## 2 48 5745.1 2 1002.8 4.1891 0.02104 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # also, consider doing a stepwise approach for finding the best model: step(lm.mod2)  ## Start: AIC=264.02 ## breaks ~ wool * tension ## ## Df Sum of Sq RSS AIC ## \u0026lt;none\u0026gt; 5745.1 264.02 ## - wool:tension 2 1002.8 6747.9 268.71  ## ## Call: ## lm(formula = breaks ~ wool * tension, data = warpbreaks) ## ## Coefficients: ## (Intercept) woolB tensionM tensionH woolB:tensionM ## 44.56 -16.33 -20.56 -20.00 21.11 ## woolB:tensionH ## 10.56  Model diagnostics plot(lm.mod2) #this will produce 4 plots of residuals  shapiro.test(resid(lm.mod2)) #standard shapiro-wilk test.  ## ## Shapiro-Wilk normality test ## ## data: resid(lm.mod2) ## W = 0.98686, p-value = 0.8162  # this variable could be analyzed with a log-normal model instead  Least squares means \u0026amp; contrasts The emmeans package is a flexible package for extracting the estimated marginal means (in SAS, the \u0026ldquo;least squares means\u0026rdquo;) from different linear models. It is compatible with a large number of R linear modelling packages.\nHere is some code for extracting the marginal means and conducting contrasts.\n# extract least squares means for 'tension' (lsm \u0026lt;- emmeans(lm.mod2, ~ tension))  ## NOTE: Results may be misleading due to involvement in interactions  ## tension emmean SE df lower.CL upper.CL ## L 36.4 2.58 48 31.2 41.6 ## M 26.4 2.58 48 21.2 31.6 ## H 21.7 2.58 48 16.5 26.9 ## ## Results are averaged over the levels of: wool ## Confidence level used: 0.95  emmeans(lm.mod2, \u0026quot;wool\u0026quot;)  ## NOTE: Results may be misleading due to involvement in interactions  ## wool emmean SE df lower.CL upper.CL ## A 31.0 2.11 48 26.8 35.3 ## B 25.3 2.11 48 21.0 29.5 ## ## Results are averaged over the levels of: tension ## Confidence level used: 0.95  All pairwise comparisons within each level of tension:\ncontrast(lsm, \u0026quot;pairwise\u0026quot;)  ## contrast estimate SE df t.ratio p.value ## L - M 10.00 3.65 48 2.742 0.0229 ## L - H 14.72 3.65 48 4.037 0.0006 ## M - H 4.72 3.65 48 1.295 0.4049 ## ## Results are averaged over the levels of: wool ## P value adjustment: tukey method for comparing a family of 3 estimates  Conduct custom contrasts comparing \u0026lsquo;Low\u0026rsquo; tension versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;High\u0026rsquo; and \u0026lsquo;High\u0026rsquo; versus \u0026lsquo;Medium\u0026rsquo; and \u0026lsquo;Low\u0026rsquo;.\n# see the order of each level in a factor levels(warpbreaks$tension)  ## [1] \u0026quot;L\u0026quot; \u0026quot;M\u0026quot; \u0026quot;H\u0026quot;  # construct a list of constructs # each item must be same length as the the number of levels present in the variable tension # use numbers and fracions to indicate the contrasting levels # the numbers must sum to zero cList \u0026lt;- list(LvMH = c(1, -0.5, -0.5), # low vs high + medium HvLM = c(0.5, 0.5, -1)) # high vs low + medium # check that each contrast sums to zero lapply(cList, sum)  ## $LvMH ## [1] 0 ## ## $HvLM ## [1] 0  # perform custom contrast and include a Bonferroni adjustment summary(contrast(lsm, cList, adjust = \u0026quot;bonferroni\u0026quot;))  ## contrast estimate SE df t.ratio p.value ## LvMH 12.36 3.16 48 3.914 0.0006 ## HvLM 9.72 3.16 48 3.078 0.0069 ## ## Results are averaged over the levels of: wool ## P value adjustment: bonferroni method for 2 tests  Randomised Complete Block Design (RCBD) - fixed effects model This example uses rapeseed yield data from multiple locations, years and cultivars. Within a single location or year, the replication is often balanced.\nLoad Data and examine:\ndata(shafii.rapeseed) # from the 'agridat' package rapeseed1987 \u0026lt;- shafii.rapeseed %\u0026gt;% filter(year == 87) %\u0026gt;% mutate(block = fct_drop(rep), Cv = fct_drop(gen), loc = fct_drop(loc)) str(rapeseed1987)  ## 'data.frame':\t216 obs. of 7 variables: ## $ year : int 87 87 87 87 87 87 87 87 87 87 ... ## $ loc : Factor w/ 9 levels \u0026quot;GGA\u0026quot;,\u0026quot;ID\u0026quot;,\u0026quot;MT\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ rep : Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ gen : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ... ## $ yield: num 961 1329 1781 1698 1605 ... ## $ block: Factor w/ 4 levels \u0026quot;R1\u0026quot;,\u0026quot;R2\u0026quot;,\u0026quot;R3\u0026quot;,..: 1 2 3 4 1 2 3 4 1 2 ... ## $ Cv : Factor w/ 6 levels \u0026quot;Bienvenu\u0026quot;,\u0026quot;Bridger\u0026quot;,..: 1 1 1 1 2 2 2 2 3 3 ...  count_na(rapeseed1987)  ## year loc rep gen yield block Cv ## 0 0 0 0 0 0 0  table(rapeseed1987$Cv, rapeseed1987$loc) #experiment has 1 rep per block  ## ## GGA ID MT NC OR SC TGA TX WA ## Bienvenu 4 4 4 4 4 4 4 4 4 ## Bridger 4 4 4 4 4 4 4 4 4 ## Cascade 4 4 4 4 4 4 4 4 4 ## Dwarf 4 4 4 4 4 4 4 4 4 ## Glacier 4 4 4 4 4 4 4 4 4 ## Jet 4 4 4 4 4 4 4 4 4  hist(rapeseed1987$yield, col = \u0026quot;gold\u0026quot;)  boxplot(yield ~ Cv, data = rapeseed1987, col = \u0026quot;orangered\u0026quot;)  boxplot(yield ~ loc, data = rapeseed1987, col = \u0026quot;chartreuse\u0026quot;)  Analyse experiment:\n# for this example, the analysis will only be done for a single year # block is nested within location # if each block had a unique name, 'Error(block)' would suffce shaf.aov \u0026lt;- aov(yield ~ Cv*loc + Error(block), data = rapeseed1987) summary(shaf.aov)  ## ## Error: block ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Residuals 3 336565 112188 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Cv 5 3203992 640798 2.645 0.025111 * ## loc 8 318197192 39774649 164.165 \u0026lt; 2e-16 *** ## Cv:loc 40 22707425 567686 2.343 0.000103 *** ## Residuals 159 38523267 242285 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(shaf.aov, ~ Cv | loc)  ## Note: re-fitting model with sum-to-zero contrasts  ## loc = GGA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1442 245 161 959 1926 ## Bridger 1363 245 161 880 1847 ## Cascade 1505 245 161 1021 1988 ## Dwarf 1295 245 161 811 1779 ## Glacier 1681 245 161 1197 2164 ## Jet 1091 245 161 607 1575 ## ## loc = ID: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1242 245 161 759 1726 ## Bridger 947 245 161 463 1430 ## Cascade 773 245 161 290 1257 ## Dwarf 932 245 161 448 1415 ## Glacier 1111 245 161 627 1595 ## Jet 1064 245 161 580 1548 ## ## loc = MT: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 2616 245 161 2132 3100 ## Bridger 2828 245 161 2345 3312 ## Cascade 2916 245 161 2433 3400 ## Dwarf 3452 245 161 2968 3935 ## Glacier 3307 245 161 2823 3790 ## Jet 3660 245 161 3177 4144 ## ## loc = NC: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1001 245 161 517 1485 ## Bridger 1064 245 161 581 1548 ## Cascade 745 245 161 262 1229 ## Dwarf 1014 245 161 530 1497 ## Glacier 1229 245 161 746 1713 ## Jet 1674 245 161 1190 2157 ## ## loc = OR: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 4556 245 161 4072 5039 ## Bridger 2530 245 161 2046 3013 ## Cascade 3336 245 161 2852 3819 ## Dwarf 3932 245 161 3448 4415 ## Glacier 4185 245 161 3702 4669 ## Jet 3220 245 161 2736 3703 ## ## loc = SC: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 2500 245 161 2016 2983 ## Bridger 2705 245 161 2221 3189 ## Cascade 2119 245 161 1635 2602 ## Dwarf 1894 245 161 1410 2377 ## Glacier 2717 245 161 2234 3201 ## Jet 2833 245 161 2349 3316 ## ## loc = TGA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 1258 245 161 774 1741 ## Bridger 1868 245 161 1384 2351 ## Cascade 1708 245 161 1224 2191 ## Dwarf 873 245 161 389 1356 ## Glacier 1453 245 161 970 1937 ## Jet 954 245 161 470 1438 ## ## loc = TX: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 838 245 161 354 1322 ## Bridger 1069 245 161 585 1553 ## Cascade 735 245 161 251 1218 ## Dwarf 988 245 161 505 1472 ## Glacier 952 245 161 468 1435 ## Jet 1408 245 161 925 1892 ## ## loc = WA: ## Cv emmean SE df lower.CL upper.CL ## Bienvenu 4375 245 161 3891 4859 ## Bridger 4604 245 161 4120 5087 ## Cascade 4464 245 161 3981 4948 ## Dwarf 3974 245 161 3490 4458 ## Glacier 4740 245 161 4256 5224 ## Jet 4344 245 161 3861 4828 ## ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  ANOVA for mixed models (models with random and fixed effects)\nRandom effects are often those treatments levels drawn from a large population of possible treatment levels and there is interest in understanding the distribution and variance of that population. This in contrast to fixed effects, where the inferences are restricted to the treatment levels tested.\nBlocking factors and Year are often considered random factors because a researcher is not interested in particular years or a blocking factor. When there is unbalanced replication, the variance components should be estimated with maximum likelihood or REML, which implies use of the packages \u0026ldquo;lmer\u0026rdquo; and/or \u0026ldquo;nlme\u0026rdquo;.\nRandomised Complete Block Design (RCBD) - mixed effects The \u0026ldquo;shafii.rapeseed\u0026rdquo; data set will be used for this section.\nAnalyse experiment using a mixed model:\nThis uses the function lme() from the package \u0026ldquo;nlme\u0026rdquo;. Functionally, it is very similar to calling lme4::lmer(). The degrees of freedom are different (lmer() is using Satterthwaite\u0026rsquo;s approximation), but the p-values are the same.\n# turn year into the factor \u0026quot;Year\u0026quot; shafii.rapeseed$Year \u0026lt;- as.factor(shafii.rapeseed$year) # create a blocking variable that is unique for each location-by-year combination # so R doesn't conflate \u0026quot;R1\u0026quot; from one location/year with another location/year shafii.rapeseed$Rep \u0026lt;- as.factor(paste(shafii.rapeseed$loc, shafii.rapeseed$year, shafii.rapeseed$rep, sep = \u0026quot;_\u0026quot;)) shaf.lme \u0026lt;- lme(fixed = yield ~ gen*loc + Year, random = ~ 1|Rep, data = shafii.rapeseed, method = \u0026quot;REML\u0026quot;) # view sum of squares table # when anova() is called for an lme object, the function called is actually anova.lme() anova(shaf.lme, type = \u0026quot;marginal\u0026quot;) # \u0026quot;marginal\u0026quot; is equivalent to type III sums of squares  ## numDF denDF F-value p-value ## (Intercept) 1 470 16.204597 0.0001 ## gen 5 470 1.092341 0.3637 ## loc 13 92 13.074492 \u0026lt;.0001 ## Year 2 92 2.035054 0.1365 ## gen:loc 65 470 2.575753 \u0026lt;.0001  Anova(shaf.lme, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 *** ## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 *** ## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # FYI: use \u0026quot;anova(model.lme)\u0026quot; for type I sums of squares # lmer notation shaf.lmer \u0026lt;- lmer(yield ~ gen*loc + Year + (1|Rep), data = shafii.rapeseed, REML = T) anova(shaf.lmer, type = \u0026quot;marginal\u0026quot;)  ## Marginal Analysis of Variance Table with Satterthwaite's method ## Sum Sq Mean Sq NumDF DenDF F value Pr(\u0026gt;F) ## gen 1860586 372117 5 470.00 1.0923 0.3637 ## loc 57901484 4453960 13 159.37 13.0745 \u0026lt; 2.2e-16 *** ## Year 1386524 693262 2 92.00 2.0351 0.1365 ## gen:loc 57034691 877457 65 470.00 2.5758 5.499e-09 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Anova(shaf.lmer, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 16.2046 1 5.686e-05 *** ## gen 5.4617 5 0.3622 ## loc 169.9684 13 \u0026lt; 2.2e-16 *** ## Year 4.0701 2 0.1307 ## gen:loc 167.4239 65 5.579e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Diagnostics, model building plot(shaf.lme)  qqnorm(shaf.lme, abline = c(0, 1))  Least squares means # for cultivar (lme.means.cv \u0026lt;- emmeans(shaf.lme, \u0026quot;gen\u0026quot;))  ## NOTE: Results may be misleading due to involvement in interactions  ## gen emmean SE df lower.CL upper.CL ## Bienvenu 2432 112 92 2211 2654 ## Bridger 2314 112 92 2092 2536 ## Cascade 2184 112 92 1962 2406 ## Dwarf 2308 112 92 2087 2530 ## Glacier 2463 112 92 2242 2685 ## Jet 2304 112 92 2082 2525 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95  # for location (lme.means.loc \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;))  ## NOTE: Results may be misleading due to involvement in interactions  ## loc emmean SE df lower.CL upper.CL ## GGA 1682 329 92 1030 2335 ## ID 4217 261 92 3698 4736 ## KS 1120 476 92 174 2066 ## MS 2204 476 92 1258 3150 ## MT 3339 474 92 2398 4280 ## NC 1328 329 92 676 1981 ## NY 3139 476 92 2193 4085 ## OR 3292 329 92 2640 3945 ## SC 1819 261 92 1300 2338 ## TGA 1028 261 92 509 1547 ## TN 2543 476 92 1597 3490 ## TX 827 329 92 174 1479 ## VA 2282 328 92 1631 2932 ## WA 3861 261 92 3342 4380 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95  # for cultivar means within each location lme.means.int \u0026lt;- emmeans(shaf.lme, ~ gen | loc + Year) # this code would produce location means within each cultivar # emmeans(model.lme, ~ loc | gen)) # also: # emmeans(model.lme, ~ loc | gen)) provides the same estimates as 'emmeans(model.lme, ~ gen | loc))'  Pairwise Contrasts: # all pairwise pairs(lme.means.cv)  ## contrast estimate SE df t.ratio p.value ## Bienvenu - Bridger 118.57 87.6 470 1.353 0.7548 ## Bienvenu - Cascade 248.34 87.6 470 2.834 0.0539 ## Bienvenu - Dwarf 124.11 87.6 470 1.417 0.7170 ## Bienvenu - Glacier -31.00 87.6 470 -0.354 0.9993 ## Bienvenu - Jet 128.70 87.6 470 1.469 0.6843 ## Bridger - Cascade 129.77 87.6 470 1.481 0.6765 ## Bridger - Dwarf 5.54 87.6 470 0.063 1.0000 ## Bridger - Glacier -149.57 87.6 470 -1.707 0.5277 ## Bridger - Jet 10.13 87.6 470 0.116 1.0000 ## Cascade - Dwarf -124.23 87.6 470 -1.418 0.7161 ## Cascade - Glacier -279.34 87.6 470 -3.188 0.0190 ## Cascade - Jet -119.64 87.6 470 -1.366 0.7477 ## Dwarf - Glacier -155.10 87.6 470 -1.770 0.4861 ## Dwarf - Jet 4.59 87.6 470 0.052 1.0000 ## Glacier - Jet 159.70 87.6 470 1.823 0.4521 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: tukey method for comparing a family of 6 estimates  # plot results plot(lme.means.cv, comparison = T)  plot(lme.means.loc, comparison = T, horizontal = F) # rotate plots to vertical position  ## Warning: Comparison discrepancy in group \u0026quot;1\u0026quot;, GGA - OR: ## Target overlap = 0.0083, overlap on graph = -0.0111  # blue bars = lsmeans confidence 95% confidence intervals # red arrows. pairwise differences (overlapping arrows = not significantly different)  For those who want the letters assigned to treatments based on all pairwise comparisons, it\u0026rsquo;s an unwieldy road:\nlibrary(multcomp) # this will need to be installed if you do not already have it tukey \u0026lt;- glht(shaf.lme, linfct = mcp(loc = \u0026quot;Tukey\u0026quot;)) ### extract information cld_tukey \u0026lt;- cld(tukey) print(cld_tukey)  ## GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ac\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;bc\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;ab\u0026quot; \u0026quot;a\u0026quot; \u0026quot;a\u0026quot; \u0026quot;bc\u0026quot;  Interaction plots can also be done:\n(but, it gets unwieldy)\nplot(lme.means.int, comparison = T, adjust = \u0026quot;tukey\u0026quot;)  Other pre-set contrasts # compare to a control, e.g. \u0026quot;Bridger\u0026quot; levels(shafii.rapeseed$gen)  ## [1] \u0026quot;Bienvenu\u0026quot; \u0026quot;Bridger\u0026quot; \u0026quot;Cascade\u0026quot; \u0026quot;Dwarf\u0026quot; \u0026quot;Glacier\u0026quot; \u0026quot;Jet\u0026quot;  # Bridger is listed in position 2 of the factor 'shafii.rapeseed$gen' # so '2' is set as the reference level in the following contrast statement: # \u0026quot;trt.vs.ctrlk\u0026quot; (treatment versus control treatment k) is a specific option to compare all treatment levels to a user-defined level # by default, it will use the last level as the reference level contrast(lme.means.cv, \u0026quot;trt.vs.ctrlk\u0026quot;, ref = 2)  ## contrast estimate SE df t.ratio p.value ## Bienvenu - Bridger 118.57 87.6 470 1.353 0.5118 ## Cascade - Bridger -129.77 87.6 470 -1.481 0.4315 ## Dwarf - Bridger -5.54 87.6 470 -0.063 0.9998 ## Glacier - Bridger 149.57 87.6 470 1.707 0.3034 ## Jet - Bridger -10.13 87.6 470 -0.116 0.9990 ## ## Results are averaged over the levels of: loc, Year ## Degrees-of-freedom method: containment ## P value adjustment: dunnettx method for 5 tests  Search ?contrast.emmGrid to see full list of options for preset contrasts.\nCustom contrasts # example: contrast Western locations versus Southern locations # first, find out what levels are present unique(shafii.rapeseed$loc)  ## [1] GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA ## Levels: GGA ID KS MS MT NC NY OR SC TGA TN TX VA WA  # next create a contrast list # this is a list of coefficients as long your list of treatment levels # indicating what coefficients to give each treatment level # in this example, levels \u0026quot;ID\u0026quot;, \u0026quot;MT\u0026quot;, \u0026quot;OR\u0026quot;, and \u0026quot;WA\u0026quot; are contrasted versus # \u0026quot;NC\u0026quot;, \u0026quot;SC\u0026quot;, \u0026quot;MS\u0026quot;, \u0026quot;TN\u0026quot;, \u0026quot;TX\u0026quot; and \u0026quot;VA\u0026quot; cList \u0026lt;- list(West_V_South = c(0, 1/4, 0, -1/6, 1/4, -1/6, 0, 1/4, -1/6, 0, -1/6, -1/6, -1/6, 1/4)) # check that each contrast sums to zero: lapply(cList, sum)  ## $West_V_South ## [1] 5.551115e-17  lme.means.loc2 \u0026lt;- emmeans(shaf.lme, \u0026quot;loc\u0026quot;, contr = cList)  ## NOTE: Results may be misleading due to involvement in interactions  summary(lme.means.loc2)  ## $emmeans ## loc emmean SE df lower.CL upper.CL ## GGA 1682 329 92 1030 2335 ## ID 4217 261 92 3698 4736 ## KS 1120 476 92 174 2066 ## MS 2204 476 92 1258 3150 ## MT 3339 474 92 2398 4280 ## NC 1328 329 92 676 1981 ## NY 3139 476 92 2193 4085 ## OR 3292 329 92 2640 3945 ## SC 1819 261 92 1300 2338 ## TGA 1028 261 92 509 1547 ## TN 2543 476 92 1597 3490 ## TX 827 329 92 174 1479 ## VA 2282 328 92 1631 2932 ## WA 3861 261 92 3342 4380 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## West_V_South 1843 233 92 7.910 \u0026lt;.0001 ## ## Results are averaged over the levels of: gen, Year ## Degrees-of-freedom method: containment  # same contrast can also be done within each level of 'gen': emmeans(shaf.lme, ~ loc | gen, contr = cList)  ## $emmeans ## gen = Bienvenu: ## loc emmean SE df lower.CL upper.CL ## GGA 1785 379 92 1032.31 2537 ## ID 4742 303 92 4140.13 5345 ## KS 1179 546 92 94.60 2263 ## MS 2455 546 92 1371.47 3539 ## MT 2825 544 92 1745.38 3904 ## NC 1330 379 92 577.36 2082 ## NY 2934 546 92 1849.69 4018 ## OR 4118 379 92 3365.98 4870 ## SC 1844 303 92 1241.42 2446 ## TGA 893 303 92 290.99 1496 ## TN 2965 546 92 1880.59 4049 ## TX 919 379 92 167.04 1671 ## VA 2124 378 92 1373.34 2875 ## WA 3943 303 92 3340.44 4545 ## ## gen = Bridger: ## loc emmean SE df lower.CL upper.CL ## GGA 1470 379 92 718.17 2223 ## ID 3591 303 92 2989.15 4194 ## KS 1091 546 92 7.35 2175 ## MS 2478 546 92 1393.89 3562 ## MT 3037 544 92 1957.63 4117 ## NC 1479 379 92 727.28 2232 ## NY 3130 546 92 2045.60 4214 ## OR 2564 379 92 1811.99 3316 ## SC 2282 303 92 1679.58 2884 ## TGA 1603 303 92 1000.66 2205 ## TN 2485 546 92 1401.33 3569 ## TX 851 379 92 99.08 1604 ## VA 2397 378 92 1646.76 3148 ## WA 3935 303 92 3332.27 4537 ## ## gen = Cascade: ## loc emmean SE df lower.CL upper.CL ## GGA 1758 379 92 1006.25 2511 ## ID 4081 303 92 3479.04 4684 ## KS 891 546 92 -193.40 1975 ## MS 1598 546 92 514.04 2682 ## MT 3125 544 92 2045.63 4205 ## NC 1062 379 92 309.61 1814 ## NY 2586 546 92 1502.21 3670 ## OR 2806 379 92 2053.82 3558 ## SC 1982 303 92 1379.70 2584 ## TGA 1492 303 92 889.83 2094 ## TN 2006 546 92 922.37 3090 ## TX 796 379 92 43.59 1548 ## VA 2191 378 92 1440.56 2942 ## WA 4203 303 92 3600.69 4805 ## ## gen = Dwarf: ## loc emmean SE df lower.CL upper.CL ## GGA 1538 379 92 785.71 2290 ## ID 4326 303 92 3723.81 4928 ## KS 1208 546 92 123.85 2292 ## MS 1966 546 92 881.69 3050 ## MT 3661 544 92 2581.14 4740 ## NC 1321 379 92 568.53 2073 ## NY 3645 546 92 2561.26 4729 ## OR 3594 379 92 2841.40 4346 ## SC 1292 303 92 690.10 1895 ## TGA 451 303 92 -151.81 1053 ## TN 2688 546 92 1603.57 3771 ## TX 654 379 92 -98.64 1406 ## VA 2250 378 92 1499.12 3000 ## WA 3726 303 92 3123.52 4328 ## ## gen = Glacier: ## loc emmean SE df lower.CL upper.CL ## GGA 2031 379 92 1278.35 2783 ## ID 4299 303 92 3696.61 4901 ## KS 1268 546 92 183.85 2352 ## MS 2861 546 92 1776.82 3945 ## MT 3516 544 92 2436.14 4595 ## NC 1452 379 92 699.82 2204 ## NY 3301 546 92 2217.49 4385 ## OR 3472 379 92 2719.36 4224 ## SC 2025 303 92 1422.97 2628 ## TGA 1109 303 92 506.90 1712 ## TN 2265 546 92 1180.58 3348 ## TX 720 379 92 -31.85 1473 ## VA 2363 378 92 1612.64 3114 ## WA 3807 303 92 3205.02 4410 ## ## gen = Jet: ## loc emmean SE df lower.CL upper.CL ## GGA 1511 379 92 758.95 2263 ## ID 4262 303 92 3659.68 4864 ## KS 1082 546 92 -2.40 2166 ## MS 1866 546 92 781.68 2950 ## MT 3869 544 92 2789.89 4949 ## NC 1326 379 92 573.58 2078 ## NY 3237 546 92 2152.80 4321 ## OR 3199 379 92 2446.70 3951 ## SC 1488 303 92 886.13 2091 ## TGA 622 303 92 19.27 1224 ## TN 2853 546 92 1768.63 3937 ## TX 1020 379 92 267.99 1772 ## VA 2364 378 92 1613.85 3115 ## WA 3554 303 92 2952.19 4157 ## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts ## gen = Bienvenu: ## contrast estimate SE df t.ratio p.value ## West_V_South 1968 267 92 7.359 \u0026lt;.0001 ## ## gen = Bridger: ## contrast estimate SE df t.ratio p.value ## West_V_South 1286 267 92 4.811 \u0026lt;.0001 ## ## gen = Cascade: ## contrast estimate SE df t.ratio p.value ## West_V_South 1948 267 92 7.286 \u0026lt;.0001 ## ## gen = Dwarf: ## contrast estimate SE df t.ratio p.value ## West_V_South 2132 267 92 7.972 \u0026lt;.0001 ## ## gen = Glacier: ## contrast estimate SE df t.ratio p.value ## West_V_South 1826 267 92 6.828 \u0026lt;.0001 ## ## gen = Jet: ## contrast estimate SE df t.ratio p.value ## West_V_South 1902 267 92 7.112 \u0026lt;.0001 ## ## Results are averaged over the levels of: Year ## Degrees-of-freedom method: containment  To perform custom contrasts on a another variable, a cList and emmeans call for that variable is required.\nANCOVA (analysis of covariance) From a R programming perspective, this is no different than running a standard linear model. A data set from agridat, \u0026ldquo;theobald.covariate\u0026rdquo; comparing corn silage yields across multiple years, locations and cultivars. The data set includes a covariate, \u0026ldquo;chu\u0026rdquo; (corn heat units, a bit like growing degree days).\nLoad data and examine:\ndata(theobald.covariate) str(theobald.covariate)  ## 'data.frame':\t256 obs. of 5 variables: ## $ year : int 1990 1990 1990 1990 1990 1991 1991 1991 1991 1991 ... ## $ env : Factor w/ 7 levels \u0026quot;E1\u0026quot;,\u0026quot;E2\u0026quot;,\u0026quot;E3\u0026quot;,..: 1 2 3 4 7 1 2 3 4 5 ... ## $ gen : Factor w/ 10 levels \u0026quot;G01\u0026quot;,\u0026quot;G02\u0026quot;,\u0026quot;G03\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ yield: num 6.27 5.57 8.45 7.35 6.5 6.71 5.59 8.36 7.25 8.09 ... ## $ chu : num 2.57 2.53 2.72 2.72 2.48 2.44 2.55 2.75 2.75 2.61 ...  count_na(theobald.covariate)  ## year env gen yield chu ## 0 0 0 0 0  Exploratory plots:\n# distributions of continuous variables hist(theobald.covariate$yield, col = \u0026quot;gold\u0026quot;)  hist(theobald.covariate$chu, col = \u0026quot;gray70\u0026quot;)  # relationship between reponse variable and covariate: with(theobald.covariate, plot(chu, yield))  length(unique(theobald.covariate$chu))  ## [1] 21  # the usual boxplots: boxplot(yield ~ env, data = theobald.covariate, col = \u0026quot;orangered\u0026quot;)  boxplot(yield ~ year, data = theobald.covariate, col = \u0026quot;chartreuse\u0026quot;)  boxplot(yield ~ gen, data = theobald.covariate, col = \u0026quot;darkcyan\u0026quot;)  Check the extent of replication:\ntheobald.covariate$Year \u0026lt;- as.factor(theobald.covariate$year) replications(yield ~ Year + env + gen, data = theobald.covariate)  ## $Year ## Year ## 1990 1991 1992 1993 1994 ## 40 63 60 45 48 ## ## $env ## env ## E1 E2 E3 E4 E5 E6 E7 ## 35 35 44 36 36 36 34 ## ## $gen ## gen ## G01 G02 G03 G04 G05 G06 G07 G08 G09 G10 ## 29 29 29 29 22 29 23 18 24 24  # with(theobald.covariate, table(gen, env, Year)) # lots of useful output  The treatments are not fully crossed, so a fully specified model of the form yield ~ Year*env*gen*chu cannot be tested. The treatments and interactions were tested in reduced models and compared (not shown). The final \u0026ldquo;best\u0026rdquo; model is shown below.\n# the covariate, chu, is added in like any other effect. theobald.lm2 \u0026lt;- lm(yield ~ Year + env*chu, data = theobald.covariate) Anova(theobald.lm2, type = \u0026quot;III\u0026quot;)  ## Anova Table (Type III tests) ## ## Response: yield ## Sum Sq Df F value Pr(\u0026gt;F) ## (Intercept) 4.309 1 6.8321 0.009524 ** ## Year 76.589 4 30.3607 \u0026lt; 2.2e-16 *** ## env 13.473 6 3.5607 0.002138 ** ## chu 11.831 1 18.7596 2.187e-05 *** ## env:chu 13.376 6 3.5350 0.002268 ** ## Residuals 150.096 238 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  # how to extract the covariate slope(s): emtrends(theobald.lm2, ~ env, \u0026quot;chu\u0026quot;)  ## env chu.trend SE df lower.CL upper.CL ## E1 7.015 1.62 238 3.82 10.21 ## E2 0.979 4.44 238 -7.76 9.72 ## E3 4.099 3.15 238 -2.11 10.31 ## E4 -2.884 3.54 238 -9.87 4.10 ## E5 8.222 2.70 238 2.90 13.54 ## E6 3.425 2.72 238 -1.93 8.78 ## E7 -0.359 2.55 238 -5.38 4.66 ## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95  # emmeans extracted as usual: emmeans(theobald.lm2, ~ env)  ## NOTE: Results may be misleading due to involvement in interactions  ## env emmean SE df lower.CL upper.CL ## E1 6.67 0.175 238 6.32 7.01 ## E2 5.13 0.256 238 4.63 5.64 ## E3 6.66 0.482 238 5.71 7.61 ## E4 7.22 0.508 238 6.22 8.22 ## E5 6.61 0.138 238 6.34 6.88 ## E6 6.43 0.236 238 5.97 6.90 ## E7 6.32 0.397 238 5.54 7.10 ## ## Results are averaged over the levels of: Year ## Confidence level used: 0.95  emmeans(theobald.lm2, ~ Year)  ## Year emmean SE df lower.CL upper.CL ## 1990 6.97 0.189 238 6.60 7.34 ## 1991 6.75 0.170 238 6.41 7.08 ## 1992 7.07 0.187 238 6.70 7.44 ## 1993 5.39 0.208 238 4.98 5.80 ## 1994 6.00 0.218 238 5.57 6.43 ## ## Results are averaged over the levels of: env ## Confidence level used: 0.95  Split-plot Load \u0026ldquo;Oats\u0026rdquo; from nlme. Nitrogen level (\u0026ldquo;nitro\u0026rdquo;) is the main plot, cultivar (\u0026ldquo;Variety\u0026rdquo;) is the sub-plot and \u0026ldquo;Block\u0026rdquo; describes the blocking layout.\ndata(Oats) str(Oats)  ## Classes 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':\t72 obs. of 4 variables: ## $ Block : Ord.factor w/ 6 levels \u0026quot;VI\u0026quot;\u0026lt;\u0026quot;V\u0026quot;\u0026lt;\u0026quot;III\u0026quot;\u0026lt;..: 6 6 6 6 6 6 6 6 6 6 ... ## $ Variety: Factor w/ 3 levels \u0026quot;Golden Rain\u0026quot;,..: 3 3 3 3 1 1 1 1 2 2 ... ## $ nitro : num 0 0.2 0.4 0.6 0 0.2 0.4 0.6 0 0.2 ... ## $ yield : num 111 130 157 174 117 114 161 141 105 140 ... ## - attr(*, \u0026quot;formula\u0026quot;)=Class 'formula' language yield ~ nitro | Block ## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt; ## - attr(*, \u0026quot;labels\u0026quot;)=List of 2 ## ..$ y: chr \u0026quot;Yield\u0026quot; ## ..$ x: chr \u0026quot;Nitrogen concentration\u0026quot; ## - attr(*, \u0026quot;units\u0026quot;)=List of 2 ## ..$ y: chr \u0026quot;(bushels/acre)\u0026quot; ## ..$ x: chr \u0026quot;(cwt/acre)\u0026quot; ## - attr(*, \u0026quot;inner\u0026quot;)=Class 'formula' language ~Variety ## .. ..- attr(*, \u0026quot;.Environment\u0026quot;)=\u0026lt;environment: R_GlobalEnv\u0026gt;  count_na(Oats)  ## Block Variety nitro yield ## 0 0 0 0  Oats$N \u0026lt;- as.factor(Oats$nitro) replications(yield ~ Variety*N*Block, data = Oats)  ## Variety N Block Variety:N Variety:Block ## 24 18 12 6 4 ## N:Block Variety:N:Block ## 3 1  table(Oats$Variety, Oats$N)  ## ## 0 0.2 0.4 0.6 ## Golden Rain 6 6 6 6 ## Marvellous 6 6 6 6 ## Victory 6 6 6 6  hist(Oats$yield, col = \u0026quot;gold\u0026quot;)  boxplot(yield ~ N, data = Oats, col = \u0026quot;dodgerblue1\u0026quot;)  boxplot(yield ~ Variety, data = Oats, col = \u0026quot;red3\u0026quot;)  Balanced Trial Analysis\nThe format for specifying split-plot error terms is Error(blocking factor/main plot).\n#contrasts(\u0026quot;contr.sum\u0026quot;) spl.oats \u0026lt;- aov(yield ~ Variety*N + Error(Block:N), data = Oats) summary(spl.oats)  ## ## Error: Block:N ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## N 3 20020 6673 7.556 0.00143 ** ## Residuals 20 17663 883 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## Variety 2 1786 893.2 2.930 0.0649 . ## Variety:N 6 322 53.6 0.176 0.9818 ## Residuals 40 12194 304.8 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(spl.oats, \u0026quot;N\u0026quot;)  ## Note: re-fitting model with sum-to-zero contrasts  ## NOTE: Results may be misleading due to involvement in interactions  ## N emmean SE df lower.CL upper.CL ## 0 79.4 7 20 64.8 94 ## 0.2 98.9 7 20 84.3 114 ## 0.4 114.2 7 20 99.6 129 ## 0.6 123.4 7 20 108.8 138 ## ## Results are averaged over the levels of: Variety ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  emmeans(spl.oats, ~ Variety)  ## Note: re-fitting model with sum-to-zero contrasts ## NOTE: Results may be misleading due to involvement in interactions  ## Variety emmean SE df lower.CL upper.CL ## Golden Rain 104.5 4.55 46.1 95.3 114 ## Marvellous 109.8 4.55 46.1 100.6 119 ## Victory 97.6 4.55 46.1 88.5 107 ## ## Results are averaged over the levels of: N ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95  Unbalanced Trial Analysis\nspl.oats2 \u0026lt;- lmer(yield ~ N*Variety + (1|Block:N), data = Oats) Anova(spl.oats2, type = \u0026quot;3\u0026quot;)  ## Analysis of Deviance Table (Type III Wald chisquare tests) ## ## Response: yield ## Chisq Df Pr(\u0026gt;Chisq) ## (Intercept) 77.1670 1 \u0026lt; 2.2e-16 *** ## N 13.9028 3 0.003041 ** ## Variety 2.2747 2 0.320663 ## N:Variety 1.0554 6 0.983423 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  emmeans(spl.oats2, \u0026quot;N\u0026quot;)  ## NOTE: Results may be misleading due to involvement in interactions  ## N emmean SE df lower.CL upper.CL ## 0 79.4 7 20 64.8 94 ## 0.2 98.9 7 20 84.3 114 ## 0.4 114.2 7 20 99.6 129 ## 0.6 123.4 7 20 108.8 138 ## ## Results are averaged over the levels of: Variety ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95  emmeans(spl.oats2, ~ Variety)  ## NOTE: Results may be misleading due to involvement in interactions  ## Variety emmean SE df lower.CL upper.CL ## Golden Rain 104.5 4.55 46.1 95.3 114 ## Marvellous 109.8 4.55 46.1 100.6 119 ## Victory 97.6 4.55 46.1 88.5 107 ## ## Results are averaged over the levels of: N ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95  Other Designs There are many other experimental designs commonly used in agricultural trials (split-split plot, split-block, alpha lattice, etc). We have written an online resource for routine incorporation of spatial covariates into field trial analysis that includes information on how to analyze different designs. You could also consider using the agricolae package.\nExtra Functions for extracting model parameters, diagnostics and other model information\nThese work differently with different R object types. That is, different output will result depending on if a \u0026ldquo;lm\u0026rdquo;, \u0026ldquo;lme\u0026rdquo; or \u0026ldquo;merMod\u0026rdquo; (lmer) object is used in the function call.\n# extract model summary summary() #extract coefficients: coef() #extract residuals resid() rstudent() residuals() # extract predicted values fits() # make diagnostic plots plot() # extract influence measures: influence.measures() #other fir diagnostics: cooks.distance() dffits() dfbeta() hat()  To see the all functions available for a particular type of linear model object, use:\nmethods(class = \u0026quot;lm\u0026quot;) # for lm objects  ## [1] add1 addterm alias ## [4] anova Anova attrassign ## [7] avPlot Boot bootCase ## [10] boxcox boxCox brief ## [13] case.names ceresPlot coerce ## [16] concordance confidenceEllipse confint ## [19] Confint cooks.distance crPlot ## [22] deltaMethod deviance dfbeta ## [25] dfbetaPlots dfbetas dfbetasPlots ## [28] drop1 dropterm dummy.coef ## [31] durbinWatsonTest effects emm_basis ## [34] extractAIC family formula ## [37] hatvalues hccm infIndexPlot ## [40] influence influencePlot initialize ## [43] inverseResponsePlot kappa labels ## [46] leveneTest leveragePlot linearHypothesis ## [49] logLik logtrans mcPlot ## [52] mmp model.frame model.matrix ## [55] ncvTest nextBoot nobs ## [58] outlierTest plot powerTransform ## [61] predict Predict print ## [64] proj qqnorm qqPlot ## [67] qr recover_data residualPlot ## [70] residualPlots residuals rstandard ## [73] rstudent S show ## [76] sigmaHat simulate slotsFromS3 ## [79] spreadLevelPlot summary symbox ## [82] variable.names vcov ## see '?methods' for accessing help and source code  methods(class = \u0026quot;lme\u0026quot;) # for lme4 objects  ## [1] ACF anova Anova augPred ## [5] coef comparePred confint Confint ## [9] deltaMethod deviance emm_basis extractAIC ## [13] fitted fixef formula getData ## [17] getGroups getGroupsFormula getResponse getVarCov ## [21] influence intervals linearHypothesis logLik ## [25] matchCoefs nobs pairs plot ## [29] predict print qqnorm ranef ## [33] recover_data residuals S sigma ## [37] simulate summary update VarCorr ## [41] Variogram vcov ## see '?methods' for accessing help and source code  methods(class = \u0026quot;merMod\u0026quot;) # for nlme objects  ## [1] anova Anova as.function coef ## [5] confint cooks.distance deltaMethod deviance ## [9] df.residual drop1 emm_basis extractAIC ## [13] family fitted fixef formula ## [17] getData getL getME hatvalues ## [21] influence isGLMM isLMM isNLMM ## [25] isREML linearHypothesis logLik matchCoefs ## [29] model.frame model.matrix na.action ngrps ## [33] nobs plot predict print ## [37] profile ranef recover_data refit ## [41] refitML rePCA residuals rstudent ## [45] show sigma simulate summary ## [49] terms update VarCorr vcov ## [53] vif weights ## see '?methods' for accessing help and source code  The package emmeans also supports a large number of models.\n","date":1637107200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637107200,"objectID":"a1d58233f436a629f860db1f7ebc5c18","permalink":"/post/anova-in-r/","publishdate":"2021-11-17T00:00:00Z","relpermalink":"/post/anova-in-r/","section":"post","summary":"Introduction ANOVA in R is a unfortunately a bit complicated. Unlike SAS, ANOVA functions in R lack a consistent structure, consistent output and the accessory packages for ANOVA display a patchwork of compatibility.","tags":["ANOVA","linear models","lme4","emmeans"],"title":"Applied ANOVA in R","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Install R: You can download R here. Get the correct R distribution for your operating system. Once downloaded, click on downloaded file, and follow the installation instructions.\nNote that R is updated several times per year. If your installation is a year old or more, consider updating your version of R to the latest version.\nInstall RStudio Rstudio is not R, rather, it is a user interface for accessing R. It is a complicated interface with many features for developers. Despite its complexity, RStudio is nevertheless a very helpful R user interface for users of all abilities. It can downloaded here. For most users, the free version of \u0026ldquo;RStudio Desktop\u0026rdquo; should be chosen. Once downloaded, click on downloaded file, and follow the installation instructions.\nInstall Rtools (optional) Only Windows users need to consider this step. This app is for compiling R packages with C, C++ and Fortran code. It is a separate piece of software that has to be downloaded and installed (it is not an R package). Rtools is not needed by all users and if you don\u0026rsquo;t know if you need this, it is absolutely fine to skip this step. If you do think you need this, You can find it here. Download and install.\nSetting up RStudio Setup (optional) This is an optional step, but it is highly recommended. This step will prevent RStudio from saving all of your objects in a session to .Rdata file that is then automatically loaded whenever you open R.\ninstall.packages(\u0026quot;usethis\u0026quot;); library(usethis) usethis::use_blank_slate()  You can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nWhy is automatic loading of an .Rdata file not recommended? Because it makes your work less reproducible. You may have created test objects that will unexpectedly interfere with downstream operations or analysis. You may have changed the original data source, but an older version is saved in the .Rdata file. More explanation is given by RStudio.\nIf you are used to opening R and seeing all of your previous objects automatically loaded into the objects pane, this will be an adjustment. The solution is to save your processes into .R scripts that capture all information from packages loaded, file import, all data manipulations and other operations important. If these steps are slow and there is a need to access intermediate objects, these can be saved in tabular formats readable by many applications (e.g. .txt or .csv) or saved as a specific R object (see saveRDS() in the R help files) and reloaded in another session.\nSet up version control (optional) If you use Git or SVN, you can perform Git operations directions from RStudio and interact with remote repositories. If you don\u0026rsquo;t use version control, this step can be skipped. If you do use version control, the command line or other third-party software (e.g. Gitkraken) are fine to use instead or in addition to RStudio\u0026rsquo;s interface. The implementation of git in R is very minimal and supports only a limited number of actions, so you are likely to need other software to perform complicated git actions. It is useful for file additions, commits, pushes and pulls.\nYou can set up Git by going to Tools \u0026ndash;\u0026gt; Global Options \u0026ndash;\u0026gt; Git/SVN.\nThis is not the right space to provide detailed instructions for using git as an R user, but Jenny Bryan has written a very helpful tutorial covering this subject.\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"8b70804b6afa070131995606b8772ebd","permalink":"/post/getting-r-setup/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/getting-r-setup/","section":"post","summary":"Some instructions for R installation and your R setup to support reproducible research.","tags":["R","reproducible research"],"title":"Getting R Set Up","type":"post"},{"authors":["Julia Piaskowski"],"categories":["R","reproducible research"],"content":"Make sure your Rstudio session is not saving .RData automatically: Note: this step requires the usethis package; please install this package if you do not already have it installed.\nStep 1 is to disable automatic saving of your objects to a .RData file. This file is automatically loaded when R restarts. Since we often create all sorts of miscellaneous objects during a session with a clear record of why, loading all objects without a clear sense of their provenance is often not reproducible by other.\nusethis::use_blank_slate()  You can read more about this function in its documentation.\nYou can disable this across all projects in R with the drop-down menu Tools \u0026ndash;\u0026gt; Global Options\u0026hellip; \u0026ndash;\u0026gt; unclick \u0026lsquo;Restore .RData into workspace at startup\u0026rsquo; and set \u0026lsquo;Save workspace to .rRData on exit\u0026rsquo; to \u0026lsquo;Never\u0026rsquo;.\nSave all code you run in an .R or .Rmd file This is your source code. It\u0026rsquo;s as real and as important as your input data. This file should capture a set of actions that can be repeated by another person (e.g. your PI, other colleagues yourself in the future) including packages loaded, files imported, all data manipulations and the outputs from these actions (e.g. visualisations, analytical outcomes). The idea is to capture your thought process and specific actions so this can be repeated in full. In most analyses, it is extremely likely* you will revisit a project and need to repeat what has already been done! Keeping a record of actions will save you considerable time because you will not have to attempt to recall and/or reconstruct exactly what you did in previous sessions.\n*Consider yourself very lucky if this does not happen!\nRegularly restart your R session Yes, that means wiping all the loaded packaged and objects from the session (if you followed the first recommendation in these instructions), but the upside is that your analysis are reproducible. This means future you can repeat those analyses and get the same results back you did earlier.\nYou can restart R by manually closing and opening RStudio. You can also restart the R session with RStudio by navigating to the menu item Session \u0026ndash;\u0026gt; Restart R.\nUse R projects This is optional, but it will make your life easier. Whenever you start a new analytical endeavor in R, create an R project by navigating to File \u0026ndash;\u0026gt; New Project in RStudio. There are many options available for setting the [project directory (where the .Rproj file lives), the type of project (e.g. R package, Shiny app or blank), and options to initialise a git repo. The simplest option is to choose New Project (no special type) in a dedicated directory. The main advantage of projects is that by opening an .Rproj file, the working directory is automatically set to that directory. If you are using a cloud solution for working across different computers or working with collaborators, this will make things easier because you can use relative paths for importing data and outputting files. There would be no more need for this at the top of your script:\nsetwd(\u0026quot;specific/path/to/my/computer\u0026quot;)  Additionally, for setting up gitbooks through \u0026lsquo;bookdown\u0026rsquo;, R packages, Shiny apps, and other complicated R endeavors, the automated set-up through R projects can be immensely helpful. This is sometimes referred to as \u0026ldquo;project-oriented workflow.\u0026rdquo; In addition to using R projects with a dedicated directory for each research project, I also prefer to have a consistent directory structure for each project like this one:\ntop-level-directory â README.md â ââââdata â â file011.txt â â file012.txt â â â ââââspatial_files â â file208.dbf â â file208.shp â â file208.shx â ââââscripts â â eda.R â â analysis.R â â plots.R â â final_report.Rmd | ââââoutputs â â plot1.png â â blups.csv | ââââextra â some_paper.pdf â ...  I put all raw data needed for analysis into the \u0026lsquo;data\u0026rsquo; directory, any and all programming scripts in the \u0026lsquo;scripts\u0026rsquo; directory, all outputs (plots, tables, intermediate data object) in the \u0026lsquo;outputs\u0026rsquo; directory and everything else ends up \u0026lsquo;extra\u0026rsquo;. Naturally, there are many different directory structures to use and this is just one example. Find something that works best for your needs!\nUse the \u0026lsquo;here\u0026rsquo; package. This is also optional. It works like R projects for setting the working directory. However, for an R project to work, you have to open the .Rproj file in RStudio. What if you or your collaborators prefer to open R files directly and start using those? Here will look for the next directory level which there is a .Rproj file and set the working directory there.\nIf you want to import a file, \u0026ldquo;datafile.csv\u0026rdquo; that located in the data directory. Your .R script is actually located in the \u0026lsquo;scripts\u0026rsquo; directory. Normally, if you try to read that in, you need to specify the full path to \u0026ldquo;mydata.csv\u0026rdquo; or set the working directory and use a relative path. Again, these paths will not work if you switch computers or your collaborators are running these scripts on their own systems. This system gets even more complicated when working with an .Rmd file. Here\u0026rsquo;s an alternative approach that works the same across files and systems:\nFirst, make sure you have .Rproj file to define the top-level directory.\nlibrary(here) mydata \u0026lt;- read.csv(here(\u0026quot;data\u0026quot;, \u0026quot;datafile.csv\u0026quot;))  This code will construct this path: \u0026ldquo;data/datafile.csv\u0026rdquo; and execute that command under the assumption that wherever that .rproj is located (going up one directory at a time until it finds it) is where the working directory is set. Putting library(here) into every .R or .Rmd file in a project will resolve these issues.\nUse R environments. Again: optional, but it will make your life easier.\nOften in academia, I might do an analysis, move on to something else and then have to return that analysis months or years later. I probably will have updated R and some or all of the packages used in that analysis. As a result of these updates, my original code may not work at all or may not do the intended actions. What I need are both the older version of R and the older packages. The package \u0026lsquo;renv\u0026rsquo; is a solution. It captures the versions of R and the loaded packages. It also builds a custom package library for your package (and caches this information across other projects using renv).\nStart here: (you need to also be using Rprojects since renv is searching for .Rproj file)\nlibrary(renv) renv::init()  If you have a mature project that\u0026rsquo;s not undergoing any further development at this time, this is all you need to do.\nIf you continue to develop your project and install new packages, update your R environment like thus to ensure new or updated packaged are included:\nrenv::snapshot()  If you\u0026rsquo;re familiar with Packrat, this is a replacement for that. This is particularly helpful for things that may have a long life span, like Shiny apps. The renv package has extensive documentation worth reading.\nFinal Comments There are many more resources and recommendations for conducting reproducible research in R. There an entire CRAN task view devoted to this!\n","date":1618444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618444800,"objectID":"32bd8072205d33315c2c1a506db82c8c","permalink":"/post/reproducible-r/","publishdate":"2021-04-15T00:00:00Z","relpermalink":"/post/reproducible-r/","section":"post","summary":"A few steps you can take to make your workflow in R more reproducible and less painful for you to deal with.","tags":["R","Reproducible Research"],"title":"Quick Tricks and Tips for Reproducible Research in R","type":"post"},{"authors":["Statistical Programs"],"categories":null,"content":"","date":1617982200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617982200,"objectID":"7a023a5229b3c480b15defa8bfb626a1","permalink":"/talks/spatial_seminar_20210409/","publishdate":"2021-04-09T15:30:00Z","relpermalink":"/talks/spatial_seminar_20210409/","section":"talks","summary":"A brief introduction into how to integrate spatial covariates into ANOVA-based analysis of field trials laid out in a lattice pattern.","tags":["spatial statistics","field experiments"],"title":"Routine Incorporation of Spatial Covariates into Analysis of Planned Field Experiments","type":"talks"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]